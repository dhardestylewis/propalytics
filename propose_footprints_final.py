# -*- coding: utf-8 -*-
"""propose_footprints_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g2oSZy3opsz7zV_ohO1QVNY33CeaSNfz
"""

# Consolidated Zoning Analysis Notebook

# Install required libraries
!pip install rhino3dm shapely geopandas

# Import necessary libraries
import requests
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from shapely.geometry import Polygon, MultiPolygon, LineString, Point, shape
from shapely.ops import unary_union
from sklearn.cluster import DBSCAN
import rhino3dm
import time
from tqdm import tqdm
import re
from joblib import Parallel, delayed
import multiprocessing

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Function to fetch MAPPLUTO data
def fetch_mappluto_data(url, params=None):
    all_data = []
    offset = 0
    while True:
        if params is None:
            params = {}
        params['resultOffset'] = offset
        params['resultRecordCount'] = 4000
        response = requests.get(url, params=params)
        data = response.json()
        if 'features' not in data or not data['features']:
            break
        all_data.extend(data['features'])
        offset += 4000
    return all_data

# Function to query MAPPLUTO
def query_mappluto(borough_code, community_district):
    url = "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0/query"
    where_clause = f"BoroCode = {borough_code} AND CD = {community_district}"
    params = {
        'where': where_clause,
        'outFields': 'Borough,Block,Lot,BoroCode,BBL,NumFloors,LotArea,BuiltFAR,ResidFAR,CommFAR,FacilFAR,LandUse,BldgClass,LotFront,LotDepth,BldgFront,BldgDepth,ZoneDist1,BldgArea,ResArea',
        'outSR': '102718',
        'f': 'geojson',
        'geometryType': 'esriGeometryPolygon',
        'inSR': '102718',
        'spatialRel': 'esriSpatialRelIntersects'
    }
    return fetch_mappluto_data(url, params)

# Function to read 3DM file and extract geometries
def read_3dm(file_path, layer_name):
    model = rhino3dm.File3dm.Read(file_path)
    layers = {layer.Index: layer.Name for layer in model.Layers}
    geometries = []
    for obj in model.Objects:
        if layers[obj.Attributes.LayerIndex] == layer_name:
            geom = obj.Geometry
            if isinstance(geom, rhino3dm.PolylineCurve):
                polyline = geom.ToPolyline()
                points = polyline
                polygon_points = [(pt.X, pt.Y) for pt in points]
                geometries.append(Polygon(polygon_points))
    gdf = gpd.GeoDataFrame({'geometry': geometries})
    return gdf

# Function to read 3DM file and extract geometries with Z-coordinates
def read_3dm_with_z(file_path, layer_name):
    model = rhino3dm.File3dm.Read(file_path)
    layers = {layer.Index: layer.Name for layer in model.Layers}
    geometries = []
    for obj in model.Objects:
        if layers[obj.Attributes.LayerIndex] == layer_name:
            geom = obj.Geometry
            if isinstance(geom, rhino3dm.PolylineCurve):
                polyline = geom.ToPolyline()
                points = polyline
                polygon_points = [(pt.X, pt.Y, pt.Z) for pt in points]
                geometries.append(polygon_points)
    return geometries

# Function to calculate building height from 3D points
def calculate_building_height(geometries):
    heights = []
    for points in geometries:
        z_coordinates = [point[2] for point in points]
        height = max(z_coordinates) - min(z_coordinates)
        heights.append(height)
    return heights

# Function to get lowest segment of a geometry
def get_lowest_segment(geom):
    if isinstance(geom, Polygon):
        exterior_coords = list(geom.exterior.coords)
    elif isinstance(geom, LineString):
        exterior_coords = list(geom.coords)
    else:
        return None

    if len(exterior_coords) < 2:
        return None

    if len(exterior_coords[0]) < 3:
        return LineString([exterior_coords[0], exterior_coords[1]])

    min_z = min(exterior_coords, key=lambda x: x[2])[2]
    lowest_segments = [
        LineString([exterior_coords[i], exterior_coords[i + 1]])
        for i in range(len(exterior_coords) - 1)
        if exterior_coords[i][2] == min_z and exterior_coords[i + 1][2] == min_z
    ]
    return lowest_segments[0] if lowest_segments else LineString([exterior_coords[0], exterior_coords[1]])

def strip_units(value):
    if isinstance(value, str):
        return re.sub(r'[^\d.]+', '', value)
    return value

# Updated get_zoning_regulation function
def get_zoning_regulation(zone, feature, standards='current'):
    if pd.isna(zone):
        return np.nan
    try:
        if standards == 'current':
            if feature == 'Residential FAR (max)':
                value = zoning_data.loc[zoning_data['Feature'] == 'Residential FAR (max)', zone].values
            elif feature == 'Building Height (max)':
                value = zoning_data.loc[zoning_data['Feature'] == 'Building height (max)', zone].values
            elif feature == 'Rear Yard Depth (min)':
                value = zoning_data.loc[zoning_data['Feature'] == 'Rear yard depth (min)', zone].values
            elif feature == 'Front Yard Depth (min)':
                value = zoning_data.loc[zoning_data['Feature'] == 'Front yard depth (min)', zone].values
            elif feature == 'Side Yards (total width)':
                value = zoning_data.loc[zoning_data['Feature'].str.contains('total width of side yards', case=False), zone].values

            if len(value) > 0 and not pd.isna(value[0]):
                return float(strip_units(value[0]))  # Strip units and convert to float
        elif standards == 'proposed':
            if zone in proposed_standards.index and feature in proposed_standards.columns:
                return float(strip_units(proposed_standards.loc[zone, feature]))  # Strip units and convert to float
        return np.nan
    except KeyError:
        return np.nan

# Function to determine max FAR based on land use and available data
def get_max_far(land_use, resid_far):
    if land_use in ['01', '02', '03', '04', '05', '06', '07', '08']:  # Residential and Mixed Residential
        return resid_far
    else:
        return resid_far  # Default to Residential FAR for other uses as well

# Function to check compliance with zoning regulations
def check_compliance(row, standards):
    try:
        zone = row['ZoneDist1']
        results = {}
        for feature, regulation in [('FAR', 'Residential FAR (max)'),
                                    ('height', 'Building Height (max)'),
                                    ('front_yard', 'Front Yard Depth (min)'),
                                    ('rear_yard', 'Rear Yard Depth (min)')]:
            standard = get_zoning_regulation(zone, regulation, standards)
            if pd.notnull(standard):
                if feature == 'FAR':
                    results[f'{feature}_compliance_{standards}'] = float(row['BuiltFAR']) <= standard
                elif feature == 'height':
                    results[f'{feature}_compliance_{standards}'] = float(row['max_height']) <= standard
                elif feature == 'front_yard':
                    results[f'{feature}_compliance_{standards}'] = float(row['FrontYardDepth']) >= standard
                elif feature == 'rear_yard':
                    results[f'{feature}_compliance_{standards}'] = float(row['RearYardDepth']) >= standard
            else:
                results[f'{feature}_compliance_{standards}'] = False
        return pd.Series(results)
    except Exception as e:
        print(f"Error checking compliance for row {row.name}: {e}")
        return pd.Series({f'{feature}_compliance_{standards}': False for feature in ['FAR', 'height', 'front_yard', 'rear_yard']})

# Precompute yard buffers for each zone
def precompute_yard_buffers(standards):
    yard_buffers = {}
    for zone in standards.index:
        front_yard = standards.loc[zone, 'Front Yard Depth (min)']
        rear_yard = standards.loc[zone, 'Rear Yard Depth (min)']
        side_yards = standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards
        yard_buffers[zone] = (front_yard, rear_yard, side_yards)
    return yard_buffers

# Function to calculate buildable area
def calculate_buildable_area(row, yard_buffers):
    zone = row['ZoneDist1']
    if zone not in yard_buffers:
        return None, None

    lot_area = row['LotArea']
    max_FAR = standards.loc[zone, 'Residential FAR (max)']
    max_floor_area = lot_area * max_FAR

    current_floor_area = row['BldgArea']
    remaining_floor_area = max_floor_area - current_floor_area

    front_yard, rear_yard, side_yards = yard_buffers[zone]

    lot_width = row['LotFront']
    lot_depth = row['LotDepth']
    buildable_width = lot_width - side_yards
    buildable_depth = lot_depth - front_yard - rear_yard

    # Avoid negative buildable dimensions
    buildable_width = max(0, buildable_width)
    buildable_depth = max(0, buildable_depth)

    buildable_footprint = buildable_width * buildable_depth

    # Ensure the buildable footprint area does not exceed the remaining floor area allowed by FAR
    buildable_footprint_area = min(remaining_floor_area, buildable_footprint)

    # Determine the number of floors needed to utilize the remaining floor area
    if buildable_footprint_area == 0:
        num_floors = 1
    else:
        num_floors = max(1, int(remaining_floor_area // buildable_footprint_area))

    return buildable_footprint_area, num_floors

def process_row(row, yard_buffers, all_existing_footprints):
    try:
        zone = row['ZoneDist1']
        if zone not in yard_buffers:
            return None, None

        lot_geometry = row['geometry']
        buildable_footprint_area, num_floors = calculate_buildable_area(row, yard_buffers)

        if buildable_footprint_area is None or num_floors is None or buildable_footprint_area <= 0:
            return None, None

        front_yard, rear_yard, side_yards = yard_buffers[zone]

        # Subtract yard spaces from the lot geometry to define the buildable area
        buildable_area = lot_geometry.buffer(-front_yard - rear_yard - side_yards)

        # Subtract all existing footprints from the buildable area
        buildable_area = buildable_area.difference(all_existing_footprints)

        if buildable_area.is_empty or buildable_area.area <= 0:
            return None, None

        if isinstance(buildable_area, MultiPolygon):
            buildable_area = max(buildable_area.geoms, key=lambda p: p.area)

        new_footprint = buildable_area.buffer(0)

        if new_footprint.area > buildable_footprint_area:
            shrink_factor = np.sqrt(buildable_footprint_area / new_footprint.area)
            if np.isfinite(shrink_factor) and shrink_factor > 0:
                new_footprint = new_footprint.buffer(shrink_factor - 1, join_style=2)
            else:
                new_footprint = None

        return new_footprint, num_floors
    except Exception as e:
        print(f"Error processing row: {e}")
        return None, None

def generate_new_footprints(gdf, yard_buffers):
    all_existing_footprints = unary_union(gdf['geometry'])

    results = Parallel(n_jobs=-1, verbose=5)(
        delayed(process_row)(row, yard_buffers, all_existing_footprints) for _, row in gdf.iterrows()
    )

    new_footprints, num_floors_list = zip(*results)
    return new_footprints, num_floors_list

def visualize_progress(gdf, interval=50):
    for i, row in gdf.iterrows():
        if i % interval == 0 and row['proposed_footprint'] is not None:
            plt.figure()
            plt.plot(*row['geometry'].exterior.xy, color='blue', label='Lot Geometry')
            plt.plot(*row['proposed_footprint'].exterior.xy, color='red', label='Proposed Footprint')
            plt.title(f'Visualization at Row {i}')
            plt.legend()
            plt.show()

def calculate_far_increase(row):
    current_built_FAR = row['BuiltFAR']
    current_max_FAR = row['current_max_FAR']
    proposed_max_FAR = row['proposed_max_FAR']

    if current_built_FAR >= proposed_max_FAR:
        # Property already exceeds proposed regulations
        return 0
    else:
        # Calculate potential increase, but don't exceed proposed maximum
        potential_increase = proposed_max_FAR - current_built_FAR
        return min(potential_increase, proposed_max_FAR - current_max_FAR)

# Function to generate new footprints
# def generate_new_footprints(gdf, yard_buffers):
#     new_footprints = []
#     num_floors_list = []

#     # Precompute the union of all existing footprints
#     all_existing_footprints = unary_union(gdf['geometry'])

#     for index, row in tqdm(gdf.iterrows(), total=gdf.shape[0], desc="Generating new footprints"):
#         try:
#             zone = row['ZoneDist1']
#             if zone not in yard_buffers:
#                 new_footprints.append(None)
#                 num_floors_list.append(None)
#                 continue

#             lot_geometry = row['geometry']
#             buildable_footprint_area, num_floors = calculate_buildable_area(row, yard_buffers)

#             if buildable_footprint_area is None or num_floors is None or buildable_footprint_area <= 0:
#                 new_footprints.append(None)
#                 num_floors_list.append(None)
#                 continue

#             front_yard, rear_yard, side_yards = yard_buffers[zone]

#             # Subtract yard spaces from the lot geometry to define the buildable area
#             buildable_area = lot_geometry.buffer(-front_yard).buffer(-rear_yard).buffer(-side_yards)

#             # Subtract all existing footprints from the buildable area
#             buildable_area = buildable_area.difference(all_existing_footprints)

#             if buildable_area.is_empty or buildable_area.area <= 0:
#                 new_footprints.append(None)
#                 num_floors_list.append(None)
#                 continue

#             if isinstance(buildable_area, MultiPolygon):
#                 buildable_area = max(buildable_area.geoms, key=lambda p: p.area)

#             new_footprint = buildable_area.buffer(0)

#             if new_footprint.area > buildable_footprint_area:
#                 shrink_factor = np.sqrt(buildable_footprint_area / new_footprint.area)
#                 if np.isfinite(shrink_factor) and shrink_factor > 0:
#                     new_footprint = new_footprint.buffer(shrink_factor - 1, join_style=2)
#                 else:
#                     new_footprint = None

#             new_footprints.append(new_footprint)
#             num_floors_list.append(num_floors)

#             # Visualize example every 500 iterations
#             if index % 50 == 0 and index > 0:
#                 print(f"\nExample at index {index}:")
#                 print(f"Zone: {zone}")
#                 print(f"Lot Geometry: {lot_geometry}")
#                 print(f"Buildable Footprint Area: {buildable_footprint_area}")
#                 print(f"Number of Floors: {num_floors}")
#                 print(f"New Footprint: {new_footprint}")

#                 # Plot the original lot and new footprint
#                 fig, ax = plt.subplots(figsize=(8, 8))
#                 lot_patch = plt.Polygon(lot_geometry.exterior.coords, fill=True, color='blue', alpha=0.5)
#                 ax.add_patch(lot_patch)
#                 if new_footprint and not new_footprint.is_empty:
#                     footprint_patch = plt.Polygon(new_footprint.exterior.coords, fill=True, color='red', alpha=0.5)
#                     ax.add_patch(footprint_patch)
#                 ax.set_title(f'Lot and New Footprint at Index {index}')
#                 ax.set_xlabel('X Coordinate')
#                 ax.set_ylabel('Y Coordinate')
#                 ax.set_aspect('equal')
#                 plt.show()

#         except Exception as e:
#             new_footprints.append(None)
#             num_floors_list.append(None)
#             print(f"Error generating new footprint for row {index}: {e}")

#     return new_footprints, num_floors_list

# Function to remove outliers
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Define the referenced functions

# Update the visualize_compliance_rates function
def visualize_compliance_rates(gdf, compliance_features, title):
    compliance_rates = {feature: gdf[feature].fillna(False).mean() for feature in compliance_features}

    plt.figure(figsize=(12, 6))
    bars = plt.bar(compliance_rates.keys(), compliance_rates.values())
    plt.title(f'Compliance Rates by Zoning Feature ({title})')
    plt.xlabel('Zoning Feature')
    plt.ylabel('Compliance Rate')
    plt.xticks(rotation=45, ha='right')

    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                 f'{height:.2%}',
                 ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

def visualize_top_clusters(filtered_gdf, N_CLUSTERS=4):
    """
    Identify and visualize top clusters based on proposed buildable area.

    :param filtered_gdf: GeoDataFrame with clustering results
    :param N_CLUSTERS: Number of top clusters to visualize
    """
    cluster_groups = filtered_gdf.groupby('cluster')['proposed_buildable_area'].sum().sort_values(ascending=False)
    top_clusters = cluster_groups.head(N_CLUSTERS).index.tolist()

    fig, axes = plt.subplots(2, 2, figsize=(20, 20))
    axes = axes.flatten()

    for i, cluster_id in enumerate(top_clusters):
        if i < N_CLUSTERS:
            cluster_gdf = filtered_gdf[filtered_gdf['cluster'] == cluster_id]
            plot_cluster(cluster_gdf, axes[i], f"Cluster {cluster_id}: {len(cluster_gdf)} lots")

    plt.tight_layout()
    plt.show()

    # Print summary for each top cluster
    for cluster_id in top_clusters:
        cluster_gdf = filtered_gdf[filtered_gdf['cluster'] == cluster_id]
        print(f"\nCluster {cluster_id}:")
        print(f"Number of lots in cluster: {len(cluster_gdf)}")
        print(f"Number of lots with proposed footprints: {cluster_gdf['proposed_footprint'].notna().sum()}")
        print(f"Total proposed buildable area: {cluster_gdf['proposed_buildable_area'].sum():.2f}")

# Update the visualize_far_comparison function
def visualize_far_comparison(gdf):
    plt.figure(figsize=(12, 6))
    plt.scatter(gdf['current_FAR'], gdf['proposed_FAR'], alpha=0.5)
    max_far = max(gdf['current_FAR'].max(), gdf['proposed_FAR'].max())
    plt.plot([0, max_far], [0, max_far], 'r--')
    plt.xlabel('Current FAR')
    plt.ylabel('Proposed FAR')
    plt.title('Current vs Proposed FAR')
    plt.tight_layout()
    plt.show()

    # Calculate and print summary statistics
    far_increase = gdf['proposed_FAR'] - gdf['current_FAR']
    print("\nFAR Increase Summary Statistics:")
    print(far_increase.describe())

    # Debug: Print additional information about FAR increases
    print(f"\nDebug - Number of lots with FAR increase: {(far_increase > 0).sum()}")
    print(f"Debug - Number of lots with no FAR change: {(far_increase == 0).sum()}")
    print(f"Debug - Number of lots with FAR decrease: {(far_increase < 0).sum()}")
    print(f"Debug - Maximum FAR increase: {far_increase.max()}")
    print(f"Debug - Minimum FAR increase: {far_increase.min()}")

# Update the plot_cluster function
def plot_cluster(cluster_gdf, ax, title):
    for _, sample_lot in cluster_gdf.iterrows():
        if sample_lot['geometry'] and not sample_lot['geometry'].is_empty:
            ax.add_patch(plt.Polygon(sample_lot['geometry'].exterior.coords, fill=False, edgecolor='black', linewidth=0.5))

        if isinstance(sample_lot['geometry'], (Polygon, MultiPolygon)) and not sample_lot['geometry'].is_empty:
            if isinstance(sample_lot['geometry'], MultiPolygon):
                for geom in sample_lot['geometry'].geoms:
                    ax.add_patch(plt.Polygon(geom.exterior.coords, fill=True, facecolor='blue', alpha=0.7))
            else:
                ax.add_patch(plt.Polygon(sample_lot['geometry'].exterior.coords, fill=True, facecolor='blue', alpha=0.7))

        if sample_lot['proposed_footprint'] is not None and not sample_lot['proposed_footprint'].is_empty:
            if isinstance(sample_lot['proposed_footprint'], (Polygon, MultiPolygon)):
                if isinstance(sample_lot['proposed_footprint'], MultiPolygon):
                    for geom in sample_lot['proposed_footprint'].geoms:
                        ax.add_patch(plt.Polygon(geom.exterior.coords, fill=True, facecolor='red', alpha=0.7))
                else:
                    ax.add_patch(plt.Polygon(sample_lot['proposed_footprint'].exterior.coords, fill=True, facecolor='red', alpha=0.7))

    x_coords = []
    y_coords = []
    for geom in cluster_gdf['geometry']:
        if geom and not geom.is_empty:
            x_coords.extend([coord[0] for coord in geom.exterior.coords])
            y_coords.extend([coord[1] for coord in geom.exterior.coords])
    if x_coords and y_coords:
        ax.set_xlim(min(x_coords), max(x_coords))
        ax.set_ylim(min(y_coords), max(y_coords))

    ax.set_title(title)
    ax.set_xlabel("X Coordinate")
    ax.set_ylabel("Y Coordinate")
    ax.set_aspect('equal', 'box')

def prioritize_sampling(gdf):
    """
    Prioritize sampling by selecting lots most likely to need a footprint to propose.
    Criteria include non-compliance with current FAR, larger lot areas, and those with existing structures.
    """
    # Filter out lots that are non-compliant with current regulations or have large lot areas and low built FAR
    non_compliant_gdf = gdf[(gdf['FAR_compliance_current'] == False) |
                            (gdf['height_compliance_current'] == False) |
                            (gdf['front_yard_compliance_current'] == False) |
                            (gdf['rear_yard_compliance_current'] == False)]

    # Sort by lot area (larger lots first) and built FAR (less built up first)
    prioritized_gdf = non_compliant_gdf.sort_values(by=['LotArea', 'BuiltFAR'], ascending=[False, True])

    return prioritized_gdf

# Main execution
if __name__ == "__main__":
    start_time = time.time()
    print("Starting zoning analysis...")

    # Specify borough code and community district
    borough_code = 4
    community_district = 409

    # Path to the 3DM file
    file_path = '/content/drive/MyDrive/NYC_3DModel_QN09.3dm'

    # Load Building Footprint geometries
    print(f"Loading building footprint data from {file_path}...")
    building_footprint_gdf = read_3dm(file_path, 'Building_FootPrint')
    building_footprint_gdf = building_footprint_gdf.set_crs(epsg=2263, allow_override=True)
    print(f"Loaded {len(building_footprint_gdf)} building footprints.")
    print(building_footprint_gdf.head())

    # Load Building Facade geometries
    print("Loading and processing building facade data...")
    building_facade_geometries = read_3dm_with_z(file_path, 'Building_Facade')
    building_heights = calculate_building_height(building_facade_geometries)
    print(f"Processed {len(building_heights)} building heights.")
    print(building_heights[:5])

    # Create GeoDataFrame for building facades with height
    print("Creating GeoDataFrame for building facades...")
    building_facade_gdf = gpd.GeoDataFrame({
        'geometry': [Polygon([point[:2] for point in geom]) for geom in building_facade_geometries],
        'height': building_heights
    })
    building_facade_gdf = building_facade_gdf.set_crs(epsg=2263, allow_override=True)
    print(building_facade_gdf.head())

    # Get the lowest segment for each facade
    print("Processing lowest segments for facades...")
    building_facade_gdf['lowest_segment'] = [get_lowest_segment(geom) for geom in tqdm(building_facade_gdf.geometry, desc="Processing facades")]
    building_facade_gdf = building_facade_gdf.dropna(subset=['lowest_segment'])
    building_facade_gdf = gpd.GeoDataFrame(building_facade_gdf, geometry='lowest_segment')
    print(f"Processed {len(building_facade_gdf)} facades with lowest segments.")
    print(building_facade_gdf.head())

    # Ensure CRS alignment
    building_facade_gdf = building_facade_gdf.set_crs(epsg=2263, allow_override=True)
    building_footprint_gdf = building_footprint_gdf.set_crs(epsg=2263, allow_override=True)

    # Create unique identifiers (if not already created)
    if 'footprint_id' not in building_footprint_gdf.columns:
        building_footprint_gdf['footprint_id'] = building_footprint_gdf.index

    if 'facade_id' not in building_facade_gdf.columns:
        building_facade_gdf['facade_id'] = building_facade_gdf.index

    # Merge building footprints with facades
    print("Merging building footprints with facades...")
    footprint_with_facade = gpd.sjoin(building_footprint_gdf, building_facade_gdf, how="left", predicate="intersects")
    print(f"Columns after spatial join: {footprint_with_facade.columns}")
    print(f"Shape after spatial join: {footprint_with_facade.shape}")

    footprint_with_facade['max_height'] = footprint_with_facade.groupby('footprint_id')['height'].transform('max')
    print(f"Columns after adding max_height: {footprint_with_facade.columns}")

    corrected_footprint_with_facade = footprint_with_facade[footprint_with_facade['height'] == footprint_with_facade['max_height']].drop_duplicates(subset=['footprint_id'])
    print(f"Columns after filtering and deduplication: {corrected_footprint_with_facade.columns}")
    print(f"Shape after filtering and deduplication: {corrected_footprint_with_facade.shape}")

    # Use 'geometry_left' as the geometry column
    corrected_footprint_with_facade = corrected_footprint_with_facade.set_geometry('geometry_left')
    print(f"Active geometry column: {corrected_footprint_with_facade.geometry.name}")
    print(f"Merged and corrected data shape: {corrected_footprint_with_facade.shape}")

    # Fetch MAPPLUTO data
    print(f"Fetching MAPPLUTO data for borough code {borough_code}, community district {community_district}...")
    mappluto_data = query_mappluto(borough_code, community_district)
    mappluto_gdf = gpd.GeoDataFrame.from_features(mappluto_data)
    mappluto_gdf.set_crs(epsg=2263, inplace=True)
    print(f"Fetched {len(mappluto_gdf)} MAPPLUTO records.")
    print(mappluto_gdf.head())

    print("\nMerging building footprints with MAPPLUTO data...")
    try:
        # Prepare data for spatial join
        corrected_footprint_with_facade = corrected_footprint_with_facade.set_crs(epsg=2263, allow_override=True)
        mappluto_gdf = mappluto_gdf.set_crs(epsg=2263, allow_override=True)

        # Remove 'index_right' column if it exists
        if 'index_right' in corrected_footprint_with_facade.columns:
            corrected_footprint_with_facade = corrected_footprint_with_facade.drop(columns=['index_right'])

        corrected_footprint_with_facade['footprint_id'] = corrected_footprint_with_facade.index if 'footprint_id' not in corrected_footprint_with_facade.columns else corrected_footprint_with_facade['footprint_id']
        mappluto_gdf['unique_lot_id'] = mappluto_gdf['Borough'] + '_' + mappluto_gdf['Block'].astype(str) + '_' + mappluto_gdf['Lot'].astype(str)
        mappluto_gdf = mappluto_gdf.drop_duplicates(subset='unique_lot_id')

        # Perform spatial join
        corrected_footprint_with_facade['centroid'] = corrected_footprint_with_facade.geometry.centroid
        centroid_footprint_gdf = corrected_footprint_with_facade.set_geometry('centroid')
        merged_gdf = gpd.sjoin(centroid_footprint_gdf, mappluto_gdf, how="left", predicate="within")

        # Reset geometry to original footprint geometry
        merged_gdf = merged_gdf.set_geometry('geometry_left')
        merged_gdf = merged_gdf.drop(columns=['centroid'])

        if 'index_right' in merged_gdf.columns:
            merged_gdf = merged_gdf.drop(columns=['index_right'])

        print("Shape of centroid_footprint_gdf:", centroid_footprint_gdf.shape)
        print("Shape of mappluto_gdf:", mappluto_gdf.shape)
        print("Shape of merged_gdf:", merged_gdf.shape)
        print("Columns in merged_gdf:", merged_gdf.columns)
        print("NaN values in merged_gdf:", merged_gdf.isna().sum())
        print("Empty geometries:", merged_gdf[merged_gdf.geometry.is_empty].shape[0])

        # Process merged data
        merged_gdf_filtered = merged_gdf.dropna(subset=['ZoneDist1'])
        match_percentage = (len(merged_gdf_filtered) / len(merged_gdf)) * 100
        print(f"Percentage of buildings successfully matched with MAPPLUTO data: {match_percentage:.2f}%")

        # Focus on R1-R5 zones
        merged_gdf_r1_r5 = merged_gdf_filtered[merged_gdf_filtered['ZoneDist1'].str.startswith(('R1', 'R2', 'R3', 'R4', 'R5'))].copy()
        print(f"Shape of merged_gdf after filtering for R1-R5 zones: {merged_gdf_r1_r5.shape}")

        # Ensure geometry column is properly set
        # Keep only 'geometry_left' and drop 'geometry_right'
        merged_gdf_r1_r5 = merged_gdf_r1_r5.drop(columns=['geometry_right'])
        merged_gdf_r1_r5 = merged_gdf_r1_r5.rename(columns={'geometry_left': 'geometry'})
        merged_gdf_r1_r5 = merged_gdf_r1_r5.set_geometry('geometry')
        merged_gdf_r1_r5 = merged_gdf_r1_r5.set_crs(epsg=2263, allow_override=True)

        print("Merge process completed successfully.")
        print(f"Active geometry column: {merged_gdf_r1_r5.geometry.name}")
        print(f"CRS of merged_gdf_r1_r5: {merged_gdf_r1_r5.crs}")

        # Save and visualize results
        merged_gdf_r1_r5.to_file("merged_building_data_improved.geojson", driver="GeoJSON")
        print("Merged data saved to merged_building_data_improved.geojson")

        fig, ax = plt.subplots(figsize=(15, 15))
        merged_gdf_r1_r5.plot(ax=ax, column='ZoneDist1', legend=True, cmap='Set3')
        plt.title('Filtered Building Footprints (R1-R5 Zones)')
        plt.xlabel('X Coordinate')
        plt.ylabel('Y Coordinate')
        unique_zones = merged_gdf_r1_r5['ZoneDist1'].unique()
        handles = [plt.Rectangle((0,0),1,1, color=plt.cm.Set3(i/len(unique_zones))) for i in range(len(unique_zones))]
        plt.legend(handles, unique_zones, title='Zoning District', loc='center left', bbox_to_anchor=(1, 0.5))
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"Error during merge process: {e}")
        print("Diagnostic information:")
        print(f"corrected_footprint_with_facade: {type(corrected_footprint_with_facade)}, CRS: {corrected_footprint_with_facade.crs}")
        print(f"mappluto_gdf: {type(mappluto_gdf)}, CRS: {mappluto_gdf.crs}")
        print(f"corrected_footprint_with_facade columns: {corrected_footprint_with_facade.columns}")
        print(f"mappluto_gdf columns: {mappluto_gdf.columns}")

    # Load zoning data
    sheet_url = "https://docs.google.com/spreadsheets/d/11-h0IW4Qe6XFJPEu4gZPIF0kELjMxeSrZ98FY9_F5rA/edit?usp=sharing"
    csv_export_url = sheet_url.replace('/edit?usp=sharing', '/export?format=csv')
    zoning_data = pd.read_csv(csv_export_url)
    zoning_data.columns = zoning_data.columns.str.strip()

    # Strip units from zoning data
    for col in zoning_data.columns:
        if col != 'Feature':
            zoning_data[col] = zoning_data[col].apply(strip_units)

    # Load proposed zoning standards
    proposed_standards_path = '/content/drive/MyDrive/proposed-r1-r5-zoning-standards.csv'
    proposed_standards = pd.read_csv(proposed_standards_path, index_col='Zone')

    # Strip units from proposed standards
    for col in proposed_standards.columns:
        proposed_standards[col] = proposed_standards[col].apply(strip_units)

    # Precompute yard buffers
    yard_buffers = precompute_yard_buffers(proposed_standards)

    # Perform compliance checks
    if 'merged_gdf_r1_r5' in locals():
        print("Performing compliance checks...")
        try:
            numeric_columns = ['BuiltFAR', 'LotArea', 'LotFront', 'LotDepth', 'BldgDepth', 'max_height', 'BldgArea']
            for col in numeric_columns:
                merged_gdf_r1_r5[col] = pd.to_numeric(merged_gdf_r1_r5[col], errors='coerce')

            merged_gdf_r1_r5 = merged_gdf_r1_r5.dropna(subset=['ZoneDist1', 'BuiltFAR', 'LotArea', 'LotFront', 'LotDepth', 'BldgDepth', 'max_height', 'BldgArea'])

            print(f"Shape of merged_gdf_r1_r5 after dropping NaNs: {merged_gdf_r1_r5.shape}")

            merged_gdf_r1_r5['FrontYardDepth'] = merged_gdf_r1_r5['LotDepth'] - merged_gdf_r1_r5['BldgDepth']
            merged_gdf_r1_r5['RearYardDepth'] = merged_gdf_r1_r5['FrontYardDepth']

            merged_gdf_r1_r5 = merged_gdf_r1_r5.join(merged_gdf_r1_r5.apply(lambda row: check_compliance(row, 'current'), axis=1))
            merged_gdf_r1_r5 = merged_gdf_r1_r5.join(merged_gdf_r1_r5.apply(lambda row: check_compliance(row, 'proposed'), axis=1))

            print(f"Shape of merged_gdf_r1_r5 after compliance checks: {merged_gdf_r1_r5.shape}")

            # Prioritize sampling
            prioritized_gdf = prioritize_sampling(merged_gdf_r1_r5)

            # Sample a very small subset for verification
            sample_size = 5  # Small sample size for testing
            sample_gdf = prioritized_gdf.head(sample_size)

            print(f"Sampled {sample_size} lots for verification.")
            print("Sampled DataFrame:")
            print(sample_gdf.head())

            print(f"Generating new footprints for a sample of {sample_size} lots...")
            new_footprints, num_floors = generate_new_footprints(sample_gdf, yard_buffers)

            sample_gdf['proposed_footprint'] = new_footprints
            sample_gdf['num_floors'] = num_floors

            # Debug: Check if proposed_footprint and num_floors are correctly assigned
            print("Sampled DataFrame after assigning proposed_footprint and num_floors:")
            print(sample_gdf[['proposed_footprint', 'num_floors']].head())

            sample_gdf['proposed_buildable_area'] = sample_gdf.apply(
                lambda row: row['proposed_footprint'].area * row['num_floors'] if row['proposed_footprint'] is not None else None,
                axis=1
            )

            sample_gdf['current_FAR'] = pd.to_numeric(sample_gdf['BuiltFAR'], errors='coerce')
            sample_gdf['proposed_FAR'] = pd.to_numeric(sample_gdf['proposed_buildable_area'], errors='coerce') / sample_gdf['LotArea']

            # Debug: Check if proposed_buildable_area is correctly calculated
            print("Sampled DataFrame after calculating proposed_buildable_area:")
            print(sample_gdf[['proposed_buildable_area']].head())

            sample_gdf = sample_gdf.replace([np.inf, -np.inf], np.nan).dropna(subset=['current_FAR', 'proposed_FAR'])

            print("First few rows of sample_gdf before calculations:")
            print(sample_gdf.head())

            print("\nInfo of sample_gdf before calculations:")
            print(sample_gdf.info())

            print("First few rows after calculating FARs:")
            print(sample_gdf[['current_FAR', 'proposed_FAR']].head())

            visualize_far_comparison(sample_gdf)

        except Exception as e:
            print(f"An error occurred during compliance checks or new footprint generation: {str(e)}")
            print("DataFrame columns:", sample_gdf.columns)
    else:
        print("Error: merged_gdf_r1_r5 is not defined. Skipping compliance checks and new footprint generation.")

    # Perform clustering analysis
    try:
        print("Performing clustering analysis...")
        if 'proposed_buildable_area' in merged_gdf_r1_r5.columns:
            cleaned_gdf = remove_outliers(merged_gdf_r1_r5, 'proposed_buildable_area')
            filtered_gdf = cleaned_gdf.dropna(subset=['proposed_buildable_area', 'geometry'])

            print(f"Shape of filtered_gdf before clustering: {filtered_gdf.shape}")

            # Ensure we have valid geometries
            filtered_gdf = filtered_gdf[filtered_gdf.geometry.is_valid]

            # Extract centroids
            centroids = filtered_gdf.geometry.centroid
            coords = np.array([(c.x, c.y) for c in centroids if c])

            print(f"Number of valid coordinates for clustering: {len(coords)}")

            if len(coords) > 0:
                # Perform DBSCAN clustering
                db = DBSCAN(eps=1000, min_samples=5).fit(coords)
                filtered_gdf['cluster'] = db.labels_
                print(f"Identified {len(set(filtered_gdf['cluster'])) - (1 if -1 in filtered_gdf['cluster'] else 0)} clusters.")
            else:
                print("No valid coordinates for clustering.")
                filtered_gdf = None
        else:
            print("Error: 'proposed_buildable_area' column not found. Skipping clustering analysis.")
            filtered_gdf = None
    except Exception as e:
        print(f"An error occurred during clustering analysis: {e}")
        print("DataFrame columns:", merged_gdf_r1_r5.columns)

    # Identify and visualize top clusters
    print("Visualizing top clusters...")
    if filtered_gdf is not None and 'cluster' in filtered_gdf.columns:
        visualize_top_clusters(filtered_gdf, N_CLUSTERS=4)
    else:
        print("Skipping top clusters visualization due to missing data or 'cluster' column.")

    # Compare current and proposed zoning
    print("Comparing current and proposed zoning...")
    merged_gdf_r1_r5['current_FAR'] = merged_gdf_r1_r5['BuiltFAR']
    merged_gdf_r1_r5['proposed_FAR'] = merged_gdf_r1_r5['proposed_buildable_area'] / merged_gdf_r1_r5['LotArea']

    # Remove rows with infinite or NaN values
    try:
        merged_gdf_r1_r5['current_FAR'] = pd.to_numeric(merged_gdf_r1_r5['current_FAR'], errors='coerce')
        merged_gdf_r1_r5['proposed_FAR'] = pd.to_numeric(merged_gdf_r1_r5['proposed_FAR'], errors='coerce')
        merged_gdf_r1_r5 = merged_gdf_r1_r5.replace([np.inf, -np.inf], np.nan).dropna(subset=['current_FAR', 'proposed_FAR'])
    except Exception as e:
        print(f"An error occurred during FAR calculation: {e}")

    visualize_far_comparison(merged_gdf_r1_r5)

    # Save results
    print("Saving results...")
    try:
        merged_gdf_r1_r5.to_file('/content/drive/MyDrive/zoning_compliance_analysis.geojson', driver='GeoJSON')
        print("\nResults saved to /content/drive/MyDrive/zoning_compliance_analysis.geojson")
    except Exception as e:
        print(f"An error occurred while saving results: {e}")

    end_time = time.time()
    print(f"\nAnalysis complete. Total execution time: {end_time - start_time:.2f} seconds.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from shapely.geometry import Polygon, MultiPolygon
from shapely.ops import unary_union
from joblib import Parallel, delayed

def calculate_buildable_area(row, proposed_standards):
    zone = row['ZoneDist1']
    if zone not in proposed_standards.index:
        return None, None

    lot_area = row['LotArea']
    proposed_far = proposed_standards.loc[zone, 'Residential FAR (max)']
    proposed_max_height = proposed_standards.loc[zone, 'Building Height (max)']

    max_floor_area = lot_area * proposed_far
    current_floor_area = row['BldgArea']
    remaining_floor_area = max(0, max_floor_area - current_floor_area)

    front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
    rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
    side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

    lot_width = row['LotFront']
    lot_depth = row['LotDepth']
    buildable_width = max(0, lot_width - 2 * side_yards)
    buildable_depth = max(0, lot_depth - front_yard - rear_yard)

    buildable_footprint = buildable_width * buildable_depth

    # Ensure the buildable footprint area does not exceed the remaining floor area allowed by FAR
    buildable_footprint_area = min(remaining_floor_area, buildable_footprint)

    # Determine the number of floors needed to utilize the remaining floor area
    if buildable_footprint_area == 0:
        num_floors = 0
    else:
        num_floors = min(
            proposed_max_height // 10,  # Assuming 10 feet per floor
            max(1, int(remaining_floor_area // buildable_footprint_area))
        )

    return buildable_footprint_area, num_floors

def process_row(row, proposed_standards, all_existing_footprints):
    try:
        zone = row['ZoneDist1']
        if zone not in proposed_standards.index:
            return None, None

        lot_geometry = row['geometry']
        buildable_footprint_area, num_floors = calculate_buildable_area(row, proposed_standards)

        if buildable_footprint_area is None or num_floors is None or buildable_footprint_area <= 0:
            return None, None

        front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
        rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
        side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

        # Subtract yard spaces from the lot geometry to define the buildable area
        buildable_area = lot_geometry.buffer(-front_yard - rear_yard - side_yards)

        # Subtract all existing footprints from the buildable area
        buildable_area = buildable_area.difference(all_existing_footprints)

        if buildable_area.is_empty or buildable_area.area <= 0:
            return None, None

        if isinstance(buildable_area, MultiPolygon):
            buildable_area = max(buildable_area.geoms, key=lambda p: p.area)

        new_footprint = buildable_area.buffer(0)

        if new_footprint.area > buildable_footprint_area:
            shrink_factor = np.sqrt(buildable_footprint_area / new_footprint.area)
            if np.isfinite(shrink_factor) and shrink_factor > 0:
                new_footprint = new_footprint.buffer(shrink_factor - 1, join_style=2)
            else:
                new_footprint = None

        return new_footprint, num_floors
    except Exception as e:
        print(f"Error processing row: {e}")
        return None, None

def generate_new_footprints(gdf, proposed_standards):
    all_existing_footprints = unary_union(gdf['geometry'])

    results = Parallel(n_jobs=-1, verbose=5)(
        delayed(process_row)(row, proposed_standards, all_existing_footprints) for _, row in gdf.iterrows()
    )

    new_footprints, num_floors_list = zip(*results)
    return new_footprints, num_floors_list

def calculate_profit_metric(row, construction_cost_per_sqft=200, market_value_per_sqft=300):
    if row['proposed_footprint'] is None or row['num_floors'] is None:
        return {
            'current_profit': 0,
            'proposed_profit': 0,
            'profit_impact': 0,
            'additional_area_impact': 0
        }

    proposed_buildable_area = row['proposed_footprint'].area * row['num_floors']
    additional_area = proposed_buildable_area

    construction_cost = additional_area * construction_cost_per_sqft
    market_value = additional_area * market_value_per_sqft

    profit = market_value - construction_cost

    return {
        'current_profit': 0,  # Assuming no changes under current zoning
        'proposed_profit': profit,
        'profit_impact': profit,
        'additional_area_impact': additional_area
    }

def apply_profit_metric(gdf):
    tqdm.pandas(desc="Calculating profit metrics")
    profit_metrics = gdf.progress_apply(calculate_profit_metric, axis=1)
    profit_df = pd.DataFrame(profit_metrics.tolist(), index=gdf.index)
    return gdf.join(profit_df)

def visualize_profit_impact(gdf):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

    gdf.plot(column='profit_impact', ax=ax1, legend=True, cmap='RdYlGn', legend_kwds={'label': 'Profit Impact ($)'})
    ax1.set_title('Profit Impact of City of Yes')

    gdf.plot(column='additional_area_impact', ax=ax2, legend=True, cmap='viridis', legend_kwds={'label': 'Additional Area Impact (sq ft)'})
    ax2.set_title('Additional Area Impact of City of Yes')

    plt.tight_layout()
    plt.show()

    print("City of Yes Impact Summary:")
    print(f"Average profit impact: ${gdf['profit_impact'].mean():,.2f}")
    print(f"Total profit impact: ${gdf['profit_impact'].sum():,.2f}")
    print(f"Average additional area impact: {gdf['additional_area_impact'].mean():,.2f} sq ft")
    print(f"Total additional area impact: {gdf['additional_area_impact'].sum():,.2f} sq ft")
    print(f"Number of lots with positive profit impact: {(gdf['profit_impact'] > 0).sum()}")
    print(f"Number of lots with negative profit impact: {(gdf['profit_impact'] < 0).sum()}")
    print(f"Number of lots with no profit impact: {(gdf['profit_impact'] == 0).sum()}")

# Main execution
print("Generating new footprints...")
new_footprints, num_floors = generate_new_footprints(merged_gdf_r1_r5, proposed_standards)

merged_gdf_r1_r5['proposed_footprint'] = new_footprints
merged_gdf_r1_r5['num_floors'] = num_floors

print("Calculating profit metrics...")
merged_gdf_r1_r5 = apply_profit_metric(merged_gdf_r1_r5)

print("Visualizing profit impact...")
visualize_profit_impact(merged_gdf_r1_r5)

print("Saving results with profit metrics...")
merged_gdf_r1_r5.to_file('/content/drive/MyDrive/city_of_yes_impact_analysis_geometric.geojson', driver='GeoJSON')

print("Analysis complete.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from shapely.geometry import Polygon, MultiPolygon

def calculate_redevelopment_potential(row, proposed_standards):
    zone = row['ZoneDist1']
    if zone not in proposed_standards.index:
        print(f"Zone {zone} not found in proposed standards.")
        return None, None

    lot_area = row['LotArea']
    proposed_far = proposed_standards.loc[zone, 'Residential FAR (max)']
    proposed_max_height = proposed_standards.loc[zone, 'Building Height (max)']

    max_floor_area = lot_area * proposed_far
    current_floor_area = row['BldgArea']

    print(f"Lot area: {lot_area:.2f} sq ft")
    print(f"Proposed FAR: {proposed_far:.2f}")
    print(f"Max floor area under proposed standards: {max_floor_area:.2f} sq ft")
    print(f"Current floor area: {current_floor_area:.2f} sq ft")

    front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
    rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
    side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

    lot_width = row['LotFront']
    lot_depth = row['LotDepth']
    buildable_width = max(0, lot_width - 2 * side_yards)
    buildable_depth = max(0, lot_depth - front_yard - rear_yard)

    print(f"Lot width: {lot_width:.2f} ft, Lot depth: {lot_depth:.2f} ft")
    print(f"Required setbacks - Front: {front_yard:.2f} ft, Rear: {rear_yard:.2f} ft, Side (each): {side_yards:.2f} ft")
    print(f"Buildable width: {buildable_width:.2f} ft, Buildable depth: {buildable_depth:.2f} ft")

    buildable_footprint = buildable_width * buildable_depth
    print(f"Maximum buildable footprint: {buildable_footprint:.2f} sq ft")

    # Determine the number of floors
    num_floors = min(
        proposed_max_height // 10,  # Assuming 10 feet per floor
        max(1, int(max_floor_area // buildable_footprint))
    )

    # Calculate the actual buildable area based on the number of floors
    actual_buildable_area = min(max_floor_area, buildable_footprint * num_floors)

    print(f"Number of floors: {num_floors}")
    print(f"Actual buildable area: {actual_buildable_area:.2f} sq ft")

    return buildable_footprint, num_floors, actual_buildable_area

def process_single_property_redevelopment(row, proposed_standards):
    try:
        zone = row['ZoneDist1']
        if zone not in proposed_standards.index:
            print(f"Zone {zone} not found in proposed standards.")
            return None, None, None

        lot_geometry = row['geometry']
        buildable_footprint, num_floors, actual_buildable_area = calculate_redevelopment_potential(row, proposed_standards)

        if buildable_footprint is None or num_floors is None or actual_buildable_area is None:
            print("Unable to calculate redevelopment potential.")
            return None, None, None

        front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
        rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
        side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

        # Create new footprint by applying setbacks
        new_footprint = lot_geometry.buffer(-front_yard - rear_yard - 2 * side_yards)
        print(f"New footprint area after setbacks: {new_footprint.area:.2f} sq ft")

        if new_footprint.is_empty or new_footprint.area <= 0:
            print("No buildable area remaining after applying setbacks.")
            return None, None, None

        if isinstance(new_footprint, MultiPolygon):
            new_footprint = max(new_footprint.geoms, key=lambda p: p.area)
            print(f"Largest buildable area polygon: {new_footprint.area:.2f} sq ft")

        # Adjust footprint if it exceeds the calculated buildable footprint
        if new_footprint.area > buildable_footprint:
            shrink_factor = np.sqrt(buildable_footprint / new_footprint.area)
            if np.isfinite(shrink_factor) and shrink_factor > 0:
                new_footprint = new_footprint.buffer(shrink_factor - 1, join_style=2)
                print(f"Shrunk new footprint to match buildable area: {new_footprint.area:.2f} sq ft")
            else:
                print("Unable to shrink new footprint to match buildable area.")
                return None, None, None

        return new_footprint, num_floors, actual_buildable_area
    except Exception as e:
        print(f"Error processing property: {e}")
        return None, None, None

def calculate_redevelopment_impact(row, new_footprint, num_floors, actual_buildable_area, construction_cost_per_sqft=200, market_value_per_sqft=300):
    if new_footprint is None or num_floors is None or actual_buildable_area is None:
        return {
            'current_value': 0,
            'redevelopment_cost': 0,
            'redeveloped_value': 0,
            'net_impact': 0
        }

    current_value = row['BldgArea'] * market_value_per_sqft
    redevelopment_cost = actual_buildable_area * construction_cost_per_sqft
    redeveloped_value = actual_buildable_area * market_value_per_sqft
    net_impact = redeveloped_value - redevelopment_cost - current_value

    return {
        'current_value': current_value,
        'redevelopment_cost': redevelopment_cost,
        'redeveloped_value': redeveloped_value,
        'net_impact': net_impact
    }

def visualize_redevelopment(row, new_footprint):
    fig, ax = plt.subplots(figsize=(10, 10))

    # Plot the lot geometry
    x, y = row['geometry'].exterior.xy
    ax.fill(x, y, alpha=0.5, fc='gray', ec='black', label='Lot')

    # Plot the existing building footprint
    x, y = row['geometry'].exterior.xy
    ax.fill(x, y, alpha=0.7, fc='blue', ec='black', label='Existing Building')

    # Plot the new footprint if it exists
    if new_footprint:
        x, y = new_footprint.exterior.xy
        ax.fill(x, y, alpha=0.7, fc='red', ec='black', label='Proposed Redevelopment')

    ax.set_aspect('equal')
    ax.set_title(f"Lot {row.name}: Existing (Blue) vs Proposed Redevelopment (Red)")
    ax.legend()
    plt.show()

# Main execution
print("Analyzing single property for redevelopment potential...")

# Select a single property (you may need to adjust this selection criteria)
single_property = merged_gdf_r1_r5.iloc[0]

print("Property details:")
print(single_property[['ZoneDist1', 'LotArea', 'BldgArea', 'NumFloors', 'LotFront', 'LotDepth']])

new_footprint, num_floors, actual_buildable_area = process_single_property_redevelopment(single_property, proposed_standards)

if new_footprint is not None:
    print(f"\nProposed redevelopment:")
    print(f"New footprint area: {new_footprint.area:.2f} sq ft")
    print(f"Number of floors: {num_floors}")
    print(f"Total buildable area: {actual_buildable_area:.2f} sq ft")

    impact = calculate_redevelopment_impact(single_property, new_footprint, num_floors, actual_buildable_area)
    print("\nRedevelopment Impact:")
    for key, value in impact.items():
        print(f"{key}: ${value:,.2f}")

    visualize_redevelopment(single_property, new_footprint)
else:
    print("\nNo redevelopment potential for this property under the proposed standards.")

print("\nAnalysis complete.")

import matplotlib.pyplot as plt
from shapely.geometry import box
import numpy as np

def calculate_buildable_area(row, proposed_standards):
    zone = row['ZoneDist1']
    lot_area = row['LotArea']
    lot_width = row['LotFront']
    lot_depth = row['LotDepth']

    proposed_far = proposed_standards.loc[zone, 'Residential FAR (max)']
    front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
    rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
    side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

    buildable_width = max(0, lot_width - 2 * side_yards)
    buildable_depth = max(0, lot_depth - front_yard - rear_yard)
    buildable_footprint = buildable_width * buildable_depth
    max_floor_area = lot_area * proposed_far

    print(f"Lot area: {lot_area:.2f} sq ft")
    print(f"Proposed FAR: {proposed_far:.2f}")
    print(f"Max floor area under proposed standards: {max_floor_area:.2f} sq ft")
    print(f"Lot width: {lot_width:.2f} ft, Lot depth: {lot_depth:.2f} ft")
    print(f"Required setbacks - Front: {front_yard:.2f} ft, Rear: {rear_yard:.2f} ft, Side (each): {side_yards:.2f} ft")
    print(f"Buildable width: {buildable_width:.2f} ft, Buildable depth: {buildable_depth:.2f} ft")
    print(f"Buildable footprint: {buildable_footprint:.2f} sq ft")

    return buildable_footprint, max_floor_area

def visualize_lot_with_annotations(row, proposed_standards):
    zone = row['ZoneDist1']
    lot_width = row['LotFront']
    lot_depth = row['LotDepth']

    front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
    rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
    side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

    fig, ax = plt.subplots(figsize=(12, 12))

    # Create lot rectangle
    lot = box(0, 0, lot_width, lot_depth)

    # Plot lot
    x, y = lot.exterior.xy
    ax.fill(x, y, alpha=0.3, fc='gray', ec='black')

    # Plot setback lines
    ax.axhline(y=front_yard, color='r', linestyle='--')
    ax.axhline(y=lot_depth - rear_yard, color='r', linestyle='--')
    ax.axvline(x=side_yards, color='r', linestyle='--')
    ax.axvline(x=lot_width - side_yards, color='r', linestyle='--')

    # Annotate dimensions
    ax.annotate(f'Lot Width: {lot_width:.2f} ft', (lot_width/2, -5), ha='center')
    ax.annotate(f'Lot Depth: {lot_depth:.2f} ft', (-5, lot_depth/2), va='center', rotation=90)

    # Annotate setbacks
    ax.annotate(f'Front Yard: {front_yard:.2f} ft', (lot_width/2, front_yard/2), ha='center', va='center')
    ax.annotate(f'Rear Yard: {rear_yard:.2f} ft', (lot_width/2, lot_depth - rear_yard/2), ha='center', va='center')
    ax.annotate(f'Side Yard: {side_yards:.2f} ft', (side_yards/2, lot_depth/2), ha='center', va='center', rotation=90)
    ax.annotate(f'Side Yard: {side_yards:.2f} ft', (lot_width - side_yards/2, lot_depth/2), ha='center', va='center', rotation=90)

    # Calculate and annotate buildable area
    buildable_width = max(0, lot_width - 2 * side_yards)
    buildable_depth = max(0, lot_depth - front_yard - rear_yard)
    buildable_area = buildable_width * buildable_depth
    ax.annotate(f'Buildable Area: {buildable_area:.2f} sq ft', (lot_width/2, lot_depth/2), ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))

    # Plot buildable area
    if buildable_area > 0:
        buildable_box = box(side_yards, front_yard, lot_width - side_yards, lot_depth - rear_yard)
        x, y = buildable_box.exterior.xy
        ax.fill(x, y, alpha=0.3, fc='green', ec='black')

    ax.set_xlim(-10, lot_width + 10)
    ax.set_ylim(-10, lot_depth + 10)
    ax.set_aspect('equal')
    ax.set_title(f"Lot Visualization: {zone} Zone")

    plt.show()

# Main execution
print("Analyzing and visualizing the lot...")

# Select a single property (you may need to adjust this selection criteria)
single_property = merged_gdf_r1_r5.iloc[0]

print("Property details:")
print(single_property[['ZoneDist1', 'LotArea', 'BldgArea', 'NumFloors', 'LotFront', 'LotDepth']])

buildable_footprint, max_floor_area = calculate_buildable_area(single_property, proposed_standards)

print(f"\nBuildable footprint: {buildable_footprint:.2f} sq ft")
print(f"Max floor area: {max_floor_area:.2f} sq ft")

visualize_lot_with_annotations(single_property, proposed_standards)

print("Analysis and visualization complete.")

def analyze_redevelopment_potential(row, proposed_standards, construction_cost_per_sqft=350, market_value_per_sqft=582):
    zone = row['ZoneDist1']
    lot_area = row['LotArea']
    current_floor_area = row['BldgArea']
    current_num_floors = row['NumFloors']

    proposed_far = proposed_standards.loc[zone, 'Residential FAR (max)']
    front_yard = proposed_standards.loc[zone, 'Front Yard Depth (min)']
    rear_yard = proposed_standards.loc[zone, 'Rear Yard Depth (min)']
    side_yards = proposed_standards.loc[zone, 'Side Yards (total width)'] / 2  # Assuming equal side yards

    lot_width = row['LotFront']
    lot_depth = row['LotDepth']

    buildable_width = max(0, lot_width - 2 * side_yards)
    buildable_depth = max(0, lot_depth - front_yard - rear_yard)
    buildable_footprint = buildable_width * buildable_depth
    max_floor_area = lot_area * proposed_far

    proposed_num_floors = min(3, max(1, int(max_floor_area / buildable_footprint)))
    proposed_floor_area = min(max_floor_area, buildable_footprint * proposed_num_floors)

    print(f"Current building: {current_floor_area:.2f} sq ft, {current_num_floors:.0f} floors")
    print(f"Proposed building: {proposed_floor_area:.2f} sq ft, {proposed_num_floors:.0f} floors")

    current_value = current_floor_area * market_value_per_sqft
    redevelopment_cost = proposed_floor_area * construction_cost_per_sqft
    redeveloped_value = proposed_floor_area * market_value_per_sqft

    net_value_change = redeveloped_value - redevelopment_cost - current_value

    print(f"\nCurrent building value: ${current_value:,.2f}")
    print(f"Redevelopment cost: ${redevelopment_cost:,.2f}")
    print(f"Redeveloped building value: ${redeveloped_value:,.2f}")
    print(f"Net value change: ${net_value_change:,.2f}")

    return {
        'current_floor_area': current_floor_area,
        'current_num_floors': current_num_floors,
        'proposed_floor_area': proposed_floor_area,
        'proposed_num_floors': proposed_num_floors,
        'net_value_change': net_value_change
    }

# Main execution
print("Analyzing redevelopment potential...")

single_property = merged_gdf_r1_r5.iloc[0]

print("Property details:")
print(single_property[['ZoneDist1', 'LotArea', 'BldgArea', 'NumFloors', 'LotFront', 'LotDepth']])

redevelopment_analysis = analyze_redevelopment_potential(single_property, proposed_standards)

print("\nRedevelopment potential analysis complete.")

import numpy as np
from scipy.optimize import newton

def npv(rate, cashflows):
    return np.sum(np.array(cashflows) / (1 + rate)**np.arange(len(cashflows)))

def irr(cashflows):
    return newton(lambda r: npv(r, cashflows), 0.1)

def analyze_redevelopment_potential_professional(row, proposed_standards, market_value_per_sqft=531,
                                                 construction_cost_per_sqft=500, land_value_per_sqft=55,
                                                 demolition_cost_per_sqft=60, holding_period_years=5,
                                                 discount_rate=0.12, appreciation_rate=0.04):
    # Current property details
    current_floor_area = row['BldgArea']
    lot_area = row['LotArea']

    # Proposed building details (from previous calculation)
    buildable_footprint = 1063.80  # from previous calculation
    proposed_floor_area = min(row['LotArea'] * proposed_standards.loc[row['ZoneDist1'], 'Residential FAR (max)'],
                              buildable_footprint * 2)  # Assuming 2 floors max

    # Costs
    land_value = lot_area * land_value_per_sqft
    demolition_cost = current_floor_area * demolition_cost_per_sqft
    construction_cost = proposed_floor_area * construction_cost_per_sqft
    total_development_cost = land_value + demolition_cost + construction_cost

    # Revenues
    current_value = current_floor_area * market_value_per_sqft
    redeveloped_value = proposed_floor_area * market_value_per_sqft

    # Cash flows
    initial_investment = -(total_development_cost)
    annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
    sale_price = redeveloped_value * (1 + appreciation_rate)**holding_period_years

    cashflows = [initial_investment] + [annual_noi] * (holding_period_years - 1) + [annual_noi + sale_price]

    # Calculate metrics
    npv_value = npv(discount_rate, cashflows)
    irr_value = irr(cashflows)
    equity_multiple = sum(cashflows) / abs(initial_investment)

    print(f"Current building value: ${current_value:,.2f}")
    print(f"Redevelopment cost: ${total_development_cost:,.2f}")
    print(f"Redeveloped building value: ${redeveloped_value:,.2f}")
    print(f"\nNet Present Value: ${npv_value:,.2f}")
    print(f"Internal Rate of Return: {irr_value:.2%}")
    print(f"Equity Multiple: {equity_multiple:.2f}")

    return {
        'current_value': current_value,
        'redevelopment_cost': total_development_cost,
        'redeveloped_value': redeveloped_value,
        'npv': npv_value,
        'irr': irr_value,
        'equity_multiple': equity_multiple
    }

# Main execution
print("Analyzing redevelopment potential professionally...")

single_property = merged_gdf_r1_r5.iloc[0]

print("Property details:")
print(single_property[['ZoneDist1', 'LotArea', 'BldgArea', 'NumFloors', 'LotFront', 'LotDepth']])

professional_analysis = analyze_redevelopment_potential_professional(single_property, proposed_standards)

print("\nProfessional redevelopment potential analysis complete.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import newton
from tqdm import tqdm
import re
import warnings
from joblib import Parallel, delayed
import multiprocessing

def strip_units(value):
    if isinstance(value, str):
        return re.sub(r'[^\d.]+', '', value)
    return value

def load_current_standards(sheet_url):
    csv_export_url = sheet_url.replace('/edit?usp=sharing', '/export?format=csv')
    current_standards = pd.read_csv(csv_export_url, index_col=0)

    # Strip units from current standards
    for col in current_standards.columns:
        current_standards[col] = current_standards[col].apply(strip_units)

    return current_standards

# URL of the updated Google Sheets document
current_standards_url = "https://docs.google.com/spreadsheets/d/11-h0IW4Qe6XFJPEu4gZPIF0kELjMxeSrZ98FY9_F5rA/edit?usp=sharing"

# Load the updated current standards
current_standards = load_current_standards(current_standards_url)

print("Updated Current Standards:")
print(current_standards.head())

def extract_current_standards(zoning_data):
    features = [
        'Residential FAR (max)',
        'Building height (max)',
        'Front yard depth (min)',
        'Rear yard depth (min)',
        'Side yards (total width)'
    ]

    current_standards = {}

    for zone in zoning_data.columns[1:]:
        zone_standards = {}
        for feature in features:
            value = zoning_data.loc[zoning_data['Feature'] == feature, zone].values
            if len(value) > 0 and not pd.isna(value[0]):
                zone_standards[feature] = float(strip_units(value[0]))
            else:
                zone_standards[feature] = np.nan
        current_standards[zone] = zone_standards

    current_standards_df = pd.DataFrame.from_dict(current_standards, orient='index')

    current_standards_df = current_standards_df.rename(columns={
        'Residential FAR (max)': 'Residential FAR (max)',
        'Building height (max)': 'Building Height (max)',
        'Front yard depth (min)': 'Front Yard Depth (min)',
        'Rear yard depth (min)': 'Rear Yard Depth (min)',
        'Side yards (total width)': 'Side Yards (total width)'
    })

    return current_standards_df

warnings.filterwarnings("ignore", category=RuntimeWarning)

def safe_npv(rate, cashflows):
    with np.errstate(all='ignore'):
        return np.sum(cashflows / (1 + rate)**np.arange(cashflows.shape[1]), axis=1)

def safe_irr(cashflows):
    with np.errstate(all='ignore'):
        return np.apply_along_axis(lambda x: np.irr(x) if np.any(x != 0) else np.nan, 1, cashflows)

def calculate_max_floors(max_height, floor_height):
    return np.maximum(1, (max_height / floor_height).astype(int))

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

def precompute_zoning_data(gdf, standards):
    zones = gdf['ZoneDist1'].unique()
    zoning_data = {}
    for feature in ['Residential FAR (max)', 'Front Yard Depth (min)', 'Rear Yard Depth (min)', 'Side Yards (total width)', 'Building Height (max)']:
        zoning_data[feature] = {zone: float(get_zoning_regulation(zone, feature, standards)) for zone in zones}
    return zoning_data

def analyze_redevelopment_potential(gdf, zoning_data, params):
    try:
        proposed_far = gdf['ZoneDist1'].map(zoning_data['Residential FAR (max)'])
        front_yard = gdf['ZoneDist1'].map(zoning_data['Front Yard Depth (min)'])
        rear_yard = gdf['ZoneDist1'].map(zoning_data['Rear Yard Depth (min)'])
        side_yards = gdf['ZoneDist1'].map(zoning_data['Side Yards (total width)']) / 2
        max_height = gdf['ZoneDist1'].map(zoning_data['Building Height (max)'])

        buildable_width = np.maximum(0, gdf['LotFront'].values - 2 * side_yards.values)
        buildable_depth = np.maximum(0, gdf['LotDepth'].values - front_yard.values - rear_yard.values)
        buildable_footprint = buildable_width * buildable_depth

        max_floors = calculate_max_floors(max_height.values, params['floor_height'])
        proposed_floor_area = np.minimum(gdf['LotArea'].values * proposed_far.values, buildable_footprint * max_floors)

        land_value = gdf['LotArea'].values * params['land_value_per_sqft']
        demolition_cost = gdf['BldgArea'].values * params['demolition_cost_per_sqft']
        construction_cost = proposed_floor_area * params['construction_cost_per_sqft']
        total_development_cost = land_value + demolition_cost + construction_cost

        current_value = gdf['BldgArea'].values * params['market_value_per_sqft']
        redeveloped_value = proposed_floor_area * params['market_value_per_sqft']

        equity_investment = total_development_cost * (1 - params['debt_ratio'])
        debt = total_development_cost * params['debt_ratio']

        annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
        annual_debt_service = debt * (params['interest_rate'] * (1 + params['interest_rate'])**params['holding_period_years']) / ((1 + params['interest_rate'])**params['holding_period_years'] - 1)

        sale_price = redeveloped_value * (1 + params['appreciation_rate'])**params['holding_period_years']

        cashflows = np.column_stack((-equity_investment,
                                     np.tile(annual_noi - annual_debt_service, (params['holding_period_years'] - 1)),
                                     annual_noi - annual_debt_service + sale_price - debt))

        npv_value = safe_npv(params['discount_rate'], cashflows)
        irr_value = safe_irr(cashflows)
        equity_multiple = np.sum(cashflows, axis=1) / equity_investment

        return pd.DataFrame({
            'current_value': current_value,
            'redevelopment_cost': total_development_cost,
            'redeveloped_value': redeveloped_value,
            'npv': npv_value,
            'irr': irr_value,
            'equity_multiple': equity_multiple,
            'proposed_floor_area': proposed_floor_area,
            'max_floors': max_floors
        })
    except Exception as e:
        print(f"Error in analyze_redevelopment_potential: {e}")
        return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=gdf.index)

def run_single_simulation(gdf, current_zoning_data, proposed_zoning_data, params):
    current_sim = analyze_redevelopment_potential(gdf, current_zoning_data, params)
    proposed_sim = analyze_redevelopment_potential(gdf, proposed_zoning_data, params)
    return current_sim, proposed_sim

def run_monte_carlo_analysis(gdf, current_standards, proposed_standards, num_simulations=100):
    current_zoning_data = precompute_zoning_data(gdf, current_standards)
    proposed_zoning_data = precompute_zoning_data(gdf, proposed_standards)

    params_list = [sample_parameters(len(gdf)) for _ in range(num_simulations)]

    num_cores = multiprocessing.cpu_count()
    results = Parallel(n_jobs=num_cores)(delayed(run_single_simulation)(gdf, current_zoning_data, proposed_zoning_data, params)
                                         for params in tqdm(params_list, desc="Running Monte Carlo simulations"))

    results_current, results_proposed = zip(*results)
    return results_current, results_proposed

def analyze_monte_carlo_results(results_current, results_proposed):
    metrics = ['npv', 'irr', 'equity_multiple']
    summary = {}

    for standard, results in [('current', results_current), ('proposed', results_proposed)]:
        summary[standard] = {}
        results_array = np.array([df.values for df in results])

        for i, metric in enumerate(metrics):
            metric_results = results_array[:, :, i].T
            summary[standard][metric] = {
                'mean': np.nanmean(metric_results, axis=1),
                'median': np.nanmedian(metric_results, axis=1),
                'std': np.nanstd(metric_results, axis=1),
                '5th_percentile': np.nanpercentile(metric_results, 5, axis=1),
                '95th_percentile': np.nanpercentile(metric_results, 95, axis=1)
            }

    return summary

def run_analysis(gdf, current_standards, proposed_standards):
    results_current = []
    results_proposed = []

    for _, row in tqdm(gdf.iterrows(), total=len(gdf)):
        results_current.append(analyze_redevelopment_potential(row, 'current'))
        results_proposed.append(analyze_redevelopment_potential(row, 'proposed'))

    gdf['current_analysis'] = results_current
    gdf['proposed_analysis'] = results_proposed

    return gdf

def calculate_metrics(gdf):
    current_viable = ((gdf['current_analysis'].apply(lambda x: x['irr']) >= 0.15) &
                      (gdf['current_analysis'].apply(lambda x: x['equity_multiple']) >= 1.5))
    proposed_viable = ((gdf['proposed_analysis'].apply(lambda x: x['irr']) >= 0.15) &
                       (gdf['proposed_analysis'].apply(lambda x: x['equity_multiple']) >= 1.5))

    print(f"Number of viable properties under current standards: {current_viable.sum()}")
    print(f"Number of viable properties under proposed standards: {proposed_viable.sum()}")
    print(f"Net increase in viable properties: {proposed_viable.sum() - current_viable.sum()}")

    return current_viable, proposed_viable

def visualize_results(gdf, current_viable, proposed_viable):
    fig, axes = plt.subplots(2, 2, figsize=(20, 20))

    # IRR Comparison
    axes[0, 0].scatter(gdf['current_analysis'].apply(lambda x: x['irr']),
                       gdf['proposed_analysis'].apply(lambda x: x['irr']),
                       alpha=0.5)
    axes[0, 0].set_xlabel('Current IRR')
    axes[0, 0].set_ylabel('Proposed IRR')
    axes[0, 0].set_title('IRR Comparison')
    axes[0, 0].plot([0, 1], [0, 1], 'r--')

    # Equity Multiple Comparison
    axes[0, 1].scatter(gdf['current_analysis'].apply(lambda x: x['equity_multiple']),
                       gdf['proposed_analysis'].apply(lambda x: x['equity_multiple']),
                       alpha=0.5)
    axes[0, 1].set_xlabel('Current Equity Multiple')
    axes[0, 1].set_ylabel('Proposed Equity Multiple')
    axes[0, 1].set_title('Equity Multiple Comparison')
    axes[0, 1].plot([0, 3], [0, 3], 'r--')

    # Floor Area Comparison
    axes[1, 0].scatter(gdf['BldgArea'],
                       gdf['proposed_analysis'].apply(lambda x: x['proposed_floor_area']),
                       alpha=0.5)
    axes[1, 0].set_xlabel('Current Floor Area')
    axes[1, 0].set_ylabel('Proposed Floor Area')
    axes[1, 0].set_title('Floor Area Comparison')
    axes[1, 0].plot([0, 10000], [0, 10000], 'r--')

    # Map of Viable Properties
    gdf.plot(ax=axes[1, 1], color='gray', alpha=0.5)
    gdf[current_viable].plot(ax=axes[1, 1], color='blue', alpha=0.5, label='Viable under current standards')
    gdf[proposed_viable].plot(ax=axes[1, 1], color='red', alpha=0.5, label='Viable under proposed standards')
    axes[1, 1].set_title('Map of Viable Properties')
    axes[1, 1].legend()

    plt.tight_layout()
    plt.show()

# Main execution
print("Running Monte Carlo analysis...")
results_current, results_proposed = run_monte_carlo_analysis(merged_gdf_r1_r5, 'current', 'proposed', num_simulations=1000)

print("Analyzing Monte Carlo results...")
monte_carlo_summary = analyze_monte_carlo_results(results_current, results_proposed)

print("Monte Carlo analysis complete. Summary of results:")
for standard in ['current', 'proposed']:
    print(f"\n{standard.capitalize()} Standards:")
    for metric in ['npv', 'irr', 'equity_multiple']:
        print(f"  {metric.upper()}:")
        for stat, value in monte_carlo_summary[standard][metric].items():
            print(f"    {stat}: Mean = {np.nanmean(value):.4f}, Median = {np.nanmedian(value):.4f}")

print("\nAnalysis complete.")

# Main execution
print("Running analysis...")
gdf_with_analysis = run_analysis(merged_gdf_r1_r5, 'current', 'proposed')

print("Calculating metrics...")
current_viable, proposed_viable = calculate_metrics(gdf_with_analysis)

print("Visualizing results...")
visualize_results(gdf_with_analysis, current_viable, proposed_viable)

print("Analysis complete.")

!pip install numpy_financial

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import warnings
from joblib import Parallel, delayed
import multiprocessing
import numpy_financial as npf

def strip_units(value):
    if isinstance(value, str):
        return re.sub(r'[^\d.]+', '', value)
    return value

def load_current_standards(sheet_url):
    csv_export_url = sheet_url.replace('/edit?usp=sharing', '/export?format=csv')
    current_standards = pd.read_csv(csv_export_url, index_col=0)

    # Strip units from current standards
    for col in current_standards.columns:
        current_standards[col] = current_standards[col].apply(strip_units)

    return current_standards

# URL of the updated Google Sheets document
current_standards_url = "https://docs.google.com/spreadsheets/d/11-h0IW4Qe6XFJPEu4gZPIF0kELjMxeSrZ98FY9_F5rA/edit?usp=sharing"

# Load the updated current standards
current_standards = load_current_standards(current_standards_url)

# Load proposed zoning standards
proposed_standards_path = '/content/drive/MyDrive/proposed-r1-r5-zoning-standards.csv'
proposed_standards = pd.read_csv(proposed_standards_path, index_col='Zone')

# Strip units from proposed standards
for col in proposed_standards.columns:
    proposed_standards[col] = proposed_standards[col].apply(strip_units)

print("Updated Current Standards:")
print(current_standards.head())

warnings.filterwarnings("ignore", category=RuntimeWarning)

def safe_npv(rate, cashflows):
    with np.errstate(all='ignore'):
        return np.sum(cashflows / (1 + rate)**np.arange(cashflows.shape[1]), axis=1)

def safe_irr(cashflows):
    with np.errstate(all='ignore'):
        return np.apply_along_axis(lambda x: npf.irr(x) if np.any(x != 0) else np.nan, 1, cashflows)

def calculate_max_floors(max_height, floor_height):
    return np.maximum(1, (max_height / floor_height).astype(int))

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

# Updated get_zoning_regulation function
def get_zoning_regulation(zone, feature, standards, standards_type='current'):
    if pd.isna(zone):
        return np.nan
    try:
        if standards_type == 'current':
            if feature == 'Residential FAR (max)':
                value = standards.loc[standards.index == 'Residential FAR (max)', zone].values
            elif feature == 'Building Height (max)':
                value = standards.loc[standards.index == 'Building height (max)', zone].values
            elif feature == 'Rear Yard Depth (min)':
                value = standards.loc[standards.index == 'Rear yard depth (min)', zone].values
            elif feature == 'Front Yard Depth (min)':
                value = standards.loc[standards.index == 'Front yard depth (min)', zone].values
            elif feature == 'Side Yards (total width)':
                value = standards.loc[standards.index.str.contains('total width of side yards', case=False), zone].values

            if len(value) > 0 and not pd.isna(value[0]):
                return float(strip_units(value[0]))  # Strip units and convert to float
        elif standards_type == 'proposed':
            if zone in standards.index and feature in standards.columns:
                return float(strip_units(standards.loc[zone, feature]))  # Strip units and convert to float
        return np.nan
    except KeyError:
        return np.nan

# Function to check compliance with zoning regulations
def check_compliance(row, standards, standards_type):
    try:
        zone = row['ZoneDist1']
        results = {}
        for feature, regulation in [('FAR', 'Residential FAR (max)'),
                                    ('height', 'Building Height (max)'),
                                    ('front_yard', 'Front Yard Depth (min)'),
                                    ('rear_yard', 'Rear Yard Depth (min)')]:
            standard = get_zoning_regulation(zone, regulation, standards, standards_type)
            if pd.notnull(standard):
                if feature == 'FAR':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['BuiltFAR']) <= standard
                elif feature == 'height':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['max_height']) <= standard
                elif feature == 'front_yard':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['FrontYardDepth']) >= standard
                elif feature == 'rear_yard':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['RearYardDepth']) >= standard
            else:
                results[f'{feature}_compliance_{standards_type}'] = False
        return pd.Series(results)
    except Exception as e:
        print(f"Error checking compliance for row {row.name}: {e}")
        return pd.Series({f'{feature}_compliance_{standards_type}': False for feature in ['FAR', 'height', 'front_yard', 'rear_yard']})

def precompute_zoning_data(gdf, standards, standards_type):
    zones = gdf['ZoneDist1'].unique()
    zoning_data = {}
    for feature in ['Residential FAR (max)', 'Front Yard Depth (min)', 'Rear Yard Depth (min)', 'Side Yards (total width)', 'Building Height (max)']:
        zoning_data[feature] = {zone: get_zoning_regulation(zone, feature, standards, standards_type) for zone in zones}
    return zoning_data

def analyze_redevelopment_potential_single(gdf, zoning_data, params):
    try:
        proposed_far = zoning_data['Residential FAR (max)'][gdf['ZoneDist1']]
        front_yard = zoning_data['Front Yard Depth (min)'][gdf['ZoneDist1']] if zoning_data['Front Yard Depth (min)'][gdf['ZoneDist1']] is not None else 0
        rear_yard = zoning_data['Rear Yard Depth (min)'][gdf['ZoneDist1']] if zoning_data['Rear Yard Depth (min)'][gdf['ZoneDist1']] is not None else 0
        side_yards = zoning_data['Side Yards (total width)'][gdf['ZoneDist1']] if zoning_data['Side Yards (total width)'][gdf['ZoneDist1']] is not None else 0 / 2
        max_height = zoning_data['Building Height (max)'][gdf['ZoneDist1']] if zoning_data['Building Height (max)'][gdf['ZoneDist1']] is not None else np.inf

        buildable_width = max(0, gdf['LotFront'] - 2 * side_yards)
        buildable_depth = max(0, gdf['LotDepth'] - front_yard - rear_yard)
        buildable_footprint = buildable_width * buildable_depth

        max_floors = calculate_max_floors(max_height, params['floor_height'])[0]
        proposed_floor_area = min(gdf['LotArea'] * proposed_far, buildable_footprint * max_floors)

        land_value = gdf['LotArea'] * params['land_value_per_sqft'][0]
        demolition_cost = gdf['BldgArea'] * params['demolition_cost_per_sqft'][0]
        construction_cost = proposed_floor_area * params['construction_cost_per_sqft'][0]
        total_development_cost = land_value + demolition_cost + construction_cost

        current_value = gdf['BldgArea'] * params['market_value_per_sqft'][0]
        redeveloped_value = proposed_floor_area * params['market_value_per_sqft'][0]

        equity_investment = total_development_cost * (1 - params['debt_ratio'][0])
        debt = total_development_cost * params['debt_ratio'][0]

        annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
        annual_debt_service = debt * (params['interest_rate'][0] * (1 + params['interest_rate'][0])**params['holding_period_years'][0]) / ((1 + params['interest_rate'][0])**params['holding_period_years'][0] - 1)

        sale_price = redeveloped_value * (1 + params['appreciation_rate'][0])**params['holding_period_years'][0]

        print("Intermediate values for debugging:")
        print("Proposed FAR:", proposed_far)
        print("Front Yard Depth:", front_yard)
        print("Rear Yard Depth:", rear_yard)
        print("Side Yards:", side_yards)
        print("Max Height:", max_height)
        print("Buildable Width:", buildable_width)
        print("Buildable Depth:", buildable_depth)
        print("Buildable Footprint:", buildable_footprint)
        print("Max Floors:", max_floors)
        print("Proposed Floor Area:", proposed_floor_area)
        print("Land Value:", land_value)
        print("Demolition Cost:", demolition_cost)
        print("Construction Cost:", construction_cost)
        print("Total Development Cost:", total_development_cost)
        print("Current Value:", current_value)
        print("Redeveloped Value:", redeveloped_value)
        print("Equity Investment:", equity_investment)
        print("Debt:", debt)
        print("Annual NOI:", annual_noi)
        print("Annual Debt Service:", annual_debt_service)
        print("Sale Price:", sale_price)

        cashflows = np.column_stack((
            -equity_investment,
            np.tile(annual_noi - annual_debt_service, (params['holding_period_years'][0] - 1, 1)).T,
            (annual_noi - annual_debt_service + sale_price - debt).reshape(-1, 1)
        ))

        if np.any(np.isinf(cashflows)) or np.any(np.isnan(cashflows)):
            print("Cashflows contain infs or NaNs")
            return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

        npv_value = safe_npv(params['discount_rate'][0], cashflows)
        irr_value = safe_irr(cashflows)
        equity_multiple = np.sum(cashflows, axis=1) / equity_investment

        return pd.DataFrame({
            'current_value': current_value,
            'redevelopment_cost': total_development_cost,
            'redeveloped_value': redeveloped_value,
            'npv': npv_value,
            'irr': irr_value,
            'equity_multiple': equity_multiple,
            'proposed_floor_area': proposed_floor_area,
            'max_floors': max_floors
        }, index=[gdf.name])
    except Exception as e:
        print(f"Error in analyze_redevelopment_potential_single: {e}")
        return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

# Main execution
print("Loading merged_gdf_r1_r5 for debugging...")

# Assuming merged_gdf_r1_r5 is already loaded as a GeoDataFrame with the necessary columns
# Selecting random properties for debugging
random_properties_gdf = merged_gdf_r1_r5.sample(n=5, random_state=1)

print("Precomputing zoning data...")
current_zoning_data = precompute_zoning_data(random_properties_gdf, current_standards, standards_type='current')
proposed_zoning_data = precompute_zoning_data(random_properties_gdf, proposed_standards, standards_type='proposed')

print("Sampling parameters...")
params = sample_parameters(1)

results = []

for idx, row in random_properties_gdf.iterrows():
    print(f"\nAnalyzing property {idx} with current standards...")
    current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
    print(current_sim)

    print(f"\nAnalyzing property {idx} with proposed standards...")
    proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
    print(proposed_sim)

    results.append((current_sim, proposed_sim))

print("Analysis complete.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import warnings
from joblib import Parallel, delayed
import multiprocessing
import numpy_financial as npf
from tqdm import tqdm
import numpy_financial

def strip_units(value):
    if isinstance(value, str):
        return re.sub(r'[^\d.]+', '', value)
    return value

def load_current_standards(sheet_url):
    csv_export_url = sheet_url.replace('/edit?usp=sharing', '/export?format=csv')
    current_standards = pd.read_csv(csv_export_url, index_col=0)

    # Strip units from current standards
    for col in current_standards.columns:
        current_standards[col] = current_standards[col].apply(strip_units)

    return current_standards

# URL of the updated Google Sheets document
current_standards_url = "https://docs.google.com/spreadsheets/d/11-h0IW4Qe6XFJPEu4gZPIF0kELjMxeSrZ98FY9_F5rA/edit?usp=sharing"

# Load the updated current standards
current_standards = load_current_standards(current_standards_url)

# Load proposed zoning standards
proposed_standards_path = '/content/drive/MyDrive/proposed-r1-r5-zoning-standards.csv'
proposed_standards = pd.read_csv(proposed_standards_path, index_col='Zone')

# Strip units from proposed standards
for col in proposed_standards.columns:
    proposed_standards[col] = proposed_standards[col].apply(strip_units)

print("Updated Current Standards:")
print(current_standards.head())

warnings.filterwarnings("ignore", category=RuntimeWarning)

def safe_npv(rate, cashflows):
    with np.errstate(all='ignore'):
        return np.sum(cashflows / (1 + rate)**np.arange(cashflows.shape[1]), axis=1)

def safe_irr(cashflows):
    with np.errstate(all='ignore'):
        return np.apply_along_axis(lambda x: npf.irr(x) if np.any(x != 0) else np.nan, 1, cashflows)

def calculate_max_floors(max_height, floor_height):
    return np.maximum(1, (max_height / floor_height).astype(int))

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

# Updated get_zoning_regulation function
def get_zoning_regulation(zone, feature, standards, standards_type='current'):
    if pd.isna(zone):
        return np.nan
    try:
        if standards_type == 'current':
            if feature == 'Residential FAR (max)':
                value = standards.loc[standards.index == 'Residential FAR (max)', zone].values
            elif feature == 'Building Height (max)':
                value = standards.loc[standards.index == 'Building height (max)', zone].values
            elif feature == 'Rear Yard Depth (min)':
                value = standards.loc[standards.index == 'Rear yard depth (min)', zone].values
            elif feature == 'Front Yard Depth (min)':
                value = standards.loc[standards.index == 'Front yard depth (min)', zone].values
            elif feature == 'Side Yards (total width)':
                value = standards.loc[standards.index.str.contains('total width of side yards', case=False), zone].values

            if len(value) > 0 and not pd.isna(value[0]):
                return float(strip_units(value[0]))  # Strip units and convert to float
        elif standards_type == 'proposed':
            if zone in standards.index and feature in standards.columns:
                return float(strip_units(standards.loc[zone, feature]))  # Strip units and convert to float
        return np.nan
    except KeyError:
        return np.nan

# Function to check compliance with zoning regulations
def check_compliance(row, standards, standards_type):
    try:
        zone = row['ZoneDist1']
        results = {}
        for feature, regulation in [('FAR', 'Residential FAR (max)'),
                                    ('height', 'Building Height (max)'),
                                    ('front_yard', 'Front Yard Depth (min)'),
                                    ('rear_yard', 'Rear Yard Depth (min)')]:
            standard = get_zoning_regulation(zone, regulation, standards, standards_type)
            if pd.notnull(standard):
                if feature == 'FAR':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['BuiltFAR']) <= standard
                elif feature == 'height':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['max_height']) <= standard
                elif feature == 'front_yard':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['FrontYardDepth']) >= standard
                elif feature == 'rear_yard':
                    results[f'{feature}_compliance_{standards_type}'] = float(row['RearYardDepth']) >= standard
            else:
                results[f'{feature}_compliance_{standards_type}'] = False
        return pd.Series(results)
    except Exception as e:
        print(f"Error checking compliance for row {row.name}: {e}")
        return pd.Series({f'{feature}_compliance_{standards_type}': False for feature in ['FAR', 'height', 'front_yard', 'rear_yard']})

def precompute_zoning_data(gdf, standards, standards_type):
    zones = gdf['ZoneDist1'].unique()
    zoning_data = {}
    for feature in ['Residential FAR (max)', 'Front Yard Depth (min)', 'Rear Yard Depth (min)', 'Side Yards (total width)', 'Building Height (max)']:
        zoning_data[feature] = {zone: get_zoning_regulation(zone, feature, standards, standards_type) for zone in zones}
    return zoning_data

def analyze_redevelopment_potential_single(gdf, zoning_data, params):
    try:
        proposed_far = zoning_data['Residential FAR (max)'][gdf['ZoneDist1']]
        front_yard = zoning_data['Front Yard Depth (min)'][gdf['ZoneDist1']] if zoning_data['Front Yard Depth (min)'][gdf['ZoneDist1']] is not None else 0
        rear_yard = zoning_data['Rear Yard Depth (min)'][gdf['ZoneDist1']] if zoning_data['Rear Yard Depth (min)'][gdf['ZoneDist1']] is not None else 0
        side_yards = zoning_data['Side Yards (total width)'][gdf['ZoneDist1']] if zoning_data['Side Yards (total width)'][gdf['ZoneDist1']] is not None else 0 / 2
        max_height = zoning_data['Building Height (max)'][gdf['ZoneDist1']] if zoning_data['Building Height (max)'][gdf['ZoneDist1']] is not None else np.inf

        buildable_width = max(0, gdf['LotFront'] - 2 * side_yards)
        buildable_depth = max(0, gdf['LotDepth'] - front_yard - rear_yard)
        buildable_footprint = buildable_width * buildable_depth

        max_floors = calculate_max_floors(max_height, params['floor_height'])[0]
        proposed_floor_area = min(gdf['LotArea'] * proposed_far, buildable_footprint * max_floors)

        land_value = gdf['LotArea'] * params['land_value_per_sqft'][0]
        demolition_cost = gdf['BldgArea'] * params['demolition_cost_per_sqft'][0]
        construction_cost = proposed_floor_area * params['construction_cost_per_sqft'][0]
        total_development_cost = land_value + demolition_cost + construction_cost

        current_value = gdf['BldgArea'] * params['market_value_per_sqft'][0]
        redeveloped_value = proposed_floor_area * params['market_value_per_sqft'][0]

        equity_investment = total_development_cost * (1 - params['debt_ratio'][0])
        debt = total_development_cost * params['debt_ratio'][0]

        annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
        annual_debt_service = debt * (params['interest_rate'][0] * (1 + params['interest_rate'][0])**params['holding_period_years'][0]) / ((1 + params['interest_rate'][0])**params['holding_period_years'][0] - 1)

        sale_price = redeveloped_value * (1 + params['appreciation_rate'][0])**params['holding_period_years'][0]

        cashflows = np.column_stack((
            -equity_investment,
            np.tile(annual_noi - annual_debt_service, (params['holding_period_years'][0] - 1, 1)).T,
            (annual_noi - annual_debt_service + sale_price - debt).reshape(-1, 1)
        ))

        if np.any(np.isinf(cashflows)) or np.any(np.isnan(cashflows)):
            print("Cashflows contain infs or NaNs")
            return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

        npv_value = safe_npv(params['discount_rate'][0], cashflows)
        irr_value = safe_irr(cashflows)
        equity_multiple = np.sum(cashflows, axis=1) / equity_investment

        return pd.DataFrame({
            'current_value': current_value,
            'redevelopment_cost': total_development_cost,
            'redeveloped_value': redeveloped_value,
            'npv': npv_value,
            'irr': irr_value,
            'equity_multiple': equity_multiple,
            'proposed_floor_area': proposed_floor_area,
            'max_floors': max_floors
        }, index=[gdf.name])
    except Exception as e:
        print(f"Error in analyze_redevelopment_potential_single: {e}")
        return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

# Main execution
print("Loading merged_gdf_r1_r5 for debugging...")

# Assuming merged_gdf_r1_r5 is already loaded as a GeoDataFrame with the necessary columns
print("Precomputing zoning data for all properties...")
current_zoning_data = precompute_zoning_data(merged_gdf_r1_r5, current_standards, standards_type='current')
proposed_zoning_data = precompute_zoning_data(merged_gdf_r1_r5, proposed_standards, standards_type='proposed')

print("Sampling parameters...")
params = sample_parameters(len(merged_gdf_r1_r5))

def analyze_property(idx, row, current_zoning_data, proposed_zoning_data, params):
    print(f"\nAnalyzing property {idx} with current standards...")
    current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
    print(current_sim)

    print(f"\nAnalyzing property {idx} with proposed standards...")
    proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
    print(proposed_sim)

    return current_sim, proposed_sim

num_cores = multiprocessing.cpu_count()
results = Parallel(n_jobs=num_cores)(
    delayed(analyze_property)(idx, row, current_zoning_data, proposed_zoning_data, params)
    for idx, row in tqdm(merged_gdf_r1_r5.iterrows(), total=len(merged_gdf_r1_r5))
)

print("Analysis complete.")

!pip install numpy_financial

current_results.columns

# Assuming results is a list of tuples containing (current_sim, proposed_sim)
# Extract the results into separate DataFrames
current_results = pd.concat([current_sim for current_sim, proposed_sim in results])
proposed_results = pd.concat([proposed_sim for current_sim, proposed_sim in results])

# Reset the index to ensure the join works correctly
current_results.reset_index(drop=True, inplace=True)
proposed_results.reset_index(drop=True, inplace=True)
merged_gdf_r1_r5.reset_index(drop=True, inplace=True)

# Add suffix '_current' to columns of current_results
current_results = current_results.add_suffix('_current')

# Merge the current results into merged_gdf_r1_r5
merged_gdf_r1_r5 = merged_gdf_r1_r5.join(current_results, how='left')

# Optionally, merge the proposed results into merged_gdf_r1_r5 with a suffix
merged_gdf_r1_r5 = merged_gdf_r1_r5.join(proposed_results, rsuffix='_proposed', how='left')

# Check the updated GeoDataFrame
print(merged_gdf_r1_r5.head())

# Combine the results into a single DataFrame for both current and proposed standards
current_results = pd.concat([res[0] for res in results], axis=0)
proposed_results = pd.concat([res[1] for res in results], axis=0)

# Sampled parameters (assuming these are the same for each property, just extract one set)
sampled_params = pd.DataFrame(params)

# Merge sampled parameters with results
current_results = current_results.reset_index(drop=True).join(sampled_params)
proposed_results = proposed_results.reset_index(drop=True).join(sampled_params)

# Select target variables and feature variables
target_variables = ['npv', 'irr', 'equity_multiple']
feature_variables = ['market_value_per_sqft', 'construction_cost_per_sqft', 'land_value_per_sqft',
                     'demolition_cost_per_sqft', 'holding_period_years', 'discount_rate',
                     'appreciation_rate', 'debt_ratio', 'interest_rate', 'floor_height']

# Perform regression analysis
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

def perform_regression_analysis(df, target, features):
    X = df[features]
    results = {}

    for target_var in target:
        y = df[target_var].dropna()
        X_target = X.loc[y.index]

        model = LinearRegression()
        model.fit(X_target, y)
        y_pred = model.predict(X_target)
        mse = mean_squared_error(y, y_pred)

        results[target_var] = {
            'model': model,
            'coefficients': model.coef_,
            'intercept': model.intercept_,
            'mse': mse
        }

    return results

# Perform regression analysis for current and proposed standards
current_regression_results = perform_regression_analysis(current_results, target_variables, feature_variables)
proposed_regression_results = perform_regression_analysis(proposed_results, target_variables, feature_variables)

# Display regression results
def display_regression_results(results):
    for target, res in results.items():
        print(f"\nRegression Analysis for {target}:\n")
        print(f"Coefficients: {res['coefficients']}")
        print(f"Intercept: {res['intercept']}")
        print(f"Mean Squared Error: {res['mse']}\n")

        feature_importance = pd.DataFrame({'Feature': feature_variables, 'Importance': res['coefficients']})
        feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

        plt.figure(figsize=(10, 5))
        sns.barplot(x='Importance', y='Feature', data=feature_importance)
        plt.title(f'Feature Importance for {target}')
        plt.show()

print("Current Standards Regression Results:")
display_regression_results(current_regression_results)

print("Proposed Standards Regression Results:")
display_regression_results(proposed_regression_results)

# Define the target IRR and EM
target_irr = 0.15
target_em = 1.5
tolerance_irr = 0.01  # Define a tolerance range for IRR
tolerance_em = 0.05   # Define a tolerance range for EM

# Filter the results for current standards
filtered_current_results = current_results[
    (current_results['irr'] > target_irr) &
    (current_results['equity_multiple'] > target_em)
]

# Filter the results for proposed standards
filtered_proposed_results = proposed_results[
    (proposed_results['irr'] > target_irr) &
    (proposed_results['equity_multiple'] > target_em)
]

print(f"Filtered Current Results: {filtered_current_results.shape[0]} rows")
print(f"Filtered Proposed Results: {filtered_proposed_results.shape[0]} rows")

import seaborn as sns
import matplotlib.pyplot as plt

def plot_distributions(filtered_results, title):
    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 20))
    axes = axes.flatten()

    param_names = feature_variables
    for i, param in enumerate(param_names):
        sns.histplot(filtered_results[param], bins=30, ax=axes[i])
        axes[i].set_title(f'Distribution of {param} for {title}')
        axes[i].set_xlabel(param)
        axes[i].set_ylabel('Frequency')

    plt.tight_layout()
    plt.show()

plot_distributions(filtered_current_results, 'Current Standards')
plot_distributions(filtered_proposed_results, 'Proposed Standards')

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

def identify_parameter_combinations(filtered_results, param_names):
    # Select the relevant parameters
    X = filtered_results[param_names]

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Apply KMeans clustering
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_scaled)

    # Add the cluster labels to the filtered results
    filtered_results['Cluster'] = kmeans.labels_

    # Plot the cluster centers
    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
    cluster_centers_df = pd.DataFrame(cluster_centers, columns=param_names)

    print("Cluster Centers (Parameter Combinations):")
    print(cluster_centers_df)

    # Visualize the cluster centers
    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 20))
    axes = axes.flatten()

    for i, param in enumerate(param_names):
        sns.histplot(filtered_results[param], bins=30, ax=axes[i])
        for center in cluster_centers_df[param]:
            axes[i].axvline(center, color='r', linestyle='--')
        axes[i].set_title(f'Distribution of {param} with Cluster Centers for IRR > {target_irr*100}% and EM > {target_em}')
        axes[i].set_xlabel(param)
        axes[i].set_ylabel('Frequency')

    plt.tight_layout()
    plt.show()

    return cluster_centers_df

# Identify parameter combinations for current standards
current_cluster_centers = identify_parameter_combinations(filtered_current_results, feature_variables)

# Identify parameter combinations for proposed standards
proposed_cluster_centers = identify_parameter_combinations(filtered_proposed_results, feature_variables)

import seaborn as sns
import matplotlib.pyplot as plt
def plot_target_distributions(df, title):
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

    sns.histplot(df['irr'], bins=30, ax=axes[0])
    axes[0].set_title(f'Distribution of IRR for {title}')
    axes[0].set_xlabel('IRR')
    axes[0].set_ylabel('Frequency')

    sns.histplot(df['equity_multiple'], bins=30, ax=axes[1])
    axes[1].set_title(f'Distribution of Equity Multiple for {title}')
    axes[1].set_xlabel('Equity Multiple')
    axes[1].set_ylabel('Frequency')

    plt.tight_layout()
    plt.show()

# Plot IRR and EM distributions for current standards
plot_target_distributions(current_results, 'Current Standards')

# Plot IRR and EM distributions for proposed standards
plot_target_distributions(proposed_results, 'Proposed Standards')

# Filter the results for IRR > 10% and Equity Multiple > 1.0
filtered_current_results = current_results[(current_results['irr'] > 0.08) & (current_results['equity_multiple'] > 1.5)]
filtered_proposed_results = proposed_results[(proposed_results['irr'] > 0.08) & (proposed_results['equity_multiple'] > 1.5)]

# Print the number of filtered results
print(f"Filtered Current Results: {len(filtered_current_results)} rows")
print(f"Filtered Proposed Results: {len(filtered_proposed_results)} rows")
import seaborn as sns
import matplotlib.pyplot as plt

def plot_filtered_distributions(df, title):
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

    sns.histplot(df['irr'], bins=30, ax=axes[0])
    axes[0].set_title(f'Distribution of IRR for {title} (Filtered)')
    axes[0].set_xlabel('IRR')
    axes[0].set_ylabel('Frequency')

    sns.histplot(df['equity_multiple'], bins=30, ax=axes[1])
    axes[1].set_title(f'Distribution of Equity Multiple for {title} (Filtered)')
    axes[1].set_xlabel('Equity Multiple')
    axes[1].set_ylabel('Frequency')

    plt.tight_layout()
    plt.show()

# Plot filtered IRR and EM distributions for current standards
plot_filtered_distributions(filtered_current_results, 'Current Standards')

# Plot filtered IRR and EM distributions for proposed standards
plot_filtered_distributions(filtered_proposed_results, 'Proposed Standards')
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

def identify_parameter_combinations(filtered_results, param_names):
    if filtered_results.empty:
        print("No results to cluster.")
        return pd.DataFrame()

    # Select the relevant parameters
    X = filtered_results[param_names]

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Apply KMeans clustering
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_scaled)

    # Add the cluster labels to the filtered results
    filtered_results['Cluster'] = kmeans.labels_

    # Plot the cluster centers
    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
    cluster_centers_df = pd.DataFrame(cluster_centers, columns=param_names)

    print("Cluster Centers (Parameter Combinations):")
    print(cluster_centers_df)

    # Visualize the cluster centers
    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 20))
    axes = axes.flatten()

    for i, param in enumerate(param_names):
        sns.histplot(filtered_results[param], bins=30, ax=axes[i])
        for center in cluster_centers_df[param]:
            axes[i].axvline(center, color='r', linestyle='--')
        axes[i].set_title(f'Distribution of {param} with Cluster Centers for >10% IRR and >1.0 EM')
        axes[i].set_xlabel(param)
        axes[i].set_ylabel('Frequency')

    plt.tight_layout()
    plt.show()

    return cluster_centers_df

# Identify parameter combinations for current standards
current_cluster_centers = identify_parameter_combinations(filtered_current_results, feature_variables)

# Identify parameter combinations for proposed standards
proposed_cluster_centers = identify_parameter_combinations(filtered_proposed_results, feature_variables)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Define the target IRR and EM thresholds
target_irr_threshold = 0.08
target_em_threshold = 1.5

# Filter the results for current standards
filtered_current_results = current_results[
    (current_results['irr'] > target_irr_threshold) &
    (current_results['equity_multiple'] > target_em_threshold)
]

# Filter the results for proposed standards
filtered_proposed_results = proposed_results[
    (proposed_results['irr'] > target_irr_threshold) &
    (proposed_results['equity_multiple'] > target_em_threshold)
]

print(f"Filtered Current Results: {filtered_current_results.shape[0]} rows")
print(f"Filtered Proposed Results: {filtered_proposed_results.shape[0]} rows")

# Calculate correlations
def calculate_correlations(filtered_results, target_variables, feature_variables):
    correlations = {}
    for target in target_variables:
        corr = filtered_results[feature_variables + [target]].corr()[target].drop(target)
        correlations[target] = corr
    return correlations

target_variables = ['irr', 'equity_multiple']
feature_variables = ['market_value_per_sqft', 'construction_cost_per_sqft', 'land_value_per_sqft',
                     'demolition_cost_per_sqft', 'holding_period_years', 'discount_rate',
                     'appreciation_rate', 'debt_ratio', 'interest_rate', 'floor_height']

current_correlations = calculate_correlations(filtered_current_results, target_variables, feature_variables)
proposed_correlations = calculate_correlations(filtered_proposed_results, target_variables, feature_variables)

print("Current Standards Correlations:")
print(pd.DataFrame(current_correlations))

print("\nProposed Standards Correlations:")
print(pd.DataFrame(proposed_correlations))

# Plotting the correlations
def plot_correlations(correlations, title):
    corr_df = pd.DataFrame(correlations)
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
    plt.title(f'Correlation Matrix for {title}')
    plt.show()

plot_correlations(current_correlations, 'Current Standards')
plot_correlations(proposed_correlations, 'Proposed Standards')

# Perform regression analysis to determine feature importance
def perform_regression_analysis(filtered_results, target, features):
    X = filtered_results[features]
    results = {}

    for target_var in target:
        y = filtered_results[target_var].dropna()
        X_target = X.loc[y.index]

        model = LinearRegression()
        model.fit(X_target, y)
        y_pred = model.predict(X_target)
        mse = mean_squared_error(y, y_pred)

        results[target_var] = {
            'model': model,
            'coefficients': model.coef_,
            'intercept': model.intercept_,
            'mse': mse
        }

    return results

# Perform regression analysis for current and proposed standards
current_regression_results = perform_regression_analysis(filtered_current_results, target_variables, feature_variables)
proposed_regression_results = perform_regression_analysis(filtered_proposed_results, target_variables, feature_variables)

# Display regression results
def display_regression_results(results, title):
    for target, res in results.items():
        print(f"\nRegression Analysis for {target} ({title}):\n")
        print(f"Coefficients: {res['coefficients']}")
        print(f"Intercept: {res['intercept']}")
        print(f"Mean Squared Error: {res['mse']}\n")

        feature_importance = pd.DataFrame({'Feature': feature_variables, 'Importance': res['coefficients']})
        feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

        plt.figure(figsize=(10, 5))
        sns.barplot(x='Importance', y='Feature', data=feature_importance)
        plt.title(f'Feature Importance for {target} ({title})')
        plt.show()

display_regression_results(current_regression_results, 'Current Standards')
display_regression_results(proposed_regression_results, 'Proposed Standards')

import seaborn as sns
import matplotlib.pyplot as plt

def plot_feature_importance(feature_importance, title):
    plt.figure(figsize=(10, 5))
    sns.barplot(x='Importance', y='Feature', data=feature_importance)
    plt.title(f'Feature Importance for {title}')
    plt.show()

# Current Standards Feature Importance
current_feature_importance_irr = pd.DataFrame({'Feature': feature_variables, 'Importance': current_regression_results['irr']['coefficients']})
current_feature_importance_irr = current_feature_importance_irr.sort_values(by='Importance', ascending=False)

current_feature_importance_em = pd.DataFrame({'Feature': feature_variables, 'Importance': current_regression_results['equity_multiple']['coefficients']})
current_feature_importance_em = current_feature_importance_em.sort_values(by='Importance', ascending=False)

plot_feature_importance(current_feature_importance_irr, 'IRR (Current Standards)')
plot_feature_importance(current_feature_importance_em, 'Equity Multiple (Current Standards)')

# Proposed Standards Feature Importance
proposed_feature_importance_irr = pd.DataFrame({'Feature': feature_variables, 'Importance': proposed_regression_results['irr']['coefficients']})
proposed_feature_importance_irr = proposed_feature_importance_irr.sort_values(by='Importance', ascending=False)

proposed_feature_importance_em = pd.DataFrame({'Feature': feature_variables, 'Importance': proposed_regression_results['equity_multiple']['coefficients']})
proposed_feature_importance_em = proposed_feature_importance_em.sort_values(by='Importance', ascending=False)

plot_feature_importance(proposed_feature_importance_irr, 'IRR (Proposed Standards)')
plot_feature_importance(proposed_feature_importance_em, 'Equity Multiple (Proposed Standards)')

!pip install geopandas esda splot libpysal mgwr

import geopandas as gpd
import numpy as np
from esda.moran import Moran
from splot.esda import moran_scatterplot
from libpysal.weights import KNN
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
from mgwr.gwr import GWR
from mgwr.sel_bw import Sel_BW
from tqdm import tqdm

# Function to check for necessary columns
def check_necessary_columns(gdf, columns):
    missing = [col for col in columns if col not in gdf.columns]
    if missing:
        print(f"Missing columns: {missing}")
        return False
    return True

# Main analysis function
def perform_spatial_analysis(merged_gdf_r1_r5):
    print("Starting spatial analysis...")

    # Ensure geometry is in a suitable projected CRS
    projected_crs = 'EPSG:2263'  # NAD83 / New York Long Island
    merged_gdf_r1_r5 = merged_gdf_r1_r5.to_crs(projected_crs)

    # Check if 'irr' and 'equity_multiple' columns exist
    if 'irr' not in merged_gdf_r1_r5.columns or 'equity_multiple' not in merged_gdf_r1_r5.columns:
        print("Error: 'irr' or 'equity_multiple' columns are missing.")
        return

    # Drop rows with missing values in 'irr' and 'equity_multiple'
    merged_gdf_r1_r5 = merged_gdf_r1_r5.dropna(subset=['irr', 'equity_multiple'])

    # Spatial Analysis: Moran's I
    # Create a spatial weights matrix using KNN
    print("Creating spatial weights matrix...")
    k = 8  # number of neighbors, adjust as needed
    try:
        w = KNN.from_dataframe(merged_gdf_r1_r5, k=k)
        print(f"Spatial weights matrix created with {k} neighbors.")
    except Exception as e:
        print(f"Error creating spatial weights matrix: {e}")
        return

    # Perform Moran's I analysis
    print("Performing Moran's I analysis...")
    try:
        moran_irr = Moran(merged_gdf_r1_r5['irr'].values, w)
        moran_em = Moran(merged_gdf_r1_r5['equity_multiple'].values, w)

        # Plot Moran's I scatterplot
        fig, axs = plt.subplots(1, 2, figsize=(15, 7))
        moran_scatterplot(moran_irr, ax=axs[0])
        axs[0].set_title("Moran's I for IRR")
        moran_scatterplot(moran_em, ax=axs[1])
        axs[1].set_title("Moran's I for Equity Multiple")
        plt.show()
    except Exception as e:
        print(f"Error in Moran's I analysis: {e}")

    # KDE Visualization
    print("Performing KDE analysis...")
    try:
        # Extract centroids
        centroids = merged_gdf_r1_r5.geometry.centroid
        coords = np.array([(geom.x, geom.y) for geom in centroids])

        # Perform KDE
        kde = gaussian_kde(coords.T, bw_method=0.1)

        # Create a grid of points
        xmin, ymin, xmax, ymax = merged_gdf_r1_r5.total_bounds
        x = np.linspace(xmin, xmax, 100)
        y = np.linspace(ymin, ymax, 100)
        X, Y = np.meshgrid(x, y)
        positions = np.vstack([X.ravel(), Y.ravel()])
        Z = np.reshape(kde(positions), X.shape)

        # Plot KDE
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(np.rot90(Z), cmap='hot', extent=[xmin, xmax, ymin, ymax])
        ax.plot(coords[:, 0], coords[:, 1], 'k.', markersize=2)
        ax.set_xlim([xmin, xmax])
        ax.set_ylim([ymin, ymax])
        plt.title('KDE of IRR and EM Values')
        plt.show()
    except Exception as e:
        print(f"Error in KDE analysis: {e}")

    # GWR Analysis
    print("Preparing for GWR analysis...")
    necessary_columns = ['market_value_per_sqft', 'construction_cost_per_sqft', 'land_value_per_sqft',
                         'demolition_cost_per_sqft', 'holding_period_years', 'discount_rate',
                         'appreciation_rate', 'debt_ratio', 'interest_rate', 'floor_height']

    if check_necessary_columns(merged_gdf_r1_r5, necessary_columns):
        try:
            # Prepare data for GWR
            coords = np.array([(geom.x, geom.y) for geom in merged_gdf_r1_r5.geometry.centroid])
            X = merged_gdf_r1_r5[necessary_columns].values
            y = merged_gdf_r1_r5['irr'].values.reshape(-1, 1)

            # Select bandwidth for GWR
            print("Selecting bandwidth for GWR...")
            selector = Sel_BW(coords, y, X)
            bw = selector.search(tqdm_kwargs={"desc": "Bandwidth Selection"})

            # Fit GWR model
            print("Fitting GWR model...")
            gwr_model = GWR(coords, y, X, bw)
            gwr_results = gwr_model.fit()

            # Print GWR results
            print(gwr_results.summary())
        except Exception as e:
            print(f"Error in GWR analysis: {e}")
    else:
        print("Skipping GWR analysis due to missing columns.")

# Run the analysis
perform_spatial_analysis(merged_gdf_r1_r5)

import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
import pandas as pd

# Assuming merged_gdf_r1_r5 is already loaded as a GeoDataFrame with the necessary columns

# Verify and set the initial CRS if not set
if merged_gdf_r1_r5.crs is None:
    merged_gdf_r1_r5 = merged_gdf_r1_r5.set_crs(epsg=4326)

# Reproject to EPSG:2263
merged_gdf_r1_r5 = merged_gdf_r1_r5.to_crs(epsg=2263)

# Calculate centroids in the correct projection
merged_gdf_r1_r5['centroid'] = merged_gdf_r1_r5.geometry.centroid

# Extract coordinates of centroids
coords = np.array([(geom.x, geom.y) for geom in merged_gdf_r1_r5['centroid']])

# Identify and drop duplicate columns
merged_gdf_r1_r5 = merged_gdf_r1_r5.loc[:, ~merged_gdf_r1_r5.columns.duplicated()]

# Inspect and clean the necessary columns
for column in ['irr_current', 'equity_multiple_current', 'irr_proposed', 'equity_multiple_proposed']:
    # Output sample values to debug
    print(f"Sample values from {column} before conversion:\n", merged_gdf_r1_r5[column].head(10))

    # Convert the column to string first
    merged_gdf_r1_r5[column] = merged_gdf_r1_r5[column].astype(str)

    # Then convert to numeric, coercing errors
    merged_gdf_r1_r5[column] = pd.to_numeric(merged_gdf_r1_r5[column], errors='coerce')

    # Output sample values after conversion
    print(f"Sample values from {column} after conversion:\n", merged_gdf_r1_r5[column].head(10))

# Create boolean indices for valid data, ensuring they match the DataFrame's length
valid_irr_indices_current = (merged_gdf_r1_r5['irr_current'] >= 0) & merged_gdf_r1_r5['irr_current'].notna()
valid_em_indices_current = (merged_gdf_r1_r5['equity_multiple_current'] >= 0) & merged_gdf_r1_r5['equity_multiple_current'].notna()
valid_irr_indices_proposed = (merged_gdf_r1_r5['irr_proposed'] >= 0) & merged_gdf_r1_r5['irr_proposed'].notna()
valid_em_indices_proposed = (merged_gdf_r1_r5['equity_multiple_proposed'] >= 0) & merged_gdf_r1_r5['equity_multiple_proposed'].notna()

# Combine all boolean indices to filter both coordinates and weights
valid_indices = valid_irr_indices_current & valid_em_indices_current & valid_irr_indices_proposed & valid_em_indices_proposed

# Filter coordinates and weights for valid indices
coords_filtered = coords[valid_indices.values]
weights_irr_current_filtered = merged_gdf_r1_r5.loc[valid_indices, 'irr_current'].values.flatten()
weights_em_current_filtered = merged_gdf_r1_r5.loc[valid_indices, 'equity_multiple_current'].values.flatten()
weights_irr_proposed_filtered = merged_gdf_r1_r5.loc[valid_indices, 'irr_proposed'].values.flatten()
weights_em_proposed_filtered = merged_gdf_r1_r5.loc[valid_indices, 'equity_multiple_proposed'].values.flatten()

# Debugging: Print the lengths of the indices and coords
print(f"Length of coords: {len(coords)}")
print(f"Length of valid_indices: {len(valid_indices)}")
print(f"Length of coords_filtered: {len(coords_filtered)}")
print(f"Length of weights_irr_current_filtered: {len(weights_irr_current_filtered)}")
print(f"Length of weights_em_current_filtered: {len(weights_em_current_filtered)}")
print(f"Length of weights_irr_proposed_filtered: {len(weights_irr_proposed_filtered)}")
print(f"Length of weights_em_proposed_filtered: {len(weights_em_proposed_filtered)}")

def plot_kde(coords, weights, title, ax):
    if len(coords) != len(weights):
        raise ValueError("The length of the weights array must match the length of the coordinates array.")
    kde = gaussian_kde(coords.T, weights=weights, bw_method=0.1)

    # Create a grid of points
    xmin, ymin, xmax, ymax = merged_gdf_r1_r5.total_bounds
    x = np.linspace(xmin, xmax, 100)
    y = np.linspace(ymin, ymax, 100)
    X, Y = np.meshgrid(x, y)
    positions = np.vstack([X.ravel(), Y.ravel()])
    Z = np.reshape(kde(positions), X.shape)

    # Plot KDE without rotating
    ax.imshow(np.flipud(Z), cmap='hot', extent=[xmin, xmax, ymin, ymax])
    ax.plot(coords[:, 0], coords[:, 1], 'k.', markersize=2)
    ax.set_xlim([xmin, xmax])
    ax.set_ylim([ymin, ymax])
    ax.set_title(title)

fig, axs = plt.subplots(2, 2, figsize=(15, 15))

# Plot KDE for current standards
plot_kde(coords_filtered, weights_irr_current_filtered, 'KDE of IRR Values in EPSG:2263 (Current Standards)', axs[0, 0])
plot_kde(coords_filtered, weights_em_current_filtered, 'KDE of Equity Multiple Values in EPSG:2263 (Current Standards)', axs[0, 1])

# Plot KDE for proposed standards
plot_kde(coords_filtered, weights_irr_proposed_filtered, 'KDE of IRR Values in EPSG:2263 (Proposed Standards)', axs[1, 0])
plot_kde(coords_filtered, weights_em_proposed_filtered, 'KDE of Equity Multiple Values in EPSG:2263 (Proposed Standards)', axs[1, 1])

plt.tight_layout()
plt.show()

def plot_kde_difference(coords, weights_current, weights_proposed, title, ax):
    kde_current = gaussian_kde(coords.T, weights=weights_current, bw_method=0.1)
    kde_proposed = gaussian_kde(coords.T, weights=weights_proposed, bw_method=0.1)

    xmin, ymin, xmax, ymax = merged_gdf_r1_r5.total_bounds
    x = np.linspace(xmin, xmax, 100)
    y = np.linspace(ymin, ymax, 100)
    X, Y = np.meshgrid(x, y)
    positions = np.vstack([X.ravel(), Y.ravel()])

    Z_current = np.reshape(kde_current(positions), X.shape)
    Z_proposed = np.reshape(kde_proposed(positions), X.shape)

    Z_diff = Z_proposed - Z_current

    im = ax.imshow(np.flipud(Z_diff), cmap='RdBu_r', extent=[xmin, xmax, ymin, ymax])
    ax.set_title(title)
    return im

def plot_kde_ratio(coords, weights_current, weights_proposed, title, ax):
    kde_current = gaussian_kde(coords.T, weights=weights_current, bw_method=0.1)
    kde_proposed = gaussian_kde(coords.T, weights=weights_proposed, bw_method=0.1)

    xmin, ymin, xmax, ymax = merged_gdf_r1_r5.total_bounds
    x = np.linspace(xmin, xmax, 100)
    y = np.linspace(ymin, ymax, 100)
    X, Y = np.meshgrid(x, y)
    positions = np.vstack([X.ravel(), Y.ravel()])

    Z_current = np.reshape(kde_current(positions), X.shape)
    Z_proposed = np.reshape(kde_proposed(positions), X.shape)

    # Avoid division by zero
    Z_ratio = np.where(Z_current != 0, Z_proposed / Z_current, 1)

    # Log scale for better visualization
    Z_log_ratio = np.log2(Z_ratio)

    im = ax.imshow(np.flipud(Z_log_ratio), cmap='RdBu_r', extent=[xmin, xmax, ymin, ymax])
    ax.set_title(title)
    return im

fig, axs = plt.subplots(2, 2, figsize=(20, 20))

# Difference maps
im1 = plot_kde_difference(coords_filtered, weights_irr_current_filtered, weights_irr_proposed_filtered,
                          'Difference in IRR KDE (Proposed - Current)', axs[0, 0])
im2 = plot_kde_difference(coords_filtered, weights_em_current_filtered, weights_em_proposed_filtered,
                          'Difference in Equity Multiple KDE (Proposed - Current)', axs[0, 1])

# Ratio maps
im3 = plot_kde_ratio(coords_filtered, weights_irr_current_filtered, weights_irr_proposed_filtered,
                     'Ratio of IRR KDE (Proposed / Current, log2 scale)', axs[1, 0])
im4 = plot_kde_ratio(coords_filtered, weights_em_current_filtered, weights_em_proposed_filtered,
                     'Ratio of Equity Multiple KDE (Proposed / Current, log2 scale)', axs[1, 1])

# Add colorbars
plt.colorbar(im1, ax=axs[0, 0], label='Difference in Density')
plt.colorbar(im2, ax=axs[0, 1], label='Difference in Density')
plt.colorbar(im3, ax=axs[1, 0], label='Log2 Ratio of Density')
plt.colorbar(im4, ax=axs[1, 1], label='Log2 Ratio of Density')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def calculate_zoning_impact(gdf):
    # Calculate the impact for each lot
    gdf['irr_impact'] = gdf['irr_proposed'] - gdf['irr_current']
    gdf['em_impact'] = gdf['equity_multiple_proposed'] - gdf['equity_multiple_current']

    # Group by zoning district and calculate mean impact
    impact_by_zone = gdf.groupby('ZoneDist1').agg({
        'irr_impact': 'mean',
        'em_impact': 'mean'
    }).reset_index()

    return impact_by_zone

def visualize_zoning_impact(impact_df):
    # Set up the plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))

    # Plot IRR impact
    sns.barplot(x='ZoneDist1', y='irr_impact', data=impact_df, ax=ax1)
    ax1.set_title('Average Impact on IRR by Zoning District')
    ax1.set_xlabel('Zoning District')
    ax1.set_ylabel('Change in IRR')
    ax1.tick_params(axis='x', rotation=45)

    # Plot EM impact
    sns.barplot(x='ZoneDist1', y='em_impact', data=impact_df, ax=ax2)
    ax2.set_title('Average Impact on Equity Multiple by Zoning District')
    ax2.set_xlabel('Zoning District')
    ax2.set_ylabel('Change in Equity Multiple')
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

# Assuming merged_gdf_r1_r5 is your main DataFrame
impact_by_zone = calculate_zoning_impact(merged_gdf_r1_r5)
visualize_zoning_impact(impact_by_zone)

# Print the results
print("Impact by Zoning District:")
print(impact_by_zone.to_string(index=False))

import geopandas as gpd
import plotly.express as px

def calculate_property_impact(gdf):
    # Calculate the impact for each property
    gdf['irr_impact'] = gdf['irr_proposed'] - gdf['irr_current']
    gdf['em_impact'] = gdf['equity_multiple_proposed'] - gdf['equity_multiple_current']
    return gdf

def visualize_property_impact_plotly_express(impact_gdf):
    # Check if CRS is set, if not, assume it's EPSG:2263 (New York State Plane Long Island)
    if impact_gdf.crs is None:
        print("Warning: CRS not set. Assuming EPSG:2263 (New York State Plane Long Island)")
        impact_gdf = impact_gdf.set_crs(epsg=2263, allow_override=True)

    # Ensure the GeoDataFrame is in the correct CRS for web mapping
    impact_gdf = impact_gdf.to_crs(epsg=4326)

    # Create a figure for IRR impact
    fig_irr = px.choropleth_mapbox(impact_gdf,
                                   geojson=impact_gdf.geometry.__geo_interface__,
                                   locations=impact_gdf.index,
                                   color='irr_impact',
                                   color_continuous_scale="RdYlGn",
                                   range_color=(impact_gdf['irr_impact'].min(), impact_gdf['irr_impact'].max()),
                                   mapbox_style="carto-positron",
                                   zoom=10,
                                   center={"lat": impact_gdf.geometry.centroid.y.mean(),
                                           "lon": impact_gdf.geometry.centroid.x.mean()},
                                   opacity=0.7,
                                   labels={'irr_impact':'Change in IRR'},
                                   hover_data=['ZoneDist1', 'irr_impact'])

    fig_irr.update_traces(marker_line_width=0)  # Remove border lines
    fig_irr.update_layout(title='Property-Level Impact on IRR')

    # Create a figure for Equity Multiple impact
    fig_em = px.choropleth_mapbox(impact_gdf,
                                  geojson=impact_gdf.geometry.__geo_interface__,
                                  locations=impact_gdf.index,
                                  color='em_impact',
                                  color_continuous_scale="RdYlGn",
                                  range_color=(impact_gdf['em_impact'].min(), impact_gdf['em_impact'].max()),
                                  mapbox_style="carto-positron",
                                  zoom=10,
                                  center={"lat": impact_gdf.geometry.centroid.y.mean(),
                                          "lon": impact_gdf.geometry.centroid.x.mean()},
                                  opacity=0.7,
                                  labels={'em_impact':'Change in Equity Multiple'},
                                  hover_data=['ZoneDist1', 'em_impact'])

    fig_em.update_traces(marker_line_width=0)  # Remove border lines
    fig_em.update_layout(title='Property-Level Impact on Equity Multiple')

    # Show the plots
    fig_irr.show()
    fig_em.show()

    # Print summary statistics
    print("\nSummary Statistics:")
    print(impact_gdf[['irr_impact', 'em_impact']].describe())

    # Print impact statistics by zoning district
    print("\nImpact by Zoning District (Mean):")
    print(impact_gdf.groupby('ZoneDist1')[['irr_impact', 'em_impact']].mean())

# Assuming merged_gdf_r1_r5 is your main GeoDataFrame
try:
    # Check if the required columns exist
    required_columns = ['ZoneDist1', 'irr_current', 'irr_proposed', 'equity_multiple_current', 'equity_multiple_proposed', 'geometry']
    missing_columns = [col for col in required_columns if col not in merged_gdf_r1_r5.columns]

    if missing_columns:
        raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")

    # Check if CRS is set, if not, assume it's EPSG:2263
    if merged_gdf_r1_r5.crs is None:
        print("Warning: CRS not set in input GeoDataFrame. Assuming EPSG:2263 (New York State Plane Long Island)")
        merged_gdf_r1_r5 = merged_gdf_r1_r5.set_crs(epsg=2263, allow_override=True)

    impact_gdf = calculate_property_impact(merged_gdf_r1_r5)
    visualize_property_impact_plotly_express(impact_gdf)
except Exception as e:
    print(f"An error occurred: {str(e)}")
    print("\nDebug information:")
    print(f"Columns in merged_gdf_r1_r5: {merged_gdf_r1_r5.columns.tolist()}")
    print(f"CRS of merged_gdf_r1_r5: {merged_gdf_r1_r5.crs}")
    print(f"Shape of merged_gdf_r1_r5: {merged_gdf_r1_r5.shape}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.stats import f_oneway
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import warnings
from tqdm import tqdm
import psutil
import logging
from io import StringIO

warnings.filterwarnings('ignore')

# Configure logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.propagate = False

# Assuming the necessary imports and functions from your provided simulation logic are already run

# Convert zoning data dictionaries to DataFrames
current_zoning_df = pd.DataFrame.from_dict(current_zoning_data).transpose().reset_index()
current_zoning_df.rename(columns={'index': 'ZoneDist1'}, inplace=True)

proposed_zoning_df = pd.DataFrame.from_dict(proposed_zoning_data).transpose().reset_index()
proposed_zoning_df.rename(columns={'index': 'ZoneDist1'}, inplace=True)

# Merge current zoning data with properties
merged_gdf_r1_r5 = merged_gdf_r1_r5.merge(current_zoning_df, on='ZoneDist1', how='left', suffixes=('', '_current'))

# Merge proposed zoning data with properties
merged_gdf_r1_r5 = merged_gdf_r1_r5.merge(proposed_zoning_df, on='ZoneDist1', how='left', suffixes=('', '_proposed'))

# Ensure columns have the correct names
merged_gdf_r1_r5.rename(columns={
    'Residential FAR (max)': 'max_residential_far_current',
    'Front Yard Depth (min)': 'min_front_yard_depth_current',
    'Rear Yard Depth (min)': 'min_rear_yard_depth_current',
    'Side Yards (total width)': 'min_side_yard_width_current',
    'Building Height (max)': 'max_building_height_current',
    'Residential FAR (max)_proposed': 'max_residential_far_proposed',
    'Front Yard Depth (min)_proposed': 'min_front_yard_depth_proposed',
    'Rear Yard Depth (min)_proposed': 'min_rear_yard_depth_proposed',
    'Side Yards (total width)_proposed': 'min_side_yard_width_proposed',
    'Building Height (max)_proposed': 'max_building_height_proposed'
}, inplace=True)

# List of relevant features
features = [
    'LotArea', 'proposed_floor_area', 'max_floors_proposed', 'proposed_floor_area_proposed',
    'max_height', 'LotFront', 'LotDepth', 'Block', 'max_floors', 'max_floors_current',
    'max_residential_far_current', 'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'min_rear_yard_depth_proposed',
    'min_side_yard_width_proposed', 'max_building_height_proposed'
]

# Extract and normalize the features
X = merged_gdf_r1_r5[features]

# Normalize numerical columns
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X.select_dtypes(include=[np.number]))

# Combine the normalized numerical columns with non-numerical columns
X_scaled_df = pd.DataFrame(X_scaled, columns=X.select_dtypes(include=[np.number]).columns)
X_scaled_df['Block'] = X['Block'].values  # Keep Block as is for now

# Convert centroid to separate latitude and longitude columns if needed
# For simplicity, assuming centroid is a tuple of (latitude, longitude)
if 'centroid' in X:
    X_scaled_df[['latitude', 'longitude']] = pd.DataFrame(X['centroid'].tolist(), index=X.index)
    X_scaled_df.drop(columns=['centroid'], inplace=True)

# Function to ensure calculations do not produce NaN values
def handle_nan_values(df):
    for col in df.columns:
        df[col] = df[col].apply(lambda x: x if not pd.isna(x) and np.isfinite(x) else 0)
    return df

# Function to run simulations for representative properties in each cluster
def run_simulations_for_cluster(cluster_data, current_zoning_data, proposed_zoning_data, params, progress_dict, cluster_id):
    def simulate_single_property(idx, row):
        current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
        proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
        result = current_sim.copy()
        result['npv_proposed'] = proposed_sim['npv'].values[0] if not pd.isna(proposed_sim['npv'].values[0]) else 0
        result['irr_proposed'] = proposed_sim['irr'].values[0] if not pd.isna(proposed_sim['irr'].values[0]) else 0
        result['equity_multiple_proposed'] = proposed_sim['equity_multiple'].values[0] if not pd.isna(proposed_sim['equity_multiple'].values[0]) else 0
        return result

    # Determine the representative property/properties for the cluster
    representative_indices, _ = pairwise_distances_argmin_min(
        cluster_data[features],
        cluster_data[features].mean().values.reshape(1, -1)
    )

    representative_properties = cluster_data.iloc[representative_indices]

    results = []
    total_properties = len(representative_properties)
    progress_bar = tqdm(total=total_properties, desc=f"Cluster {cluster_id}", position=cluster_id, leave=False)

    for idx, row in representative_properties.iterrows():
        logger.info(f"Cluster {cluster_id}: Simulating property {idx + 1}/{total_properties}")
        print(f"Cluster {cluster_id}: Simulating property {idx + 1}/{total_properties}")
        result = simulate_single_property(idx, row)
        results.append(result)
        progress_dict[cluster_id] = (idx + 1) / total_properties
        progress_bar.update(1)

    progress_bar.close()
    results_df = pd.concat(results)
    return handle_nan_values(results_df)

# Evaluate Cluster Homogeneity
def evaluate_cluster_homogeneity(combined_results):
    cluster_summary = combined_results.groupby('cluster').agg({
        'npv': ['mean', 'var'],
        'irr': ['mean', 'var'],
        'equity_multiple': ['mean', 'var'],
        'npv_proposed': ['mean', 'var'],
        'irr_proposed': ['mean', 'var'],
        'equity_multiple_proposed': ['mean', 'var']
    }).reset_index()

    # Perform ANOVA to test if performance metrics differ by cluster
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    anova_result_npv = f_oneway(*npv_clusters)
    irr_clusters = [group['irr'].values for name, group in combined_results.groupby('cluster')]
    anova_result_irr = f_oneway(*irr_clusters)
    equity_multiple_clusters = [group['equity_multiple'].values for name, group in combined_results.groupby('cluster')]
    anova_result_em = f_oneway(*equity_multiple_clusters)

    return anova_result_npv.pvalue, anova_result_irr.pvalue, anova_result_em.pvalue

# Function to parallelize the evaluation of multiple cluster numbers
def evaluate_clusters_parallel(k, merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params, progress_dict):
    logger.info(f"Evaluating {k} clusters...")
    print(f"Evaluating {k} clusters...")
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    merged_gdf_r1_r5['cluster'] = kmeans.fit_predict(X_scaled_df)

    # Run simulations for each cluster
    clustered_results = []
    for cluster in merged_gdf_r1_r5['cluster'].unique():
        cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster]
        logger.info(f"Running simulations for cluster {cluster}")
        print(f"Running simulations for cluster {cluster}")
        simulated_data = run_simulations_for_cluster(cluster_data, current_zoning_data, proposed_zoning_data, params, progress_dict, cluster)
        simulated_data['cluster'] = cluster  # Add cluster label to the results
        clustered_results.append(simulated_data)

    # Combine results
    combined_results = pd.concat(clustered_results)

    # Evaluate cluster homogeneity
    npv_pvalue, irr_pvalue, em_pvalue = evaluate_cluster_homogeneity(combined_results)

    logger.info(f"Completed evaluation for {k} clusters: NPV p-value: {npv_pvalue}, IRR p-value: {irr_pvalue}, EM p-value: {em_pvalue}")
    print(f"Completed evaluation for {k} clusters: NPV p-value: {npv_pvalue}, IRR p-value: {irr_pvalue}, EM p-value: {em_pvalue}")
    return k, npv_pvalue, irr_pvalue, em_pvalue

# Determine number of available logical CPUs
num_workers = psutil.cpu_count(logical=True)

# Initial Coarse-Grained Search favoring lower range
def initial_coarse_grained_search(merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params, num_workers):
    # Logarithmic scale to favor lower range
    broad_range = np.unique(np.geomspace(2, len(merged_gdf_r1_r5), num_workers).astype(int))
    manager = multiprocessing.Manager()
    progress_dict = manager.dict()

    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        future_to_k = {executor.submit(evaluate_clusters_parallel, k, merged_gdf_r1_r5.copy(), X_scaled_df, current_zoning_data, proposed_zoning_data, params, progress_dict): k for k in broad_range}

        results = []
        for future in tqdm(as_completed(future_to_k), total=len(future_to_k), desc="Initial Coarse-Grained Search"):
            k = future_to_k[future]
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Testing with {k} clusters: NPV p-value: {result[1]}, IRR p-value: {result[2]}, EM p-value: {result[3]}")
                print(f"Testing with {k} clusters: NPV p-value: {result[1]}, IRR p-value: {result[2]}, EM p-value: {result[3]}")

                # Early termination if results are promising
                if result[1] < 0.05 and result[3] < 0.05:
                    break
            except Exception as e:
                logger.error(f"Error evaluating {k} clusters: {e}")
                print(f"Error evaluating {k} clusters: {e}")

    promising_ranges = [(k, npv_pvalue, irr_pvalue, em_pvalue) for k, npv_pvalue, irr_pvalue, em_pvalue in results if npv_pvalue < 0.05 and em_pvalue < 0.05]

    if promising_ranges:
        min_cluster, max_cluster = min([r[0] for r in promising_ranges]), max([r[0] for r in promising_ranges])
    else:
        min_cluster, max_cluster = 2, len(merged_gdf_r1_r5)

    return min_cluster, max_cluster

# Intermediate and Finer Search
def dynamic_search(merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params, min_cluster, max_cluster, num_workers):
    manager = multiprocessing.Manager()
    progress_dict = manager.dict()

    while (max_cluster - min_cluster) > 10:
        search_range = np.unique(np.geomspace(min_cluster, max_cluster, num_workers).astype(int))
        with ProcessPoolExecutor(max_workers=num_workers) as executor:
            future_to_k = {executor.submit(evaluate_clusters_parallel, k, merged_gdf_r1_r5.copy(), X_scaled_df, current_zoning_data, proposed_zoning_data, params, progress_dict): k for k in search_range}

            results = []
            for future in tqdm(as_completed(future_to_k), total=len(future_to_k), desc="Dynamic Search"):
                k = future_to_k[future]
                try:
                    result = future.result()
                    results.append(result)
                    logger.info(f"Testing with {k} clusters: NPV p-value: {result[1]}, IRR p-value: {result[2]}, EM p-value: {result[3]}")
                    print(f"Testing with {k} clusters: NPV p-value: {result[1]}, IRR p-value: {result[2]}, EM p-value: {result[3]}")

                    # Early termination if results are promising
                    if result[1] < 0.05 and result[3] < 0.05:
                        break
                except Exception as e:
                    logger.error(f"Error evaluating {k} clusters: {e}")
                    print(f"Error evaluating {k} clusters: {e}")

        promising_ranges = [(k, npv_pvalue, irr_pvalue, em_pvalue) for k, npv_pvalue, irr_pvalue, em_pvalue in results if npv_pvalue < 0.05 and em_pvalue < 0.05]

        if promising_ranges:
            min_cluster, max_cluster = min([r[0] for r in promising_ranges]), max([r[0] for r in promising_ranges])
        else:
            break

    # Final fine-grained search within the last range
    search_range = range(min_cluster, max_cluster + 1)
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        future_to_k = {executor.submit(evaluate_clusters_parallel, k, merged_gdf_r1_r5.copy(), X_scaled_df, current_zoning_data, proposed_zoning_data, params, progress_dict): k for k in search_range}

        for future in tqdm(as_completed(future_to_k), total=len(future_to_k), desc="Final Search"):
            k = future_to_k[future]
            try:
                result = future.result()
                logger.info(f"Testing with {k} clusters: NPV p-value: {result[1]}, IRR p-value: {result[2]}, EM p-value: {result[3]}")
                print(f"Testing with {k} clusters: NPV p-value: {result[1]}, IRR p-value: {result[2]}, EM p-value: {result[3]}")
            except Exception as e:
                logger.error(f"Error evaluating {k} clusters: {e}")
                print(f"Error evaluating {k} clusters: {e}")

# Main execution
print("Performing initial coarse-grained search...")
min_cluster, max_cluster = initial_coarse_grained_search(merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params, num_workers)

print(f"Promising range found between {min_cluster} and {max_cluster} clusters.")

print("Performing dynamic search within the promising range...")
dynamic_search(merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params, min_cluster, max_cluster, num_workers)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from scipy.stats import f_oneway
import warnings
from tqdm import tqdm
import logging

warnings.filterwarnings('ignore')

# Configure logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.propagate = False

# Assuming the necessary imports and functions from your provided simulation logic are already run

# Convert zoning data dictionaries to DataFrames
current_zoning_df = pd.DataFrame.from_dict(current_zoning_data).transpose().reset_index()
current_zoning_df.rename(columns={'index': 'ZoneDist1'}, inplace=True)

proposed_zoning_df = pd.DataFrame.from_dict(proposed_zoning_data).transpose().reset_index()
proposed_zoning_df.rename(columns={'index': 'ZoneDist1'}, inplace=True)

# Merge current zoning data with properties
merged_gdf_r1_r5 = merged_gdf_r1_r5.merge(current_zoning_df, on='ZoneDist1', how='left', suffixes=('', '_current'))

# Merge proposed zoning data with properties
merged_gdf_r1_r5 = merged_gdf_r1_r5.merge(proposed_zoning_df, on='ZoneDist1', how='left', suffixes=('', '_proposed'))

# Ensure columns have the correct names
merged_gdf_r1_r5.rename(columns={
    'Residential FAR (max)': 'max_residential_far_current',
    'Front Yard Depth (min)': 'min_front_yard_depth_current',
    'Rear Yard Depth (min)': 'min_rear_yard_depth_current',
    'Side Yards (total width)': 'min_side_yard_width_current',
    'Building Height (max)': 'max_building_height_current',
    'Residential FAR (max)_proposed': 'max_residential_far_proposed',
    'Front Yard Depth (min)_proposed': 'min_front_yard_depth_proposed',
    'Rear Yard Depth (min)_proposed': 'min_rear_yard_depth_proposed',
    'Side Yards (total width)_proposed': 'min_side_yard_width_proposed',
    'Building Height (max)_proposed': 'max_building_height_proposed'
}, inplace=True)

# List of relevant features
features = [
    'LotArea', 'proposed_floor_area',
    'max_height', 'LotFront', 'LotDepth', 'Block', 'max_floors', 'max_floors_current',
    'max_residential_far_current', 'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'min_rear_yard_depth_proposed',
    'min_side_yard_width_proposed', 'max_building_height_proposed'
]

# Extract and normalize the features
X = merged_gdf_r1_r5[features]

# Normalize numerical columns
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X.select_dtypes(include=[np.number]))

# Combine the normalized numerical columns with non-numerical columns
X_scaled_df = pd.DataFrame(X_scaled, columns=X.select_dtypes(include=[np.number]).columns)
X_scaled_df['Block'] = X['Block'].values  # Keep Block as is for now

# Convert centroid to separate latitude and longitude columns if needed
# For simplicity, assuming centroid is a tuple of (latitude, longitude)
if 'centroid' in X:
    X_scaled_df[['latitude', 'longitude']] = pd.DataFrame(X['centroid'].tolist(), index=X.index)
    X_scaled_df.drop(columns=['centroid'], inplace=True)

# Function to ensure calculations do not produce NaN values
def handle_nan_values(df):
    for col in df.columns:
        df[col] = df[col].apply(lambda x: x if not pd.isna(x) and np.isfinite(x) else 0)
    return df

# Function to run simulations for representative properties in each cluster
def run_simulations_for_cluster(cluster_data, current_zoning_data, proposed_zoning_data, params, cluster_id):
    def simulate_single_property(idx, row):
        current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
        proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
        result = current_sim.copy()
        result['npv_proposed'] = proposed_sim['npv'].values[0] if not pd.isna(proposed_sim['npv'].values[0]) else 0
        result['irr_proposed'] = proposed_sim['irr'].values[0] if not pd.isna(proposed_sim['irr'].values[0]) else 0
        result['equity_multiple_proposed'] = proposed_sim['equity_multiple'].values[0] if not pd.isna(proposed_sim['equity_multiple'].values[0]) else 0
        return result

    # Determine the representative property/properties for the cluster
    representative_indices, _ = pairwise_distances_argmin_min(
        cluster_data[features],
        cluster_data[features].mean().values.reshape(1, -1)
    )

    representative_properties = cluster_data.iloc[representative_indices]

    results = []
    total_properties = len(representative_properties)

    logger.info(f"Cluster {cluster_id}: Simulating {total_properties} representative properties")

    with tqdm(total=total_properties, desc=f"Cluster {cluster_id}", miniters=375) as pbar:
        for i, (idx, row) in enumerate(representative_properties.iterrows()):
            # Optionally reduce logging frequency further if desired
            if i % 1000 == 0:
                logger.info(f"Cluster {cluster_id}: Simulating property {i}/{total_properties}")
            result = simulate_single_property(idx, row)
            results.append(result)
            pbar.update(1)

    results_df = pd.concat(results)
    return handle_nan_values(results_df)

# Evaluate Cluster Homogeneity
def evaluate_cluster_homogeneity(combined_results):
    cluster_summary = combined_results.groupby('cluster').agg({
        'npv': ['mean', 'var'],
        'irr': ['mean', 'var'],
        'equity_multiple': ['mean', 'var'],
        'npv_proposed': ['mean', 'var'],
        'irr_proposed': ['mean', 'var'],
        'equity_multiple_proposed': ['mean', 'var']
    }).reset_index()

    # Perform ANOVA to test if performance metrics differ by cluster
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    anova_result_npv = f_oneway(*npv_clusters)
    irr_clusters = [group['irr'].values for name, group in combined_results.groupby('cluster')]
    anova_result_irr = f_oneway(*irr_clusters)
    equity_multiple_clusters = [group['equity_multiple'].values for name, group in combined_results.groupby('cluster')]
    anova_result_em = f_oneway(*equity_multiple_clusters)

    return anova_result_npv.pvalue, anova_result_irr.pvalue, anova_result_em.pvalue

# Function to evaluate multiple cluster numbers sequentially
def evaluate_clusters_sequentially(k_values, merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params):
    results = []
    best_cluster = None
    best_pvalues = (1, 1, 1)  # Initialize with non-significant p-values

    with tqdm(total=len(k_values), desc="Sequential Cluster Evaluation") as pbar:
        for k in k_values:
            logger.info(f"Evaluating {k} clusters...")
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            merged_gdf_r1_r5['cluster'] = kmeans.fit_predict(X_scaled_df)

            # Run simulations for each cluster
            clustered_results = []
            for cluster in merged_gdf_r1_r5['cluster'].unique():
                cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster]
                simulated_data = run_simulations_for_cluster(cluster_data, current_zoning_data, proposed_zoning_data, params, cluster)
                simulated_data['cluster'] = cluster  # Add cluster label to the results
                clustered_results.append(simulated_data)

            # Combine results
            combined_results = pd.concat(clustered_results)

            # Evaluate cluster homogeneity
            npv_pvalue, irr_pvalue, em_pvalue = evaluate_cluster_homogeneity(combined_results)

            # Log p-values
            logger.info(f"Completed evaluation for {k} clusters: NPV p-value: {npv_pvalue}, IRR p-value: {irr_pvalue}, EM p-value: {em_pvalue}")
            print(f"Completed evaluation for {k} clusters: NPV p-value: {npv_pvalue}, IRR p-value: {irr_pvalue}, EM p-value: {em_pvalue}")
            results.append((k, npv_pvalue, irr_pvalue, em_pvalue))

            # Update best clustering if current clustering is better
            if npv_pvalue < best_pvalues[0] and irr_pvalue < best_pvalues[1] and em_pvalue < best_pvalues[2]:
                best_cluster = k
                best_pvalues = (npv_pvalue, irr_pvalue, em_pvalue)

            pbar.update(1)

    return results, best_cluster, best_pvalues

# Main execution
# Initial coarse-grained search favoring lower range
broad_range = np.unique(np.geomspace(2, len(merged_gdf_r1_r5), 10).astype(int))

print("Performing initial coarse-grained search...")
initial_results, best_cluster_initial, best_pvalues_initial = evaluate_clusters_sequentially(broad_range, merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params)

# Identify promising range
promising_ranges = [(k, npv_pvalue, irr_pvalue, em_pvalue) for k, npv_pvalue, irr_pvalue, em_pvalue in initial_results if npv_pvalue < 0.05 and em_pvalue < 0.05]

if promising_ranges:
    min_cluster, max_cluster = min([r[0] for r in promising_ranges]), max([r[0] for r in promising_ranges])
else:
    min_cluster, max_cluster = 2, len(merged_gdf_r1_r5)

print(f"Promising range found between {min_cluster} and {max_cluster} clusters.")

# Dynamic search within the promising range
dynamic_range = np.unique(np.geomspace(min_cluster, max_cluster, 10).astype(int))
print("Performing dynamic search within the promising range...")
dynamic_results, best_cluster_dynamic, best_pvalues_dynamic = evaluate_clusters_sequentially(dynamic_range, merged_gdf_r1_r5, X_scaled_df, current_zoning_data, proposed_zoning_data, params)

# Display best cluster and p-values
print(f"Best cluster found: {best_cluster_dynamic} with NPV p-value: {best_pvalues_dynamic[0]}, IRR p-value: {best_pvalues_dynamic[1]}, EM p-value: {best_pvalues_dynamic[2]}")

!pip install gap-stat

import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import psutil
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.stats import f_oneway
import warnings
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import concurrent.futures
from concurrent.futures import ProcessPoolExecutor
import sys

warnings.filterwarnings('ignore')

# List of relevant features for scaling
features = [
    'LotArea', 'proposed_floor_area',
    'max_height', 'LotFront', 'LotDepth', 'Block', 'max_floors', 'max_floors_current',
    'max_residential_far_current', 'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'min_rear_yard_depth_proposed',
    'min_side_yard_width_proposed', 'max_building_height_proposed'
]

# Function to evaluate cluster metrics
def evaluate_cluster_metrics(X_scaled_df, labels):
    silhouette_avg = silhouette_score(X_scaled_df, labels)
    davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
    calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
    return silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg

# Function to calculate the gap statistic with parallel processing
def calculate_gap_statistic(X_scaled_df, k, n_refs=3):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_scaled_df)
    log_inertia = np.log(km.inertia_)

    def compute_reference_inertia(_):
        random_data = np.random.random_sample(size=X_scaled_df.shape)
        km_random = KMeans(n_clusters=k, random_state=42, n_init=10)
        km_random.fit(random_data)
        return np.log(km_random.inertia_)

    with ThreadPoolExecutor(max_workers=4) as executor:  # Limit number of workers to avoid overloading
        reference_inertia = list(executor.map(compute_reference_inertia, range(n_refs)))

    gap = np.mean(reference_inertia) - log_inertia
    return gap

def print_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")

# Function to dynamically calculate normalization ranges with detailed logging and timing
def dynamic_normalization_ranges(X_scaled_df, k_values, metric_targets):
    print(f"X_scaled_df shape: {X_scaled_df.shape}")
    print_memory_usage()

    silhouette_values = []
    davies_bouldin_values = []
    calinski_harabasz_values = []
    gap_values = []
    p_value_values = []
    times = []
    tested_k_values = []

    silhouette_peak, davies_bouldin_peak, calinski_harabasz_peak, gap_peak, p_value_peak = False, False, False, False, False

    for k in k_values:
        if silhouette_peak and davies_bouldin_peak and calinski_harabasz_peak and gap_peak and p_value_peak:
            break

        tested_k_values.append(k)  # Keep track of tested k values

        print(f"Starting clustering for k={k}")
        start_time = time.time()

        minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)
        labels = minibatch_kmeans.fit_predict(X_scaled_df)

        if not silhouette_peak:
            silhouette_avg = silhouette_score(X_scaled_df, labels)
            silhouette_values.append(silhouette_avg)
            if silhouette_avg >= metric_targets['silhouette']:
                silhouette_peak = True

        if not davies_bouldin_peak:
            davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
            davies_bouldin_values.append(davies_bouldin_avg)
            if davies_bouldin_avg <= metric_targets['davies_bouldin']:
                davies_bouldin_peak = True

        if not calinski_harabasz_peak:
            calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
            calinski_harabasz_values.append(calinski_harabasz_avg)
            if calinski_harabasz_avg >= metric_targets['calinski_harabasz']:
                calinski_harabasz_peak = True

        if not gap_peak:
            gap = calculate_gap_statistic(X_scaled_df, k)
            gap_values.append(gap)
            if gap >= metric_targets['gap']:
                gap_peak = True

        if not p_value_peak:
            combined_results = merged_gdf_r1_r5.copy()
            combined_results['cluster'] = labels
            npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
            anova_result_npv = f_oneway(*npv_clusters)
            p_value = anova_result_npv.pvalue
            p_value_values.append(p_value)
            if p_value <= metric_targets['p_value']:
                p_value_peak = True

        end_time = time.time()
        times.append(end_time - start_time)

        print(f"Metrics for k = {k}")
        print(f"Silhouette: {silhouette_avg if not silhouette_peak else silhouette_values[-1]}")
        print(f"Davies-Bouldin: {davies_bouldin_avg if not davies_bouldin_peak else davies_bouldin_values[-1]}")
        print(f"Calinski-Harabasz: {calinski_harabasz_avg if not calinski_harabasz_peak else calinski_harabasz_values[-1]}")
        print(f"Gap: {gap if not gap_peak else gap_values[-1]}")
        print(f"p-value: {p_value if not p_value_peak else p_value_values[-1]}")
        print(f"Time taken: {end_time - start_time} seconds")
        print_memory_usage()
        print()

    # Perform one final calculation for all metrics
    final_k = tested_k_values[-1]  # Use the last tested k value
    print(f"Starting final MiniBatchKMeans with k={final_k}")
    start_time = time.time()
    minibatch_kmeans = MiniBatchKMeans(n_clusters=final_k, random_state=42, batch_size=1000)
    labels = minibatch_kmeans.fit_predict(X_scaled_df)
    end_time = time.time()
    print(f"Final MiniBatchKMeans completed in {end_time - start_time} seconds")

    final_silhouette = silhouette_score(X_scaled_df, labels)
    final_davies_bouldin = davies_bouldin_score(X_scaled_df, labels)
    final_calinski_harabasz = calinski_harabasz_score(X_scaled_df, labels)
    final_gap = calculate_gap_statistic(X_scaled_df, final_k)

    combined_results = merged_gdf_r1_r5.copy()
    combined_results['cluster'] = labels
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    final_p_value = f_oneway(*npv_clusters).pvalue

    # Select between final calculation and previous ranges
    silhouette_range = (min(silhouette_values + [final_silhouette]), max(silhouette_values + [final_silhouette]))
    davies_bouldin_range = (min(davies_bouldin_values + [final_davies_bouldin]), max(davies_bouldin_values + [final_davies_bouldin]))
    calinski_harabasz_range = (min(calinski_harabasz_values + [final_calinski_harabasz]), max(calinski_harabasz_values + [final_calinski_harabasz]))
    gap_range = (min(gap_values + [final_gap]), max(gap_values + [final_gap]))
    p_value_range = (min(p_value_values + [final_p_value]), max(p_value_values + [final_p_value]))

    print(f"\nFinal metrics for k = {final_k}")
    print(f"Silhouette: {final_silhouette}")
    print(f"Davies-Bouldin: {final_davies_bouldin}")
    print(f"Calinski-Harabasz: {final_calinski_harabasz}")
    print(f"Gap: {final_gap}")
    print(f"p-value: {final_p_value}\n")

    print("Normalization Ranges:")
    print(f"Silhouette: {silhouette_range}")
    print(f"Davies-Bouldin: {davies_bouldin_range}")
    print(f"Calinski-Harabasz: {calinski_harabasz_range}")
    print(f"Gap: {gap_range}")
    print(f"p-value: {p_value_range}")
    print_memory_usage()

    return silhouette_range, davies_bouldin_range, calinski_harabasz_range, gap_range, p_value_range

# Function to normalize metrics dynamically
def normalize_metrics(silhouette, davies_bouldin, calinski_harabasz, gap, p_value, ranges):
    silhouette_range, davies_bouldin_range, calinski_harabasz_range, gap_range, p_value_range = ranges

    # Normalize each metric to a common scale [0, 1]
    silhouette_norm = (silhouette - silhouette_range[0]) / (silhouette_range[1] - silhouette_range[0]) if silhouette_range[1] != silhouette_range[0] else 0
    davies_bouldin_norm = (davies_bouldin - davies_bouldin_range[0]) / (davies_bouldin_range[1] - davies_bouldin_range[0]) if davies_bouldin_range[1] != davies_bouldin_range[0] else 0
    calinski_harabasz_norm = (calinski_harabasz - calinski_harabasz_range[0]) / (calinski_harabasz_range[1] - calinski_harabasz_range[0]) if calinski_harabasz_range[1] != calinski_harabasz_range[0] else 0
    gap_norm = (gap - gap_range[0]) / (gap_range[1] - gap_range[0]) if gap_range[1] != gap_range[0] else 0
    p_value_norm = (p_value - p_value_range[0]) / (p_value_range[1] - p_value_range[0]) if p_value_range[1] != p_value_range[0] else 0

    return silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, gap_norm, p_value_norm

class Evaluation:
    def __init__(self, k, metrics, weights, weights_previous, lambda_value, clustering):
        self.k = k
        self.metrics = metrics
        self.weights = weights
        self.weights_previous = weights_previous
        self.lambda_value = lambda_value
        self.clustering = clustering
        self.score = None

previous_evaluations = []

# Function to calculate the weighted score with normalized metrics
def calculate_weighted_score(metrics, weights, weights_previous, , normalization_ranges):
    silhouette, davies_bouldin, calinski_harabasz, gap, p_value = metrics
    silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, gap_norm, p_value_norm = normalize_metrics(
        silhouette, davies_bouldin, calinski_harabasz, gap, p_value, normalization_ranges
    )

    regularization_term =  * np.linalg.norm(np.array(list(weights.values())) - np.array(list(weights_previous.values())))

    weighted_score = (
        weights['silhouette'] * silhouette_norm +
        weights['davies_bouldin'] * davies_bouldin_norm +
        weights['calinski_harabasz'] * calinski_harabasz_norm +
        weights['gap'] * gap_norm +
        weights['p_value'] * p_value_norm -
        regularization_term
    )

    contributions = {
        'silhouette': weights['silhouette'] * silhouette_norm,
        'davies_bouldin': weights['davies_bouldin'] * davies_bouldin_norm,
        'calinski_harabasz': weights['calinski_harabasz'] * calinski_harabasz_norm,
        'gap': weights['gap'] * gap_norm,
        'p_value': weights['p_value'] * p_value_norm,
        'regularization_term': -regularization_term
    }

    return weighted_score, contributions, (silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, gap_norm, p_value_norm)

def recalculate_scores(evaluations, new_normalization_ranges):
    for eval in evaluations:
        eval.score, eval.contributions, _ = calculate_weighted_score(
            eval.metrics,
            eval.weights,
            eval.weights_previous,
            eval.lambda_value,
            new_normalization_ranges
        )
    return max(evaluations, key=lambda x: x.score)

#Function to evaluate clustering for a specific adjustment with detailed logging and early stopping for metrics
def evaluate_adjustment(X_scaled_df, k_values, weights, weights_previous, , iter, adjustment, metric, metric_targets, normalization_ranges):
    print(f"X_scaled_df shape: {X_scaled_df.shape}")
    print_memory_usage()

    weights[metric] += adjustment
    weights = normalize_weights(weights)  # Normalize weights after adjustment

    best_evaluation = None
    peak_score = float('-inf')
    score_decreasing = False
    tested_k_values = []

    silhouette_peak, davies_bouldin_peak, calinski_harabasz_peak, gap_peak, p_value_peak = False, False, False, False, False

    current_decreasing_k = None
    for k in k_values:
        if k <= 0:
            print(f"Skipping invalid k = {k}")
            continue

        if silhouette_peak and davies_bouldin_peak and calinski_harabasz_peak and gap_peak and p_value_peak:
            break

        tested_k_values.append(k)

        print(f"Starting clustering for k={k}")
        start_time = time.time()

        minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)
        labels = minibatch_kmeans.fit_predict(X_scaled_df)

        # Early stopping checks
        if not silhouette_peak:
            silhouette_avg = silhouette_score(X_scaled_df, labels)
            if silhouette_avg >= metric_targets['silhouette']:
                silhouette_peak = True
        else:
            silhouette_avg = None

        if not davies_bouldin_peak:
            davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
            if davies_bouldin_avg <= metric_targets['davies_bouldin']:
                davies_bouldin_peak = True
        else:
            davies_bouldin_avg = None

        if not calinski_harabasz_peak:
            calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
            if calinski_harabasz_avg >= metric_targets['calinski_harabasz']:
                calinski_harabasz_peak = True
        else:
            calinski_harabasz_avg = None

        if not gap_peak:
            gap = calculate_gap_statistic(X_scaled_df, k)
            if gap >= metric_targets['gap']:
                gap_peak = True
        else:
            gap = None

        if not p_value_peak:
            combined_results = merged_gdf_r1_r5.copy()
            combined_results['cluster'] = labels
            npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
            anova_result_npv = f_oneway(*npv_clusters)
            p_value = anova_result_npv.pvalue
            if p_value <= metric_targets['p_value']:
                p_value_peak = True
        else:
            p_value = None

        # Ensure we have values for all metrics
        metrics = (
            silhouette_avg if silhouette_avg is not None else silhouette_score(X_scaled_df, labels),
            davies_bouldin_avg if davies_bouldin_avg is not None else davies_bouldin_score(X_scaled_df, labels),
            calinski_harabasz_avg if calinski_harabasz_avg is not None else calinski_harabasz_score(X_scaled_df, labels),
            gap if gap is not None else calculate_gap_statistic(X_scaled_df, k),
            p_value if p_value is not None else f_oneway(*[group['npv'].values for name, group in combined_results.groupby('cluster')]).pvalue
        )

        # Update normalization ranges
        normalization_ranges = update_normalization_ranges(normalization_ranges, metrics)

        weighted_score, contributions, _ = calculate_weighted_score(
            metrics, weights, weights_previous, , normalization_ranges
        )

        end_time = time.time()

        print(f"Metrics for k = {k}")
        print(f"Silhouette: {metrics[0]}")
        print(f"Davies-Bouldin: {metrics[1]}")
        print(f"Calinski-Harabasz: {metrics[2]}")
        print(f"Gap: {metrics[3]}")
        print(f"p-value: {metrics[4]}")
        print(f"Weighted Score: {weighted_score}")
        print(f"Time taken: {end_time - start_time} seconds")
        print_memory_usage()
        print()

        if weighted_score > peak_score:
            peak_score = weighted_score
        elif weighted_score < peak_score:
            score_decreasing = True

        if best_evaluation is None or weighted_score > best_evaluation.score:
            best_evaluation = Evaluation(k, metrics, weights.copy(), weights_previous, , labels)
            best_evaluation.score = weighted_score
            best_evaluation.contributions = contributions

        if score_decreasing and weighted_score < peak_score:
            print(f"Score consistently decreasing for k = {k}. Stopping further evaluations.")
            current_decreasing_k = k
            break

    if best_evaluation is None and tested_k_values:
        best_k = tested_k_values[-1]
        print(f"No optimal k found. Using last tested k = {best_k}")
        best_evaluation = Evaluation(best_k, metrics, weights.copy(), weights_previous, , labels)
        best_evaluation.score = float('-inf')
        best_evaluation.contributions = {m: 0 for m in weights.keys()}

    return best_evaluation, normalization_ranges

# def normalize_weights(weights):
#     total = sum(weights.values())
#     return {k: v / total for k, v in weights.items()}

def parallel_clustering(k, X_scaled_df):
    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)
    labels = minibatch_kmeans.fit_predict(X_scaled_df)

    silhouette_avg = silhouette_score(X_scaled_df, labels)
    davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
    calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
    gap = calculate_gap_statistic(X_scaled_df, k)

    combined_results = merged_gdf_r1_r5.copy()
    combined_results['cluster'] = labels
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    anova_result_npv = f_oneway(*npv_clusters)
    p_value = anova_result_npv.pvalue

    metrics = (silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, gap, p_value)
    return k, metrics, labels

# Function to evaluate clustering for a specific k
def evaluate_clustering(k, X_scaled_df, metric_targets):
    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)
    labels = minibatch_kmeans.fit_predict(X_scaled_df)

    silhouette_avg = silhouette_score(X_scaled_df, labels)
    davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
    calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
    gap = calculate_gap_statistic(X_scaled_df, k)

    combined_results = merged_gdf_r1_r5.copy()
    combined_results['cluster'] = labels
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    p_value = f_oneway(*npv_clusters).pvalue

    metrics = (silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, gap, p_value)
    return k, metrics, labels

def normalize_weights(weights):
    # Function normalize_weights(weights):
    total_weight = sum(weights.values())
    for weight in weights:
        # For each weight in weights:
        weights[weight] /= total_weight
    # Return weights
    return weights

def flush_print(*args, **kwargs):
    """ Custom print function to ensure output is flushed immediately. """
    print(*args, **kwargs)
    sys.stdout.flush()

# # Function to evaluate clustering for a specific adjustment with detailed logging and early stopping for metrics
# def evaluate_adjustment(X_scaled_df, k, weights, weights_previous, , adjustment, metric, metric_targets, normalization_ranges):
#     flush_print(f"Task started for k={k} with adjustment {adjustment} for metric {metric}")
#     weights[metric] += adjustment
#     weights = normalize_weights(weights)

#     if k <= 0:
#         flush_print(f"Skipping invalid k = {k}")
#         return None

#     try:
#         flush_print(f"Performing MiniBatchKMeans for k={k}")
#         minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)
#         labels = minibatch_kmeans.fit_predict(X_scaled_df)
#         flush_print(f"MiniBatchKMeans completed for k={k}")

#         silhouette_avg = silhouette_score(X_scaled_df, labels)
#         davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
#         calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
#         gap = calculate_gap_statistic(X_scaled_df, k)
#         flush_print(f"Metrics computed for k={k}: Silhouette={silhouette_avg}, Davies-Bouldin={davies_bouldin_avg}, Calinski-Harabasz={calinski_harabasz_avg}, Gap={gap}")

#         combined_results = merged_gdf_r1_r5.copy()
#         combined_results['cluster'] = labels
#         npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
#         p_value = f_oneway(*npv_clusters).pvalue
#         flush_print(f"ANOVA p-value computed for k={k}: {p_value}")

#         metrics = (silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, gap, p_value)

#         normalization_ranges = update_normalization_ranges(normalization_ranges, metrics)

#         weighted_score, contributions, _ = calculate_weighted_score(
#             metrics, weights, weights_previous, , normalization_ranges
#         )

#         flush_print(f"Weighted score computed for k={k}: {weighted_score}")
#         return k, weighted_score, metrics, weights.copy(), normalization_ranges, labels

#     except Exception as e:
#         flush_print(f"Exception in evaluate_adjustment for k={k} with adjustment {adjustment} for metric {metric}: {str(e)}")
#         traceback.print_exc()
#         return None

# # Parallel version of the main function
# def refine_weights_parallel(X_scaled_df, k_values, current_zoning_data, proposed_zoning_data, params,
#                             initial_weights, W, , , max_iter, metric_targets, normalization_ranges,
#                             previous_evaluations):
#     best_evaluation = None
#     best_k = None
#     decreasing_k = None
#     peak_score = float('-inf')
#     previous_best_k = None
#     weights = normalize_weights(initial_weights.copy())
#     weights_previous = weights.copy()

#     for iter in range(max_iter):
#         flush_print(f"Iteration {iter + 1}/{max_iter}")

#         if best_k is not None and decreasing_k is not None:
#             k_min = min(best_k, decreasing_k)
#             k_max = max(best_k, decreasing_k)
#             k_values = np.unique(np.geomspace(k_min, k_max, 10).astype(int))
#             k_values = k_values[(k_values >= k_min) & (k_values <= k_max)]
#             if len(k_values) < 3:
#                 k_values = np.unique(np.geomspace(max(2, k_min - 1), min(k_max + 1, X_scaled_df.shape[0]), 10).astype(int))
#         flush_print(f"Current k_values: {k_values}")

#         iteration_best_score = float('-inf')
#         iteration_best_k = None
#         score_decreasing = False
#         best_iteration_weights = weights.copy()

#         all_targets_reached = True
#         for metric in weights:
#             if metric == 'silhouette' and weights[metric] < metric_targets['silhouette']:
#                 all_targets_reached = False
#             elif metric == 'davies_bouldin' and weights[metric] > metric_targets['davies_bouldin']:
#                 all_targets_reached = False
#             elif metric == 'calinski_harabasz' and weights[metric] < metric_targets['calinski_harabasz']:
#                 all_targets_reached = False
#             elif metric == 'gap' and weights[metric] < metric_targets['gap']:
#                 all_targets_reached = False
#             elif metric == 'p_value' and weights[metric] > metric_targets['p_value']:
#                 all_targets_reached = False

#         if all_targets_reached:
#             flush_print("All metric targets reached. Stopping iterations.")
#             break

#         tasks = []
#         with ProcessPoolExecutor(max_workers=4) as executor:
#             for k in k_values:
#                 for adjustment in [-W, W]:
#                     for metric in weights:
#                         try:
#                             flush_print(f"Submitting task for k={k} with adjustment {adjustment} for metric {metric}")
#                             tasks.append(executor.submit(evaluate_adjustment, X_scaled_df, k, weights, weights_previous, , adjustment, metric, metric_targets, normalization_ranges))
#                         except Exception as e:
#                             flush_print(f"Error submitting task for k={k} with adjustment {adjustment} for metric {metric}: {str(e)}")
#                             traceback.print_exc()

#             for future in as_completed(tasks):
#                 try:
#                     result = future.result()
#                     if result:
#                         k, weighted_score, metrics, temp_weights, normalization_ranges, labels = result
#                         flush_print(f"Task completed for k={k} with metrics: {metrics} and weighted_score: {weighted_score}")
#                 except Exception as exc:
#                     flush_print(f'Task generated an exception: {exc}')
#                     flush_print(f'Exception details: {traceback.format_exc()}')
#                     continue

#                 flush_print(f"Evaluated k={k} with metrics: {metrics}")

#                 if weighted_score > iteration_best_score:
#                     iteration_best_score = weighted_score
#                     iteration_best_k = k
#                     best_iteration_weights = temp_weights.copy()

#                 if best_evaluation is None or weighted_score > best_evaluation.score:
#                     best_evaluation = Evaluation(k, metrics, temp_weights.copy(), weights_previous, , labels)
#                     best_evaluation.score = weighted_score
#                     best_evaluation.contributions = contributions
#                     best_k = k

#                     flush_print(f"Updated weights: {temp_weights}")
#                     flush_print(f"New best score: {best_evaluation.score:.4f} with k = {best_k}")
#                     for key, value in best_evaluation.contributions.items():
#                         flush_print(f"Contribution of {key}: {value:.4f}")

#                 if weighted_score < peak_score:
#                     score_decreasing = True
#                     decreasing_k = k
#                 else:
#                     peak_score = weighted_score

#                 if score_decreasing and weighted_score < peak_score:
#                     flush_print(f"Score consistently decreasing for k = {k}. Stopping further evaluations.")
#                     break

#         weights = best_iteration_weights.copy()

#         if all_targets_reached and previous_best_k == best_k:
#             flush_print("Convergence reached.")
#             break

#         previous_best_k = best_k

#     if best_evaluation is not None:
#         final_labels = KMeans(n_clusters=best_evaluation.k, random_state=42, n_init=10).fit_predict(X_scaled_df)
#         silhouette_final = silhouette_score(X_scaled_df, final_labels)
#         davies_bouldin_final = davies_bouldin_score(X_scaled_df, final_labels)
#         calinski_harabasz_final = calinski_harabasz_score(X_scaled_df, final_labels)
#         gap_final = calculate_gap_statistic(X_scaled_df, best_evaluation.k)
#         combined_results = merged_gdf_r1_r5.copy()
#         combined_results['cluster'] = final_labels
#         npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
#         anova_result_npv = f_oneway(*npv_clusters)
#         p_value_final = anova_result_npv.pvalue

#         flush_print(f"Final metrics - Silhouette: {silhouette_final}, Davies-Bouldin: {davies_bouldin_final}, Calinski-Harabasz: {calinski_harabasz_final}, Gap: {gap_final}, p-value: {p_value_final}")
#     else:
#         flush_print("No valid clustering solution found.")

#     return best_evaluation.k if best_evaluation else None, best_evaluation.score if best_evaluation else float('-inf'), weights, best_evaluation.clustering if best_evaluation else None

def broadening_search_space(k_values, current_k, max_possible_k, num_samples=10):
    new_k_values = np.unique(np.geomspace(2, max_possible_k, num_samples).astype(int))
    k_values = np.union1d(k_values, new_k_values)
    return k_values

def random_restarts(k_values, num_restarts=5):
    return [np.random.choice(k_values, len(k_values), replace=False) for _ in range(num_restarts)]

def simulated_annealing_acceptance(current_score, new_score, temperature):
    if new_score > current_score:
        return True
    else:
        probability = np.exp((new_score - current_score) / temperature)
        return np.random.rand() < probability

# Function to broaden the search space
def broaden_search_space(k_values, max_possible_k, num_samples=10):
    k_min = max(2, k_values[0] - 1)
    k_max = min(k_values[-1] * 2, max_possible_k)
    return np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))

def determine_num_samples(range_size, base_samples=5, max_samples=15):
    """
    Determines the number of samples based on the range size using a smooth function.
    """
    scaling_factor = np.log1p(range_size) / np.log1p(1000)  # log1p is log(1 + x)
    num_samples = base_samples + int((max_samples - base_samples) * scaling_factor)
    return min(max_samples, max(base_samples, num_samples))

# Function to refine weights and find the best clustering
def refine_weights(X_scaled_df, k_values, current_zoning_data, proposed_zoning_data, params,
                   initial_weights, W, , , max_iter, metric_targets, normalization_ranges,
                   previous_evaluations):
    best_evaluation = None
    best_k = None
    decreasing_k = None
    weights = normalize_weights(initial_weights.copy())
    weights_previous = weights.copy()
    peak_score = float('-inf')
    previous_best_k = None
    historical_max_k = max(k_values)
    max_possible_k = len(X_scaled_df)

    # Simulated annealing parameters
    initial_temperature = 1.0
    final_temperature = 0.01
    alpha = 0.9  # Cooling rate
    temperature = initial_temperature

    k_history = {}

    for iter in range(max_iter):
        print(f"Iteration {iter + 1}/{max_iter}")
        print(f"Current k_values: {k_values}")
        print(f"Current best_k: {best_k}, decreasing_k: {decreasing_k}")
        print(f"Current temperature: {temperature:.4f}")

        iteration_best_score = float('-inf')
        iteration_best_k = None
        best_iteration_weights = weights.copy()

        all_targets_reached = check_all_targets_reached(weights, metric_targets)

        for metric in weights:
            if check_metric_target_reached(metric, weights[metric], metric_targets):
                continue

            for adjustment in [-W, W]:
                temp_weights = weights.copy()
                temp_weights[metric] += adjustment
                temp_weights = normalize_weights(temp_weights)

                evaluation, normalization_ranges = evaluate_adjustment(
                    X_scaled_df, k_values, temp_weights, weights, , iter, adjustment, metric, metric_targets, normalization_ranges
                )

                delta_score = evaluation.score - iteration_best_score

                if delta_score > 0 or np.exp(delta_score / temperature) > np.random.rand():
                    iteration_best_score = evaluation.score
                    iteration_best_k = evaluation.k
                    best_iteration_weights = temp_weights.copy()

                if best_evaluation is None or evaluation.score > best_evaluation.score:
                    best_evaluation = evaluation
                    best_k = evaluation.k
                    weights = temp_weights
                    weights_previous = weights.copy()

                    print(f"Updated weights: {weights}")
                    print(f"New best score: {best_evaluation.score:.4f} with k = {best_k}")
                    for key, value in best_evaluation.contributions.items():
                        print(f"Contribution of {key}: {value:.4f}")

        weights = best_iteration_weights.copy()

        # Update decreasing_k between iterations
        if iteration_best_score < peak_score:
            decreasing_k = iteration_best_k
            print(f"Score decreased. Setting decreasing_k to {decreasing_k}")
        else:
            peak_score = iteration_best_score
            decreasing_k = None  # Reset decreasing_k if score didn't decrease

        # Check for convergence
        if all_targets_reached and previous_best_k == best_k:
            print("Convergence reached.")
            break

        # Update previous_best_k for the next iteration
        previous_best_k = best_k

        # Update k history
        if best_k is not None:
            k_history[best_k] = best_evaluation.score

        # Periodically broaden the search space or do random restarts
        if iter % 3 == 0:
            if np.random.rand() < 0.5:  # 50% chance
                k_values = broaden_search_space(k_values, max_possible_k)
                print("Broadened search space:", k_values)
            else:
                k_values = np.random.choice(k_values, len(k_values), replace=False)
                print("Random restart with k_values:", k_values)

        # Update k_values for the next iteration
        k_values = update_k_values(k_values, best_k, historical_max_k, max_possible_k, determine_num_samples, k_history, X_scaled_df)

        # Cooling schedule for simulated annealing
        temperature = max(final_temperature, alpha * temperature)

    # Final calculation of actual values for all metrics
    if best_evaluation:
        calculate_final_metrics(X_scaled_df, best_evaluation)

    return best_evaluation.k if best_evaluation else None, best_evaluation.score if best_evaluation else float('-inf'), weights, best_evaluation.clustering if best_evaluation else None

# Helper function to update k_values
def update_k_values(k_values, best_k, historical_max_k, max_possible_k, determine_num_samples, k_history, X_scaled_df):
    if best_k is not None and best_k in k_values:
        best_k_index = np.where(k_values == best_k)[0][0]

        if best_k_index > 0 and best_k_index + 1 < len(k_values):
            k_min = k_values[best_k_index - 1]
            k_max = k_values[best_k_index + 1]
        elif best_k_index == 0:
            print("Best k is the lowest value tested. Expanding search to lower values.")
            k_min = max(2, k_values[0] - 1)
            k_max = k_values[1]
        elif best_k_index == len(k_values) - 1:
            print("Best k is the highest value tested. Expanding search to higher values.")
            k_min = k_values[best_k_index]
            k_max = min(k_values[-1] * 2, max_possible_k)

        range_size = k_max - k_min
        num_samples = determine_num_samples(range_size)
        new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))
        print(f"Updated k_values for next iteration: {new_k_values}")
    else:
        print("k_values not updated: best_k is None or not in current k_values")
        # Revert to the best historical k if current best_k is None or not in k_values
        if k_history:
            best_k = max(k_history, key=k_history.get)
            print(f"Reverted to best historical k: {best_k}")
            best_k_index = np.where(k_values == best_k)[0][0] if best_k in k_values else None

            if best_k_index is not None:
                if best_k_index > 0 and best_k_index + 1 < len(k_values):
                    k_min = k_values[best_k_index - 1]
                    k_max = k_values[best_k_index + 1]
                elif best_k_index == 0:
                    print("Best k is the lowest value tested. Expanding search to lower values.")
                    k_min = max(2, k_values[0] - 1)
                    k_max = k_values[1]
                elif best_k_index == len(k_values) - 1:
                    print("Best k is the highest value tested. Expanding search to higher values.")
                    k_min = k_values[best_k_index]
                    k_max = min(k_values[-1] * 2, max_possible_k)

                range_size = k_max - k_min
                num_samples = determine_num_samples(range_size)
                new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))
                print(f"Updated k_values for next iteration: {new_k_values}")
            else:
                k_values = broaden_search_space(k_values, len(X_scaled_df))
                print("Broadened search space due to missing best_k:", k_values)
        else:
            k_values = broaden_search_space(k_values, len(X_scaled_df))
            print("Broadened search space due to empty k_history:", k_values)
    return new_k_values

def determine_k_range(k_values, best_k, max_possible_k):
    best_k_index = np.where(k_values == best_k)[0][0]
    if best_k_index > 0 and best_k_index + 1 < len(k_values):
        k_min = k_values[best_k_index - 1]
        k_max = k_values[best_k_index + 1]
    elif best_k_index == 0:
        print("Best k is the lowest value tested. Expanding search to lower values.")
        k_min = max(2, k_values[0] - 1)
        k_max = k_values[1]
    elif best_k_index == len(k_values) - 1:
        print("Best k is the highest value tested. Expanding search to higher values.")
        k_min = k_values[best_k_index]
        k_max = min(k_values[-1] * 2, max_possible_k)
    return k_min, k_max

def check_all_targets_reached(weights, metric_targets):
    for metric, weight in weights.items():
        if not check_metric_target_reached(metric, weight, metric_targets):
            return False
    return True

def check_metric_target_reached(metric, weight, metric_targets):
    if metric == 'silhouette' and weight < metric_targets['silhouette']:
        return False
    elif metric == 'davies_bouldin' and weight > metric_targets['davies_bouldin']:
        return False
    elif metric == 'calinski_harabasz' and weight < metric_targets['calinski_harabasz']:
        return False
    elif metric == 'gap' and weight < metric_targets['gap']:
        return False
    elif metric == 'p_value' and weight > metric_targets['p_value']:
        return False
    return True

def calculate_final_metrics(X_scaled_df, best_evaluation):
    final_labels = KMeans(n_clusters=best_evaluation.k, random_state=42, n_init=10).fit_predict(X_scaled_df)
    silhouette_final = silhouette_score(X_scaled_df, final_labels)
    davies_bouldin_final = davies_bouldin_score(X_scaled_df, final_labels)
    calinski_harabasz_final = calinski_harabasz_score(X_scaled_df, final_labels)
    gap_final = calculate_gap_statistic(X_scaled_df, best_evaluation.k)
    combined_results = merged_gdf_r1_r5.copy()
    combined_results['cluster'] = final_labels
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    anova_result_npv = f_oneway(*npv_clusters)
    p_value_final = anova_result_npv.pvalue

    print(f"Final metrics - Silhouette: {silhouette_final}, Davies-Bouldin: {davies_bouldin_final}, "
          f"Calinski-Harabasz: {calinski_harabasz_final}, Gap: {gap_final}, p-value: {p_value_final}")

def update_normalization_ranges(current_ranges, new_metrics):
    silhouette, davies_bouldin, calinski_harabasz, gap, p_value = new_metrics
    silhouette_range, davies_bouldin_range, calinski_harabasz_range, gap_range, p_value_range = current_ranges

    return (
        (min(silhouette_range[0], silhouette), max(silhouette_range[1], silhouette)),
        (min(davies_bouldin_range[0], davies_bouldin), max(davies_bouldin_range[1], davies_bouldin)),
        (min(calinski_harabasz_range[0], calinski_harabasz), max(calinski_harabasz_range[1], calinski_harabasz)),
        (min(gap_range[0], gap), max(gap_range[1], gap)),
        (min(p_value_range[0], p_value), max(p_value_range[1], p_value))
    )

# Main execution
# Assume merged_gdf_r1_r5 is your dataframe and X is the features you want to cluster
scaler = StandardScaler()
X_scaled = scaler.fit_transform(merged_gdf_r1_r5[features].select_dtypes(include=[np.number]))

# Define initial weights, W, , , and max_iter
initial_weights = {
    'silhouette': -0.3348645691656332,
    'davies_bouldin': 0.29476466725333,
    'calinski_harabasz': -0.33019431541021477,
    'gap': 1.372501212678529,
    'p_value': -0.002206995356011144
}
W = 0.05
 = 0.1
 = 1e-3
max_iter = 100

# Define numerical targets for early stopping
metric_targets = {
    'silhouette': 0.5,
    'davies_bouldin': 1,
    'calinski_harabasz': 1000,
    'gap': 1,
    'p_value': 0.05
}

# Define the range of cluster numbers
broad_range = np.unique(np.geomspace(2, len(merged_gdf_r1_r5), 10).astype(int))

# Calculate normalization ranges based on initial broad range
normalization_ranges = dynamic_normalization_ranges(X_scaled, broad_range, metric_targets)

# Initialize the list to store all evaluations
previous_evaluations = []

# Refine weights and find the best clustering
best_k, best_score, final_weights, best_clustering = refine_weights(
    X_scaled, broad_range, current_zoning_data, proposed_zoning_data, params,
    initial_weights, W, , , max_iter, metric_targets, normalization_ranges,
    previous_evaluations
)

# # Refine weights and find the best clustering
# best_k, best_score, final_weights, best_clustering = refine_weights_parallel(
#     X_scaled, broad_range, current_zoning_data, proposed_zoning_data, params,
#     initial_weights, W, , , max_iter, metric_targets, normalization_ranges,
#     previous_evaluations
# )
# # Call refine_weights_parallel to find best clustering

if best_k is not None:
    print(f"Best number of clusters: {best_k} with Weighted Score: {best_score}", flush=True)
    print(f"Final weights: {final_weights}", flush=True)
else:
    print("No valid clustering solution found.", flush=True)

"""X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.009943157914659
p-value: 0.00026477672321634764
Time taken: 9.097646951675415 seconds
Memory usage: 20.0%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 5562.398040715067
Gap: -1.2890444566011379
p-value: 0.00026477672321634764
Time taken: 9.961388111114502 seconds
Memory usage: 20.0%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 5562.398040715067
Gap: 0.05725711063413286
p-value: 0.00026477672321634764
Time taken: 5.793084383010864 seconds
Memory usage: 20.0%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 5562.398040715067
Gap: 0.8184825684350141
p-value: 0.00026477672321634764
Time taken: 11.715758323669434 seconds
Memory usage: 20.0%

Starting clustering for k=135
Metrics for k = 135
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 5562.398040715067
Gap: 1.5653256729334384
p-value: 0.00026477672321634764
Time taken: 21.68213653564453 seconds
Memory usage: 19.8%

Starting final MiniBatchKMeans with k=135
Final MiniBatchKMeans completed in 0.19473719596862793 seconds

Final metrics for k = 135
Silhouette: 0.2688120238301011
Davies-Bouldin: 1.1267529613905805
Calinski-Harabasz: 1980.6248187788258
Gap: 1.5659941081561097
p-value: 0.0

Normalization Ranges:
Silhouette: (0.2688120238301011, 0.5022451201089333)
Davies-Bouldin: (0.6859262826262669, 1.1483738533011125)
Calinski-Harabasz: (1980.6248187788258, 5562.398040715067)
Gap: (-2.009943157914659, 1.5659941081561097)
p-value: (0.0, 0.00026477672321634764)
Memory usage: 19.9%
Iteration 1/100
Current k_values: [    2     5    16    47   135   387  1111  3186  9135 26193]
Current best_k: None, decreasing_k: None
Current temperature: 1.0000
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.010490876880107
p-value: 0.00026477672321634764
Weighted Score: 0.5407767463544058
Time taken: 8.88265872001648 seconds
Memory usage: 19.8%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.288620160374128
p-value: 0.00026477672321634764
Weighted Score: 0.5456615256721994
Time taken: 9.914569854736328 seconds
Memory usage: 19.9%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05803761703541532
p-value: 0.00026477672321634764
Weighted Score: 0.6732044664510458
Time taken: 13.741336584091187 seconds
Memory usage: 19.8%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8179245483061166
p-value: 0.00026477672321634764
Weighted Score: 0.4990758544077195
Time taken: 19.347015142440796 seconds
Memory usage: 19.8%

Score consistently decreasing for k = 47. Stopping further evaluations.
Updated weights: {'silhouette': 0.2578016964081325, 'davies_bouldin': 0.24801843174872548, 'calinski_harabasz': 0.24801843174872548, 'gap': 0.24801843174872548, 'p_value': -0.0018569916543090031}
New best score: 0.6732 with k = 16
Contribution of silhouette: 0.1455
Contribution of davies_bouldin: 0.2089
Contribution of calinski_harabasz: 0.1855
Contribution of gap: 0.1434
Contribution of p_value: -0.0019
Contribution of regularization_term: -0.0084
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.0112462213766786
p-value: 0.00026477672321634764
Weighted Score: 0.38776613273563026
Time taken: 8.748329162597656 seconds
Memory usage: 19.8%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2893645946179397
p-value: 0.00026477672321634764
Weighted Score: 0.5875037951993571
Time taken: 9.96580195426941 seconds
Memory usage: 19.8%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05842887523409779
p-value: 0.00026477672321634764
Weighted Score: 0.6366810158972578
Time taken: 13.523476839065552 seconds
Memory usage: 19.8%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8183258677037397
p-value: 0.00026477672321634764
Weighted Score: 0.4523227709578793
Time taken: 17.863436698913574 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.011035989367535
p-value: 0.00026477672321634764
Weighted Score: 0.38291505143808063
Time taken: 8.95305323600769 seconds
Memory usage: 19.8%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2882149661190976
p-value: 0.00026477672321634764
Weighted Score: 0.49653287246743544
Time taken: 10.265825271606445 seconds
Memory usage: 19.8%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05764340300026127
p-value: 0.00026477672321634764
Weighted Score: 0.6302838081632998
Time taken: 12.919232845306396 seconds
Memory usage: 19.9%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8195474668020282
p-value: 0.00026477672321634764
Weighted Score: 0.5246357961379298
Time taken: 19.137775897979736 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.0112410102785248
p-value: 0.00026477672321634764
Weighted Score: 0.41680959566880477
Time taken: 8.799627542495728 seconds
Memory usage: 19.9%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2885253359134268
p-value: 0.00026477672321634764
Weighted Score: 0.5874512057545432
Time taken: 9.970177412033081 seconds
Memory usage: 19.9%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.058257821057106085
p-value: 0.00026477672321634764
Weighted Score: 0.6513010215032047
Time taken: 13.352743148803711 seconds
Memory usage: 19.8%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.818689946375498
p-value: 0.00026477672321634764
Weighted Score: 0.4766558094158046
Time taken: 18.689881324768066 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.011050409778205
p-value: 0.00026477672321634764
Weighted Score: 0.44470326230961543
Time taken: 8.806740045547485 seconds
Memory usage: 19.8%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2880899637371712
p-value: 0.00026477672321634764
Weighted Score: 0.5827362796182783
Time taken: 10.417290449142456 seconds
Memory usage: 19.8%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05847564685475781
p-value: 0.00026477672321634764
Weighted Score: 0.6486615188249147
Time taken: 13.706393003463745 seconds
Memory usage: 20.0%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8179841265111811
p-value: 0.00026477672321634764
Weighted Score: 0.4674436943089915
Time taken: 19.571772575378418 seconds
Memory usage: 20.0%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 20.0%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.010958858007223
p-value: 0.00026477672321634764
Weighted Score: 0.36365548139668846
Time taken: 8.782710313796997 seconds
Memory usage: 20.0%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.288132332658769
p-value: 0.00026477672321634764
Weighted Score: 0.5132987336559217
Time taken: 11.06395149230957 seconds
Memory usage: 20.1%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05694689630765204
p-value: 0.00026477672321634764
Weighted Score: 0.6354185045900391
Time taken: 14.867415189743042 seconds
Memory usage: 20.1%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8177516607563504
p-value: 0.00026477672321634764
Weighted Score: 0.5257054028205291
Time taken: 19.198420763015747 seconds
Memory usage: 20.1%

Score consistently decreasing for k = 47. Stopping further evaluations.
Broadened search space: [    2     5    16    47   135   387  1111  3186  9135 26193]
Iteration 2/100
Current k_values: [    2     5    16    47   135   387  1111  3186  9135 26193]
Current best_k: 16, decreasing_k: None
Current temperature: 0.9000
X_scaled_df shape: (26193, 18)
Memory usage: 20.1%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.010939863084648
p-value: 0.00026477672321634764
Weighted Score: 0.37437284236416385
Time taken: 8.75731348991394 seconds
Memory usage: 20.1%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2896085387030727
p-value: 0.00026477672321634764
Weighted Score: 0.4599262589276336
Time taken: 10.808010339736938 seconds
Memory usage: 20.1%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05849274761424361
p-value: 0.00026477672321634764
Weighted Score: 0.6398737768526368
Time taken: 13.824602365493774 seconds
Memory usage: 20.1%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8184065121057102
p-value: 0.00026477672321634764
Weighted Score: 0.5819163629342038
Time taken: 19.49715495109558 seconds
Memory usage: 20.1%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 20.0%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.009931959595802
p-value: 0.00026477672321634764
Weighted Score: 0.3530271476150241
Time taken: 8.89591121673584 seconds
Memory usage: 20.1%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2866779602473386
p-value: 0.00026477672321634764
Weighted Score: 0.5577321466468504
Time taken: 10.318651676177979 seconds
Memory usage: 20.1%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.057829221951152476
p-value: 0.00026477672321634764
Weighted Score: 0.6302968992606576
Time taken: 13.178168058395386 seconds
Memory usage: 19.9%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8182842854910657
p-value: 0.00026477672321634764
Weighted Score: 0.47594053484221754
Time taken: 19.147369623184204 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.009313530152559
p-value: 0.00026477672321634764
Weighted Score: 0.3405546660403954
Time taken: 8.9403977394104 seconds
Memory usage: 19.9%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.289030113827252
p-value: 0.00026477672321634764
Weighted Score: 0.45987591609195344
Time taken: 10.468168497085571 seconds
Memory usage: 19.9%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05817657912106711
p-value: 0.00026477672321634764
Weighted Score: 0.6226117166227724
Time taken: 14.895341396331787 seconds
Memory usage: 20.1%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8182146307928502
p-value: 0.00026477672321634764
Weighted Score: 0.5534189467161632
Time taken: 20.115445852279663 seconds
Memory usage: 20.1%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 20.0%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.010070744060455
p-value: 0.00026477672321634764
Weighted Score: 0.38207211893692217
Time taken: 9.171917200088501 seconds
Memory usage: 20.1%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2891054556708905
p-value: 0.00026477672321634764
Weighted Score: 0.55744411522548
Time taken: 10.48907732963562 seconds
Memory usage: 20.2%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.0577200905945876
p-value: 0.00026477672321634764
Weighted Score: 0.6449318883854069
Time taken: 13.166121006011963 seconds
Memory usage: 20.2%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8174355412691838
p-value: 0.00026477672321634764
Weighted Score: 0.5001952817259939
Time taken: 19.434870958328247 seconds
Memory usage: 20.2%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 20.1%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.011617652290248
p-value: 0.00026477672321634764
Weighted Score: 0.403350687626873
Time taken: 8.908732175827026 seconds
Memory usage: 20.2%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.288987378788832
p-value: 0.00026477672321634764
Weighted Score: 0.5473123845473356
Time taken: 10.40019965171814 seconds
Memory usage: 20.2%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.06047096504324401
p-value: 0.00026477672321634764
Weighted Score: 0.6422929666110836
Time taken: 13.538685321807861 seconds
Memory usage: 20.1%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8180641497226304
p-value: 0.00026477672321634764
Weighted Score: 0.4975261266497443
Time taken: 18.22303605079651 seconds
Memory usage: 20.1%

Score consistently decreasing for k = 47. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 20.0%
Starting clustering for k=2
Metrics for k = 2
Silhouette: 0.3185751768824614
Davies-Bouldin: 1.1483738533011125
Calinski-Harabasz: 5562.398040715067
Gap: -2.0099402930647248
p-value: 0.00026477672321634764
Weighted Score: 0.3300018685701777
Time taken: 8.87592887878418 seconds
Memory usage: 20.0%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.288134584713383
p-value: 0.00026477672321634764
Weighted Score: 0.4844017657380291
Time taken: 10.358893394470215 seconds
Memory usage: 20.1%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05854372546403219
p-value: 0.00026477672321634764
Weighted Score: 0.6302814337378276
Time taken: 12.865990400314331 seconds
Memory usage: 20.0%

Starting clustering for k=47
Metrics for k = 47
Silhouette: 0.25455197794051376
Davies-Bouldin: 1.2504782334865625
Calinski-Harabasz: 3625.076284334186
Gap: 0.8184228245862268
p-value: 0.00026477672321634764
Weighted Score: 0.5504342280885975
Time taken: 18.69322109222412 seconds
Memory usage: 20.0%

Score consistently decreasing for k = 47. Stopping further evaluations.
Score decreased. Setting decreasing_k to 16
Updated k_values for next iteration: [ 5  6  8 10 13 17 22 28 36 47]
Iteration 3/100
Current k_values: [ 5  6  8 10 13 17 22 28 36 47]
Current best_k: 16, decreasing_k: 16
Current temperature: 0.8100
X_scaled_df shape: (26193, 18)
Memory usage: 20.0%
Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2880334206826873
p-value: 0.0001004152794775278
Weighted Score: 0.42792424488424835
Time taken: 10.40704345703125 seconds
Memory usage: 20.0%

Starting clustering for k=6
Metrics for k = 6
Silhouette: 0.4680517425174735
Davies-Bouldin: 0.9101621511325582
Calinski-Harabasz: 7626.202846438862
Gap: -1.062760126709973
p-value: 0.0001004152794775278
Weighted Score: 0.5045530647024066
Time taken: 10.504532098770142 seconds
Memory usage: 20.0%

Starting clustering for k=8
Metrics for k = 8
Silhouette: 0.44878641130229724
Davies-Bouldin: 0.9756945640280028
Calinski-Harabasz: 7015.892855597454
Gap: -0.7017286019080178
p-value: 0.0001004152794775278
Weighted Score: 0.5415257296615098
Time taken: 11.248343467712402 seconds
Memory usage: 20.0%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48944269256135087
p-value: 0.0001004152794775278
Weighted Score: 0.5072573707717377
Time taken: 12.10425877571106 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.290010487346695
p-value: 0.0001004152794775278
Weighted Score: 0.5310930193117286
Time taken: 9.861019849777222 seconds
Memory usage: 20.0%

Starting clustering for k=6
Metrics for k = 6
Silhouette: 0.4680517425174735
Davies-Bouldin: 0.9101621511325582
Calinski-Harabasz: 7626.202846438862
Gap: -1.0649566554960668
p-value: 0.0001004152794775278
Weighted Score: 0.5687571014575624
Time taken: 10.336124897003174 seconds
Memory usage: 20.0%

Starting clustering for k=8
Metrics for k = 8
Silhouette: 0.44878641130229724
Davies-Bouldin: 0.9756945640280028
Calinski-Harabasz: 7015.892855597454
Gap: -0.7015458382059183
p-value: 0.0001004152794775278
Weighted Score: 0.585143167052287
Time taken: 10.921542167663574 seconds
Memory usage: 19.9%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4893346084564776
p-value: 0.0001004152794775278
Weighted Score: 0.5804808487047047
Time taken: 11.968507289886475 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.289755121000077
p-value: 0.0001004152794775278
Weighted Score: 0.42762700097264916
Time taken: 10.363495588302612 seconds
Memory usage: 19.9%

Starting clustering for k=6
Metrics for k = 6
Silhouette: 0.4680517425174735
Davies-Bouldin: 0.9101621511325582
Calinski-Harabasz: 7626.202846438862
Gap: -1.0644892580462297
p-value: 0.0001004152794775278
Weighted Score: 0.49997306132841846
Time taken: 10.586754083633423 seconds
Memory usage: 19.9%

Starting clustering for k=8
Metrics for k = 8
Silhouette: 0.44878641130229724
Davies-Bouldin: 0.9756945640280028
Calinski-Harabasz: 7015.892855597454
Gap: -0.701067598521778
p-value: 0.0001004152794775278
Weighted Score: 0.5393547881113968
Time taken: 10.964655876159668 seconds
Memory usage: 19.9%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.49127086625980176
p-value: 0.0001004152794775278
Weighted Score: 0.536676118634942
Time taken: 11.737020015716553 seconds
Memory usage: 20.0%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2882323036650618
p-value: 0.0001004152794775278
Weighted Score: 0.5311841297941716
Time taken: 10.204388856887817 seconds
Memory usage: 19.9%

Starting clustering for k=6
Metrics for k = 6
Silhouette: 0.4680517425174735
Davies-Bouldin: 0.9101621511325582
Calinski-Harabasz: 7626.202846438862
Gap: -1.064100593364266
p-value: 0.0001004152794775278
Weighted Score: 0.5724439328578083
Time taken: 10.375595808029175 seconds
Memory usage: 20.0%

Starting clustering for k=8
Metrics for k = 8
Silhouette: 0.44878641130229724
Davies-Bouldin: 0.9756945640280028
Calinski-Harabasz: 7015.892855597454
Gap: -0.7023485590916234
p-value: 0.0001004152794775278
Weighted Score: 0.5868393828154876
Time taken: 11.257688999176025 seconds
Memory usage: 20.0%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4898703182438169
p-value: 0.0001004152794775278
Weighted Score: 0.5547646375503658
Time taken: 12.214495182037354 seconds
Memory usage: 20.0%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2884504997831527
p-value: 0.0001004152794775278
Weighted Score: 0.5163052934049216
Time taken: 9.993816137313843 seconds
Memory usage: 19.9%

Starting clustering for k=6
Metrics for k = 6
Silhouette: 0.4680517425174735
Davies-Bouldin: 0.9101621511325582
Calinski-Harabasz: 7626.202846438862
Gap: -1.0655840417555051
p-value: 0.0001004152794775278
Weighted Score: 0.5710062439690919
Time taken: 10.306825160980225 seconds
Memory usage: 20.0%

Starting clustering for k=8
Metrics for k = 8
Silhouette: 0.44878641130229724
Davies-Bouldin: 0.9756945640280028
Calinski-Harabasz: 7015.892855597454
Gap: -0.7013298979700426
p-value: 0.0001004152794775278
Weighted Score: 0.5889574098169446
Time taken: 11.330048322677612 seconds
Memory usage: 20.0%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48952755629918165
p-value: 0.0001004152794775278
Weighted Score: 0.5621696963363764
Time taken: 11.529490947723389 seconds
Memory usage: 20.0%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2905273718267445
p-value: 0.0001004152794775278
Weighted Score: 0.4587343289653535
Time taken: 10.140312910079956 seconds
Memory usage: 19.9%

Starting clustering for k=6
Metrics for k = 6
Silhouette: 0.4680517425174735
Davies-Bouldin: 0.9101621511325582
Calinski-Harabasz: 7626.202846438862
Gap: -1.063640752020813
p-value: 0.0001004152794775278
Weighted Score: 0.5153059875899253
Time taken: 10.850867986679077 seconds
Memory usage: 20.0%

Starting clustering for k=8
Metrics for k = 8
Silhouette: 0.44878641130229724
Davies-Bouldin: 0.9756945640280028
Calinski-Harabasz: 7015.892855597454
Gap: -0.7014755679524391
p-value: 0.0001004152794775278
Weighted Score: 0.5482142779598932
Time taken: 11.152949810028076 seconds
Memory usage: 20.0%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4915067696758513
p-value: 0.0001004152794775278
Weighted Score: 0.5368007166220061
Time taken: 11.970121145248413 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 10. Stopping further evaluations.
Score decreased. Setting decreasing_k to 8
k_values not updated: best_k is None or not in current k_values
Reverted to best historical k: 16
Broadened search space due to missing best_k: [ 4  5  8 11 16 23 32 46 66 94]
Iteration 4/100
Current k_values: [ 4  5  8 11 16 23 32 46 66 94]
Current best_k: 16, decreasing_k: 8
Current temperature: 0.7290
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5512932764984164
p-value: 0.0002316374593681434
Weighted Score: 0.4398776130947842
Time taken: 9.710237503051758 seconds
Memory usage: 19.9%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.288162529019374
p-value: 0.0002316374593681434
Weighted Score: 0.40212091970436364
Time taken: 10.436870098114014 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 5. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5521481965401662
p-value: 0.0002316374593681434
Weighted Score: 0.5338567454101061
Time taken: 9.587966442108154 seconds
Memory usage: 20.0%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.289028651214684
p-value: 0.0002316374593681434
Weighted Score: 0.5101084838836812
Time taken: 9.746886253356934 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 5. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.551891896706941
p-value: 0.0002316374593681434
Weighted Score: 0.43544722284877774
Time taken: 9.39835238456726 seconds
Memory usage: 19.9%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2895655114202604
p-value: 0.0002316374593681434
Weighted Score: 0.427752950303564
Time taken: 10.412312746047974 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 5. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.8%
Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.550716659330229
p-value: 0.0002316374593681434
Weighted Score: 0.5375474353038123
Time taken: 9.517040014266968 seconds
Memory usage: 19.9%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2891274716554797
p-value: 0.0002316374593681434
Weighted Score: 0.4877388051156966
Time taken: 10.055829763412476 seconds
Memory usage: 19.9%

Score consistently decreasing for k = 5. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.9%
Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.550278674033759
p-value: 0.0002316374593681434
Weighted Score: 0.53083774477989
Time taken: 9.625989437103271 seconds
Memory usage: 19.7%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.2891200762322352
p-value: 0.0002316374593681434
Weighted Score: 0.48926677753661485
Time taken: 10.26588773727417 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 5. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5506095976564307
p-value: 0.0002316374593681434
Weighted Score: 0.4575390410220219
Time taken: 9.451316595077515 seconds
Memory usage: 19.7%

Starting clustering for k=5
Metrics for k = 5
Silhouette: 0.5022451201089333
Davies-Bouldin: 0.6859262826262669
Calinski-Harabasz: 8242.443987389803
Gap: -1.289028955499127
p-value: 0.0002316374593681434
Weighted Score: 0.4367961847390759
Time taken: 10.03325366973877 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 5. Stopping further evaluations.
Score decreased. Setting decreasing_k to 4
Broadened search space: [  3   4   7  11  18  29  47  74 118 188]
Iteration 5/100
Current k_values: [  3   4   7  11  18  29  47  74 118 188]
Current best_k: 16, decreasing_k: 4
Current temperature: 0.6561
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=3
Metrics for k = 3
Silhouette: 0.42455208866404776
Davies-Bouldin: 0.9143375411079001
Calinski-Harabasz: 8248.39678027866
Gap: -1.7841280326920934
p-value: 0.0012884318180141137
Weighted Score: 0.3678599144777176
Time taken: 9.189860582351685 seconds
Memory usage: 19.7%

Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.551955621610448
p-value: 0.0012884318180141137
Weighted Score: 0.4010496175500713
Time taken: 9.736314058303833 seconds
Memory usage: 19.7%

Starting clustering for k=7
Metrics for k = 7
Silhouette: 0.47021501382027187
Davies-Bouldin: 1.003482424812463
Calinski-Harabasz: 7952.129040670841
Gap: -0.9413503745367393
p-value: 0.0012884318180141137
Weighted Score: 0.5179166236414342
Time taken: 10.933946132659912 seconds
Memory usage: 19.7%

Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.38979151831884273
p-value: 0.0012884318180141137
Weighted Score: 0.6043656677595933
Time taken: 12.244309186935425 seconds
Memory usage: 19.7%

Starting clustering for k=18
Metrics for k = 18
Silhouette: 0.4529420621344838
Davies-Bouldin: 1.1910870555037918
Calinski-Harabasz: 6477.291811481167
Gap: 0.16088826790249122
p-value: 0.0012884318180141137
Weighted Score: 0.6690034842645265
Time taken: 14.56171727180481 seconds
Memory usage: 19.7%

Starting clustering for k=29
Metrics for k = 29
Silhouette: 0.3708684940279979
Davies-Bouldin: 1.1520293080890907
Calinski-Harabasz: 5103.589222751283
Gap: 0.48190254209089467
p-value: 0.0012884318180141137
Weighted Score: 0.6121333161338379
Time taken: 16.577327013015747 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 29. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=3
Metrics for k = 3
Silhouette: 0.42455208866404776
Davies-Bouldin: 0.9143375411079001
Calinski-Harabasz: 8248.39678027866
Gap: -1.7850956222149428
p-value: 0.0012884318180141137
Weighted Score: 0.41764213253783455
Time taken: 9.379861116409302 seconds
Memory usage: 19.7%

Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5513032577144816
p-value: 0.0012884318180141137
Weighted Score: 0.4917186363437682
Time taken: 9.339170455932617 seconds
Memory usage: 19.7%

Starting clustering for k=7
Metrics for k = 7
Silhouette: 0.47021501382027187
Davies-Bouldin: 1.003482424812463
Calinski-Harabasz: 7952.129040670841
Gap: -0.9407868084004853
p-value: 0.0012884318180141137
Weighted Score: 0.5720578073994002
Time taken: 11.065568685531616 seconds
Memory usage: 19.7%

Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3886440014717749
p-value: 0.0012884318180141137
Weighted Score: 0.6758289015035936
Time taken: 12.643272638320923 seconds
Memory usage: 19.6%

Starting clustering for k=18
Metrics for k = 18
Silhouette: 0.4529420621344838
Davies-Bouldin: 1.1910870555037918
Calinski-Harabasz: 6477.291811481167
Gap: 0.16112472126848765
p-value: 0.0012884318180141137
Weighted Score: 0.6870136819831146
Time taken: 13.940895080566406 seconds
Memory usage: 19.7%

Starting clustering for k=29
Metrics for k = 29
Silhouette: 0.3708684940279979
Davies-Bouldin: 1.1520293080890907
Calinski-Harabasz: 5103.589222751283
Gap: 0.4812804690191719
p-value: 0.0012884318180141137
Weighted Score: 0.5825569028519135
Time taken: 17.01501703262329 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 29. Stopping further evaluations.
Updated weights: {'silhouette': 0.28631161376000414, 'davies_bouldin': 0.18600403004560984, 'calinski_harabasz': 0.18600403004560984, 'gap': 0.34307299656051604, 'p_value': -0.0013926704117397258}
New best score: 0.6870 with k = 18
Contribution of silhouette: 0.2206
Contribution of davies_bouldin: 0.1664
Contribution of calinski_harabasz: 0.1016
Contribution of gap: 0.2084
Contribution of p_value: -0.0014
Contribution of regularization_term: -0.0086
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=3
Metrics for k = 3
Silhouette: 0.42455208866404776
Davies-Bouldin: 0.9143375411079001
Calinski-Harabasz: 8248.39678027866
Gap: -1.7849673453832313
p-value: 0.0012884318180141137
Weighted Score: 0.3797860581563326
Time taken: 9.184514045715332 seconds
Memory usage: 19.7%

Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5505630255293052
p-value: 0.0012884318180141137
Weighted Score: 0.4361316810135576
Time taken: 9.77384614944458 seconds
Memory usage: 19.7%

Starting clustering for k=7
Metrics for k = 7
Silhouette: 0.47021501382027187
Davies-Bouldin: 1.003482424812463
Calinski-Harabasz: 7952.129040670841
Gap: -0.9404620351555035
p-value: 0.0012884318180141137
Weighted Score: 0.5547917384169843
Time taken: 11.045881748199463 seconds
Memory usage: 19.7%

Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.38953158168943425
p-value: 0.0012884318180141137
Weighted Score: 0.6775259799604434
Time taken: 12.405162334442139 seconds
Memory usage: 19.7%

Starting clustering for k=18
Metrics for k = 18
Silhouette: 0.4529420621344838
Davies-Bouldin: 1.1910870555037918
Calinski-Harabasz: 6477.291811481167
Gap: 0.16127504619049482
p-value: 0.0012884318180141137
Weighted Score: 0.7014983371334712
Time taken: 14.186952352523804 seconds
Memory usage: 19.7%

Starting clustering for k=29
Metrics for k = 29
Silhouette: 0.3708684940279979
Davies-Bouldin: 1.1520293080890907
Calinski-Harabasz: 5103.589222751283
Gap: 0.48211323498717285
p-value: 0.0012884318180141137
Weighted Score: 0.6038525603473738
Time taken: 16.413084983825684 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 29. Stopping further evaluations.
Updated weights: {'silhouette': 0.31724278532964445, 'davies_bouldin': 0.20609864824998317, 'calinski_harabasz': 0.09806540725275327, 'gap': 0.38013628427758, 'p_value': -0.0015431251099609147}
New best score: 0.7015 with k = 18
Contribution of silhouette: 0.2444
Contribution of davies_bouldin: 0.1844
Contribution of calinski_harabasz: 0.0536
Contribution of gap: 0.2309
Contribution of p_value: -0.0015
Contribution of regularization_term: -0.0102
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=3
Metrics for k = 3
Silhouette: 0.42455208866404776
Davies-Bouldin: 0.9143375411079001
Calinski-Harabasz: 8248.39678027866
Gap: -1.7832252841820768
p-value: 0.0012884318180141137
Weighted Score: 0.41493806296610714
Time taken: 9.299249172210693 seconds
Memory usage: 19.7%

Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5506220714328993
p-value: 0.0012884318180141137
Weighted Score: 0.48807323157208704
Time taken: 9.323930740356445 seconds
Memory usage: 19.7%

Starting clustering for k=7
Metrics for k = 7
Silhouette: 0.47021501382027187
Davies-Bouldin: 1.003482424812463
Calinski-Harabasz: 7952.129040670841
Gap: -0.9403000001959576
p-value: 0.0012884318180141137
Weighted Score: 0.5701759624305276
Time taken: 11.083195209503174 seconds
Memory usage: 19.7%

Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3910565083255211
p-value: 0.0012884318180141137
Weighted Score: 0.6745545729789075
Time taken: 12.64246392250061 seconds
Memory usage: 19.7%

Starting clustering for k=18
Metrics for k = 18
Silhouette: 0.4529420621344838
Davies-Bouldin: 1.1910870555037918
Calinski-Harabasz: 6477.291811481167
Gap: 0.16050654318218527
p-value: 0.0012884318180141137
Weighted Score: 0.6865012219507032
Time taken: 13.656834363937378 seconds
Memory usage: 19.7%

Starting clustering for k=29
Metrics for k = 29
Silhouette: 0.3708684940279979
Davies-Bouldin: 1.1520293080890907
Calinski-Harabasz: 5103.589222751283
Gap: 0.4811078659890917
p-value: 0.0012884318180141137
Weighted Score: 0.5824004299972163
Time taken: 16.13980269432068 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 29. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=3
Metrics for k = 3
Silhouette: 0.42455208866404776
Davies-Bouldin: 0.9143375411079001
Calinski-Harabasz: 8248.39678027866
Gap: -1.7823547898840015
p-value: 0.0012884318180141137
Weighted Score: 0.4176199655878503
Time taken: 9.239960193634033 seconds
Memory usage: 19.6%

Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5511203134768454
p-value: 0.0012884318180141137
Weighted Score: 0.4726969990616332
Time taken: 9.604708194732666 seconds
Memory usage: 19.7%

Starting clustering for k=7
Metrics for k = 7
Silhouette: 0.47021501382027187
Davies-Bouldin: 1.003482424812463
Calinski-Harabasz: 7952.129040670841
Gap: -0.9419990901916897
p-value: 0.0012884318180141137
Weighted Score: 0.5856672855616138
Time taken: 11.237284660339355 seconds
Memory usage: 19.7%

Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.39077628519734553
p-value: 0.0012884318180141137
Weighted Score: 0.7050500342332808
Time taken: 12.657928705215454 seconds
Memory usage: 19.7%

Starting clustering for k=18
Metrics for k = 18
Silhouette: 0.4529420621344838
Davies-Bouldin: 1.1910870555037918
Calinski-Harabasz: 6477.291811481167
Gap: 0.16015394732533217
p-value: 0.0012884318180141137
Weighted Score: 0.7149902997891306
Time taken: 14.371628999710083 seconds
Memory usage: 19.7%

Starting clustering for k=29
Metrics for k = 29
Silhouette: 0.3708684940279979
Davies-Bouldin: 1.1520293080890907
Calinski-Harabasz: 5103.589222751283
Gap: 0.481178261567452
p-value: 0.0012884318180141137
Weighted Score: 0.5971234663385507
Time taken: 16.90332269668579 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 29. Stopping further evaluations.
Updated weights: {'silhouette': 0.3515155516117944, 'davies_bouldin': 0.22836415318557693, 'calinski_harabasz': 0.10865973102798146, 'gap': 0.31317039809150143, 'p_value': -0.0017098339168541991}
New best score: 0.7150 with k = 18
Contribution of silhouette: 0.2708
Contribution of davies_bouldin: 0.2043
Contribution of calinski_harabasz: 0.0593
Contribution of gap: 0.1901
Contribution of p_value: -0.0017
Contribution of regularization_term: -0.0079
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=3
Metrics for k = 3
Silhouette: 0.42455208866404776
Davies-Bouldin: 0.9143375411079001
Calinski-Harabasz: 8248.39678027866
Gap: -1.7851762191197977
p-value: 0.0012884318180141137
Weighted Score: 0.38408585222175795
Time taken: 9.56912112236023 seconds
Memory usage: 19.7%

Starting clustering for k=4
Metrics for k = 4
Silhouette: 0.49246874333148943
Davies-Bouldin: 0.7071022562944114
Calinski-Harabasz: 10214.721129347836
Gap: -1.5510425912265369
p-value: 0.0012884318180141137
Weighted Score: 0.440356904951465
Time taken: 9.445777654647827 seconds
Memory usage: 19.7%

Starting clustering for k=7
Metrics for k = 7
Silhouette: 0.47021501382027187
Davies-Bouldin: 1.003482424812463
Calinski-Harabasz: 7952.129040670841
Gap: -0.9394005167327997
p-value: 0.0012884318180141137
Weighted Score: 0.558919028183284
Time taken: 11.00313949584961 seconds
Memory usage: 19.7%

Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3888871164174663
p-value: 0.0012884318180141137
Weighted Score: 0.6814524322702554
Time taken: 12.39128065109253 seconds
Memory usage: 19.7%

Starting clustering for k=18
Metrics for k = 18
Silhouette: 0.4529420621344838
Davies-Bouldin: 1.1910870555037918
Calinski-Harabasz: 6477.291811481167
Gap: 0.1606997640427874
p-value: 0.0012884318180141137
Weighted Score: 0.7046438666535947
Time taken: 13.938915967941284 seconds
Memory usage: 19.7%

Starting clustering for k=29
Metrics for k = 29
Silhouette: 0.3708684940279979
Davies-Bouldin: 1.1520293080890907
Calinski-Harabasz: 5103.589222751283
Gap: 0.48178915646630394
p-value: 0.0012884318180141137
Weighted Score: 0.606084412736063
Time taken: 16.42185354232788 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 29. Stopping further evaluations.
Updated k_values for next iteration: [11 12 14 15 17 20 22 25 29]
Iteration 6/100
Current k_values: [11 12 14 15 17 20 22 25 29]
Current best_k: 18, decreasing_k: None
Current temperature: 0.5905
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3896046145203158
p-value: 0.0016704927594445806
Weighted Score: 0.6465507777309423
Time taken: 12.190433263778687 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30200172458316565
p-value: 0.0016704927594445806
Weighted Score: 0.645996481974092
Time taken: 12.258670330047607 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3898205261804968
p-value: 0.0016704927594445806
Weighted Score: 0.7102798229758582
Time taken: 12.510987281799316 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3023482451214772
p-value: 0.0016704927594445806
Weighted Score: 0.6675075625144102
Time taken: 12.508832931518555 seconds
Memory usage: 19.6%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3895063997204655
p-value: 0.0016704927594445806
Weighted Score: 0.6815705253347404
Time taken: 12.02004337310791 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3016595055199005
p-value: 0.0016704927594445806
Weighted Score: 0.6728983758593488
Time taken: 12.31995701789856 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3903320166923958
p-value: 0.0016704927594445806
Weighted Score: 0.6756998642405871
Time taken: 11.968355655670166 seconds
Memory usage: 19.6%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30300682470584483
p-value: 0.0016704927594445806
Weighted Score: 0.639924071767361
Time taken: 12.070931673049927 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.390098772119865
p-value: 0.0016704927594445806
Weighted Score: 0.7063734231916368
Time taken: 12.672103881835938 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30151611832718395
p-value: 0.0016704927594445806
Weighted Score: 0.6780639525270902
Time taken: 12.076925992965698 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3900149880495576
p-value: 0.0016704927594445806
Weighted Score: 0.6601298261092463
Time taken: 12.355712652206421 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3032418886088468
p-value: 0.0016704927594445806
Weighted Score: 0.6412275260687271
Time taken: 11.98244309425354 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
Score decreased. Setting decreasing_k to 11
k_values not updated: best_k is None or not in current k_values
Reverted to best historical k: 18
Broadened search space due to missing best_k: [10 12 14 17 21 26 32 39 47 58]
Iteration 7/100
Current k_values: [10 12 14 17 21 26 32 39 47 58]
Current best_k: 18, decreasing_k: 11
Current temperature: 0.5314
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48917301285985104
p-value: 2.2950640251940528e-24
Weighted Score: 0.5021535150452515
Time taken: 11.827482223510742 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30095167294600955
p-value: 2.2950640251940528e-24
Weighted Score: 0.6278375824202681
Time taken: 11.900441884994507 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10311772334165958
p-value: 2.2950640251940528e-24
Weighted Score: 0.6555266803124524
Time taken: 12.608556270599365 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09417958464399945
p-value: 2.2950640251940528e-24
Weighted Score: 0.6304806950175621
Time taken: 13.033975601196289 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4890391082598704
p-value: 2.2950640251940528e-24
Weighted Score: 0.5700294800246749
Time taken: 12.00696325302124 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3030077870896637
p-value: 2.2950640251940528e-24
Weighted Score: 0.6524277517758704
Time taken: 12.035988330841064 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10399537679533388
p-value: 2.2950640251940528e-24
Weighted Score: 0.6866387734958723
Time taken: 12.911689758300781 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09405369606086644
p-value: 2.2950640251940528e-24
Weighted Score: 0.614093952450556
Time taken: 13.55273723602295 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4906990699003355
p-value: 2.2950640251940528e-24
Weighted Score: 0.5421873458141973
Time taken: 11.705810070037842 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.303149486184088
p-value: 2.2950640251940528e-24
Weighted Score: 0.6547199682607078
Time taken: 12.230549573898315 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10286283741461588
p-value: 2.2950640251940528e-24
Weighted Score: 0.689648690018125
Time taken: 12.756672859191895 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09329722248129713
p-value: 2.2950640251940528e-24
Weighted Score: 0.6307175668132357
Time taken: 13.988676309585571 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48896698306088915
p-value: 2.2950640251940528e-24
Weighted Score: 0.5315493704430938
Time taken: 11.883520603179932 seconds
Memory usage: 19.6%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30178041019134305
p-value: 2.2950640251940528e-24
Weighted Score: 0.6253149411947064
Time taken: 12.161725997924805 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10431863240947692
p-value: 2.2950640251940528e-24
Weighted Score: 0.6534090775604446
Time taken: 12.426618814468384 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09294031379355339
p-value: 2.2950640251940528e-24
Weighted Score: 0.6098059366040636
Time taken: 13.574024438858032 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.490255713616758
p-value: 2.2950640251940528e-24
Weighted Score: 0.5530243855253115
Time taken: 11.922698020935059 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3020890733814916
p-value: 2.2950640251940528e-24
Weighted Score: 0.6609627567585546
Time taken: 12.154294490814209 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10312696446728786
p-value: 2.2950640251940528e-24
Weighted Score: 0.6895937380244372
Time taken: 12.959443807601929 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09521683511910162
p-value: 2.2950640251940528e-24
Weighted Score: 0.6276570270736936
Time taken: 13.171172857284546 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.49062902402218356
p-value: 2.2950640251940528e-24
Weighted Score: 0.5294906630811418
Time taken: 11.896894216537476 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3010276205285667
p-value: 2.2950640251940528e-24
Weighted Score: 0.6275850270394889
Time taken: 12.092774629592896 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.103503070491195
p-value: 2.2950640251940528e-24
Weighted Score: 0.6609148466812405
Time taken: 12.899059057235718 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09593165141513893
p-value: 2.2950640251940528e-24
Weighted Score: 0.620419660575799
Time taken: 13.549324035644531 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
Score decreased. Setting decreasing_k to 14
Random restart with k_values: [12 32 14 17 58 26 47 39 10 21]
Iteration 8/100
Current k_values: [12 32 14 17 58 26 47 39 10 21]
Current best_k: 18, decreasing_k: 14
Current temperature: 0.4783
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3032644327464382
p-value: 2.3241806134906097e-289
Weighted Score: 0.6093224886391958
Time taken: 12.092697620391846 seconds
Memory usage: 19.7%

Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5492003505800103
p-value: 2.3241806134906097e-289
Weighted Score: 0.6031975859547353
Time taken: 16.8690664768219 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 32. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3029139328926309
p-value: 2.3241806134906097e-289
Weighted Score: 0.637520944152062
Time taken: 11.75756287574768 seconds
Memory usage: 19.6%

Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5496536995893635
p-value: 2.3241806134906097e-289
Weighted Score: 0.5516884854051576
Time taken: 16.56420087814331 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 32. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30105563476712227
p-value: 2.3241806134906097e-289
Weighted Score: 0.6371092899301756
Time taken: 12.547802925109863 seconds
Memory usage: 19.7%

Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5497039014956826
p-value: 2.3241806134906097e-289
Weighted Score: 0.5959724638407383
Time taken: 17.109704971313477 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 32. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30264768218937554
p-value: 2.3241806134906097e-289
Weighted Score: 0.6105422437464282
Time taken: 12.810782194137573 seconds
Memory usage: 19.7%

Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5491622263015827
p-value: 2.3241806134906097e-289
Weighted Score: 0.5545249053415447
Time taken: 16.58541512489319 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 32. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3029292810335438
p-value: 2.3241806134906097e-289
Weighted Score: 0.64383632432485
Time taken: 13.145213603973389 seconds
Memory usage: 19.7%

Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5499349743295543
p-value: 2.3241806134906097e-289
Weighted Score: 0.5638800055512676
Time taken: 16.785600662231445 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 32. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30191295445396804
p-value: 2.3241806134906097e-289
Weighted Score: 0.6135583792555168
Time taken: 12.523549318313599 seconds
Memory usage: 19.7%

Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5506919927038076
p-value: 2.3241806134906097e-289
Weighted Score: 0.591313040707802
Time taken: 16.606791973114014 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 32. Stopping further evaluations.
Score decreased. Setting decreasing_k to 12
k_values not updated: best_k is None or not in current k_values
Reverted to best historical k: 18
Broadened search space due to missing best_k: [11 12 14 17 19 23 26 31 36 42]
Iteration 9/100
Current k_values: [11 12 14 17 19 23 26 31 36 42]
Current best_k: 18, decreasing_k: 12
Current temperature: 0.4305
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.38855696497835446
p-value: 0.0016704927594445806
Weighted Score: 0.5790280267364961
Time taken: 12.418174266815186 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3026029051367676
p-value: 0.0016704927594445806
Weighted Score: 0.5916207702539344
Time taken: 12.48963212966919 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10434758857305226
p-value: 0.0016704927594445806
Weighted Score: 0.6241285634446067
Time taken: 12.501582145690918 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09428351600194063
p-value: 0.0016704927594445806
Weighted Score: 0.620081815466775
Time taken: 13.49627137184143 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.38912608102575774
p-value: 0.0016704927594445806
Weighted Score: 0.654888225239493
Time taken: 12.562711238861084 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.302418927573294
p-value: 0.0016704927594445806
Weighted Score: 0.6229927105646693
Time taken: 12.200440168380737 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3899649913973562
p-value: 0.0016704927594445806
Weighted Score: 0.6146322566726035
Time taken: 12.157890796661377 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3013178344483425
p-value: 0.0016704927594445806
Weighted Score: 0.6195249966359639
Time taken: 12.077965021133423 seconds
Memory usage: 19.7%

Starting clustering for k=14
Metrics for k = 14
Silhouette: 0.46819953376490947
Davies-Bouldin: 1.1811490699446894
Calinski-Harabasz: 6043.565711159591
Gap: -0.10441077871012183
p-value: 0.0016704927594445806
Weighted Score: 0.6587185611433597
Time taken: 12.807404518127441 seconds
Memory usage: 19.7%

Starting clustering for k=17
Metrics for k = 17
Silhouette: 0.3942106216724692
Davies-Bouldin: 1.1999757032378426
Calinski-Harabasz: 6248.565070118713
Gap: 0.09509252436337334
p-value: 0.0016704927594445806
Weighted Score: 0.6210844258324665
Time taken: 13.723757028579712 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 17. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.38951089997721944
p-value: 0.0016704927594445806
Weighted Score: 0.621018632855033
Time taken: 12.50057601928711 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30117005409259434
p-value: 0.0016704927594445806
Weighted Score: 0.5963552895195436
Time taken: 12.527367115020752 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3904790188157268
p-value: 0.0016704927594445806
Weighted Score: 0.6420806704406078
Time taken: 12.593712329864502 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3012048594254093
p-value: 0.0016704927594445806
Weighted Score: 0.6273332101098354
Time taken: 12.038749694824219 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=11
Metrics for k = 11
Silhouette: 0.51204775644305
Davies-Bouldin: 1.0577476830960313
Calinski-Harabasz: 7351.435515337752
Gap: -0.3908103212043841
p-value: 0.0016704927594445806
Weighted Score: 0.607501896611966
Time taken: 13.006670475006104 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3009413782578303
p-value: 0.0016704927594445806
Weighted Score: 0.6000525868864025
Time taken: 12.048277616500854 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 12. Stopping further evaluations.
Score decreased. Setting decreasing_k to 11
k_values not updated: best_k is None or not in current k_values
Reverted to best historical k: 18
Broadened search space due to missing best_k: [10 12 16 20 25 32 41 52 66 84]
Iteration 10/100
Current k_values: [10 12 16 20 25 32 41 52 66 84]
Current best_k: 18, decreasing_k: 11
Current temperature: 0.3874
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4903840125900185
p-value: 2.2950640251940528e-24
Weighted Score: 0.4663442896618617
Time taken: 12.467992305755615 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30269879502539965
p-value: 2.2950640251940528e-24
Weighted Score: 0.5778896124411481
Time taken: 12.577150106430054 seconds
Memory usage: 19.7%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.058745648858083754
p-value: 2.2950640251940528e-24
Weighted Score: 0.5837556794964277
Time taken: 13.18228530883789 seconds
Memory usage: 19.7%

Starting clustering for k=20
Metrics for k = 20
Silhouette: 0.42567735692120173
Davies-Bouldin: 1.2014602935787795
Calinski-Harabasz: 6734.4552356306485
Gap: 0.20484256329889128
p-value: 2.2950640251940528e-24
Weighted Score: 0.6579958695323084
Time taken: 14.145547151565552 seconds
Memory usage: 19.8%

Starting clustering for k=25
Metrics for k = 25
Silhouette: 0.32827835505549413
Davies-Bouldin: 1.2178850699531736
Calinski-Harabasz: 5195.134835961782
Gap: 0.37150188381138705
p-value: 2.2950640251940528e-24
Weighted Score: 0.6289678810681564
Time taken: 15.471388578414917 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 25. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4899925637690199
p-value: 2.2950640251940528e-24
Weighted Score: 0.5407009366196468
Time taken: 12.107798099517822 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.302615416026498
p-value: 2.2950640251940528e-24
Weighted Score: 0.6117216392784982
Time taken: 12.176707983016968 seconds
Memory usage: 19.7%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05781183185726313
p-value: 2.2950640251940528e-24
Weighted Score: 0.580159222377941
Time taken: 13.028123378753662 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 16. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.49096555897276417
p-value: 2.2950640251940528e-24
Weighted Score: 0.5071981564016753
Time taken: 11.458160877227783 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.3016076063783242
p-value: 2.2950640251940528e-24
Weighted Score: 0.6059821476403402
Time taken: 11.915805339813232 seconds
Memory usage: 19.7%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.0591386242235572
p-value: 2.2950640251940528e-24
Weighted Score: 0.5821124478549906
Time taken: 13.70838451385498 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 16. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48844960149012984
p-value: 2.2950640251940528e-24
Weighted Score: 0.5030575500065129
Time taken: 11.962337493896484 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30282513871113714
p-value: 2.2950640251940528e-24
Weighted Score: 0.5850611687606807
Time taken: 12.323456764221191 seconds
Memory usage: 19.7%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05914376068220939
p-value: 2.2950640251940528e-24
Weighted Score: 0.5792069798357156
Time taken: 13.739428043365479 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 16. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4905868533315445
p-value: 2.2950640251940528e-24
Weighted Score: 0.5205466602691825
Time taken: 12.312225103378296 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30304876330676755
p-value: 2.2950640251940528e-24
Weighted Score: 0.6143462820335692
Time taken: 12.562743186950684 seconds
Memory usage: 19.7%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05919713159814499
p-value: 2.2950640251940528e-24
Weighted Score: 0.5878498039823508
Time taken: 12.971161127090454 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 16. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4905761679823808
p-value: 2.2950640251940528e-24
Weighted Score: 0.5030173912581436
Time taken: 12.000534296035767 seconds
Memory usage: 19.7%

Starting clustering for k=12
Metrics for k = 12
Silhouette: 0.4519966495700511
Davies-Bouldin: 1.2142999036515316
Calinski-Harabasz: 6052.045017025215
Gap: -0.30295424911666125
p-value: 2.2950640251940528e-24
Weighted Score: 0.5893249595967178
Time taken: 12.262197732925415 seconds
Memory usage: 19.7%

Starting clustering for k=16
Metrics for k = 16
Silhouette: 0.400591153795337
Davies-Bouldin: 1.0754308691102485
Calinski-Harabasz: 6665.094390525242
Gap: 0.05777591324404874
p-value: 2.2950640251940528e-24
Weighted Score: 0.5857414526150361
Time taken: 13.057497262954712 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 16. Stopping further evaluations.
Score decreased. Setting decreasing_k to 12
Random restart with k_values: [32 10 12 52 66 20 16 41 25 84]
Iteration 11/100
Current k_values: [32 10 12 52 66 20 16 41 25 84]
Current best_k: 18, decreasing_k: 12
Current temperature: 0.3487
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5498132838532168
p-value: 0.0
Weighted Score: 0.6157213400130921
Time taken: 16.037464141845703 seconds
Memory usage: 19.7%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48953949769501115
p-value: 0.0
Weighted Score: 0.4778221905688192
Time taken: 11.644123077392578 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5516089728458713
p-value: 0.0
Weighted Score: 0.562108496199642
Time taken: 17.02775263786316 seconds
Memory usage: 19.7%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48847181471982815
p-value: 0.0
Weighted Score: 0.5502025572785332
Time taken: 11.653378963470459 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5505079254803356
p-value: 0.0
Weighted Score: 0.6087613558530214
Time taken: 17.187886238098145 seconds
Memory usage: 19.6%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4889056979166604
p-value: 0.0
Weighted Score: 0.5186675893768344
Time taken: 12.42633867263794 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5482826927995745
p-value: 0.0
Weighted Score: 0.5647512999363401
Time taken: 16.73349952697754 seconds
Memory usage: 19.6%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.4890774505964437
p-value: 0.0
Weighted Score: 0.5120641867863337
Time taken: 11.89181661605835 seconds
Memory usage: 19.6%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.5500944111280255
p-value: 0.0
Weighted Score: 0.5773512632242447
Time taken: 17.055359840393066 seconds
Memory usage: 19.6%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.48867077329169106
p-value: 0.0
Weighted Score: 0.5311107125698716
Time taken: 12.26427674293518 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 10. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=32
Metrics for k = 32
Silhouette: 0.3372609401739697
Davies-Bouldin: 1.0877361297826096
Calinski-Harabasz: 5031.994817570158
Gap: 0.550843322266104
p-value: 0.0
Weighted Score: 0.6023803042974695
Time taken: 17.302189350128174 seconds
Memory usage: 19.7%

Starting clustering for k=10
Metrics for k = 10
Silhouette: 0.48073257007175846
Davies-Bouldin: 0.8804284705290373
Calinski-Harabasz: 5975.4686386763005
Gap: -0.489077760106424
p-value: 0.0
Weighted Score: 0.5116704005507277
Time taken: 11.83802580833435 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 10. Stopping further evaluations.
Score decreased. Setting decreasing_k to 32
k_values not updated: best_k is None or not in current k_values
Reverted to best historical k: 18
Broadened search space due to missing best_k: [ 31  37  45  54  65  79  95 115 139 168]
Iteration 12/100
Current k_values: [ 31  37  45  54  65  79  95 115 139 168]
Current best_k: 18, decreasing_k: 32
Current temperature: 0.3138
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=31
Metrics for k = 31
Silhouette: 0.3326794915207368
Davies-Bouldin: 1.1303125491257437
Calinski-Harabasz: 5198.700195332594
Gap: 0.5242490743648407
p-value: 0.0
Weighted Score: 0.6339863366676489
Time taken: 17.13040828704834 seconds
Memory usage: 19.6%

Starting clustering for k=37
Metrics for k = 37
Silhouette: 0.3515640561784081
Davies-Bouldin: 1.0854352052599867
Calinski-Harabasz: 4493.391229423317
Gap: 0.6482065399395207
p-value: 0.0
Weighted Score: 0.6470364432530066
Time taken: 17.65653419494629 seconds
Memory usage: 19.7%

Starting clustering for k=45
Metrics for k = 45
Silhouette: 0.2862532352663683
Davies-Bouldin: 1.178461900102786
Calinski-Harabasz: 3726.2600556110983
Gap: 0.8058709691416812
p-value: 0.0
Weighted Score: 0.6604275793415032
Time taken: 18.28075385093689 seconds
Memory usage: 19.7%

Starting clustering for k=54
Metrics for k = 54
Silhouette: 0.30324300008686755
Davies-Bouldin: 1.1291132882474006
Calinski-Harabasz: 3079.364643165692
Gap: 0.9263695784385302
p-value: 0.0
Weighted Score: 0.6711775060453682
Time taken: 19.344488859176636 seconds
Memory usage: 19.7%

Starting clustering for k=65
Metrics for k = 65
Silhouette: 0.28903123096248823
Davies-Bouldin: 1.1320551939289403
Calinski-Harabasz: 2940.515521590834
Gap: 1.06982858732772
p-value: 0.0
Weighted Score: 0.6890401084036701
Time taken: 20.862053394317627 seconds
Memory usage: 19.7%

Starting clustering for k=79
Metrics for k = 79
Silhouette: 0.25241010630889943
Davies-Bouldin: 1.1617404237557305
Calinski-Harabasz: 6343.82249457714
Gap: 1.190422394821196
p-value: 0.0
Weighted Score: 0.7317620032759097
Time taken: 24.03915286064148 seconds
Memory usage: 19.7%

Starting clustering for k=95
Metrics for k = 95
Silhouette: 0.2592910670968547
Davies-Bouldin: 1.2141806887087294
Calinski-Harabasz: 2080.236295541497
Gap: 1.320310778532047
p-value: 0.0
Weighted Score: 0.7344855686460356
Time taken: 24.28937268257141 seconds
Memory usage: 19.7%

Starting clustering for k=115
Metrics for k = 115
Silhouette: 0.2609848786300438
Davies-Bouldin: 1.1629335753147048
Calinski-Harabasz: 6363.417516779815
Gap: 1.4466338722552603
p-value: 0.0
Weighted Score: 0.7823624541632148
Time taken: 25.62118363380432 seconds
Memory usage: 19.7%

Starting clustering for k=139
Metrics for k = 139
Silhouette: 0.28269255257012543
Davies-Bouldin: 1.1007690838653734
Calinski-Harabasz: 3342.2869195149738
Gap: 1.596410869683348
p-value: 0.0
Weighted Score: 0.7702772470828487
Time taken: 27.395174264907837 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 139. Stopping further evaluations.
Updated weights: {'silhouette': 0.1322804609648756, 'davies_bouldin': 0.15612121510935167, 'calinski_harabasz': 0.07428525451522225, 'gap': 0.6384819978304042, 'p_value': -0.0011689284198537653}
New best score: 0.7824 with k = 115
Contribution of silhouette: 0.0044
Contribution of davies_bouldin: 0.1319
Contribution of calinski_harabasz: 0.0395
Contribution of gap: 0.6172
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0106
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=31
Metrics for k = 31
Silhouette: 0.3326794915207368
Davies-Bouldin: 1.1303125491257437
Calinski-Harabasz: 5198.700195332594
Gap: 0.524989441803946
p-value: 0.0
Weighted Score: 0.6006399178023649
Time taken: 15.576667785644531 seconds
Memory usage: 19.7%

Starting clustering for k=37
Metrics for k = 37
Silhouette: 0.3515640561784081
Davies-Bouldin: 1.0854352052599867
Calinski-Harabasz: 4493.391229423317
Gap: 0.6472939839346434
p-value: 0.0
Weighted Score: 0.6187318376505119
Time taken: 17.758429050445557 seconds
Memory usage: 19.7%

Starting clustering for k=45
Metrics for k = 45
Silhouette: 0.2862532352663683
Davies-Bouldin: 1.178461900102786
Calinski-Harabasz: 3726.2600556110983
Gap: 0.8061965349016766
p-value: 0.0
Weighted Score: 0.6077262495572809
Time taken: 18.838796377182007 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 45. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=31
Metrics for k = 31
Silhouette: 0.3326794915207368
Davies-Bouldin: 1.1303125491257437
Calinski-Harabasz: 5198.700195332594
Gap: 0.5240029780990696
p-value: 0.0
Weighted Score: 0.6562617287340486
Time taken: 15.975854635238647 seconds
Memory usage: 19.7%

Starting clustering for k=37
Metrics for k = 37
Silhouette: 0.3515640561784081
Davies-Bouldin: 1.0854352052599867
Calinski-Harabasz: 4493.391229423317
Gap: 0.6455284777258541
p-value: 0.0
Weighted Score: 0.6792033173299242
Time taken: 18.092287302017212 seconds
Memory usage: 19.7%

Starting clustering for k=45
Metrics for k = 45
Silhouette: 0.2862532352663683
Davies-Bouldin: 1.178461900102786
Calinski-Harabasz: 3726.2600556110983
Gap: 0.8059332781176636
p-value: 0.0
Weighted Score: 0.7046872553944273
Time taken: 18.806455612182617 seconds
Memory usage: 19.7%

Starting clustering for k=54
Metrics for k = 54
Silhouette: 0.30324300008686755
Davies-Bouldin: 1.1291132882474006
Calinski-Harabasz: 3079.364643165692
Gap: 0.9253774464212867
p-value: 0.0
Weighted Score: 0.7245985365149892
Time taken: 19.350144147872925 seconds
Memory usage: 19.7%

Starting clustering for k=65
Metrics for k = 65
Silhouette: 0.28903123096248823
Davies-Bouldin: 1.1320551939289403
Calinski-Harabasz: 2940.515521590834
Gap: 1.0679974133020593
p-value: 0.0
Weighted Score: 0.745875687503381
Time taken: 21.07566738128662 seconds
Memory usage: 19.7%

Starting clustering for k=79
Metrics for k = 79
Silhouette: 0.25241010630889943
Davies-Bouldin: 1.1617404237557305
Calinski-Harabasz: 6343.82249457714
Gap: 1.1902143670636391
p-value: 0.0
Weighted Score: 0.7476307956269569
Time taken: 23.72830104827881 seconds
Memory usage: 19.7%

Starting clustering for k=95
Metrics for k = 95
Silhouette: 0.2592910670968547
Davies-Bouldin: 1.2141806887087294
Calinski-Harabasz: 2080.236295541497
Gap: 1.3180858189727918
p-value: 0.0
Weighted Score: 0.805975819918014
Time taken: 24.81772804260254 seconds
Memory usage: 19.7%

Starting clustering for k=115
Metrics for k = 115
Silhouette: 0.2609848786300438
Davies-Bouldin: 1.1629335753147048
Calinski-Harabasz: 6363.417516779815
Gap: 1.4459674502561484
p-value: 0.0
Weighted Score: 0.8029236845237586
Time taken: 25.1647629737854 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 115. Stopping further evaluations.
Updated weights: {'silhouette': 0.14657114788351872, 'davies_bouldin': 0.17298749596604066, 'calinski_harabasz': -0.02572270967842411, 'gap': 0.7074592773744092, 'p_value': -0.0012952115455443388}
New best score: 0.8060 with k = 95
Contribution of silhouette: 0.0039
Contribution of davies_bouldin: 0.1619
Contribution of calinski_harabasz: -0.0003
Contribution of gap: 0.6529
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0123
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=31
Metrics for k = 31
Silhouette: 0.3326794915207368
Davies-Bouldin: 1.1303125491257437
Calinski-Harabasz: 5198.700195332594
Gap: 0.5249812682825397
p-value: 0.0
Weighted Score: 0.6311833780852817
Time taken: 17.59800124168396 seconds
Memory usage: 19.6%

Starting clustering for k=37
Metrics for k = 37
Silhouette: 0.3515640561784081
Davies-Bouldin: 1.0854352052599867
Calinski-Harabasz: 4493.391229423317
Gap: 0.6478997794258756
p-value: 0.0
Weighted Score: 0.6442762347526474
Time taken: 17.302465200424194 seconds
Memory usage: 19.7%

Starting clustering for k=45
Metrics for k = 45
Silhouette: 0.2862532352663683
Davies-Bouldin: 1.178461900102786
Calinski-Harabasz: 3726.2600556110983
Gap: 0.8059477236184289
p-value: 0.0
Weighted Score: 0.6583101352748715
Time taken: 18.332090139389038 seconds
Memory usage: 19.7%

Starting clustering for k=54
Metrics for k = 54
Silhouette: 0.30324300008686755
Davies-Bouldin: 1.1291132882474006
Calinski-Harabasz: 3079.364643165692
Gap: 0.9262783571221789
p-value: 0.0
Weighted Score: 0.6692238674257421
Time taken: 20.22786021232605 seconds
Memory usage: 19.7%

Starting clustering for k=65
Metrics for k = 65
Silhouette: 0.28903123096248823
Davies-Bouldin: 1.1320551939289403
Calinski-Harabasz: 2940.515521590834
Gap: 1.0666310669914676
p-value: 0.0
Weighted Score: 0.6865519055395398
Time taken: 21.079769611358643 seconds
Memory usage: 19.7%

Starting clustering for k=79
Metrics for k = 79
Silhouette: 0.25241010630889943
Davies-Bouldin: 1.1617404237557305
Calinski-Harabasz: 6343.82249457714
Gap: 1.189798902427862
p-value: 0.0
Weighted Score: 0.7267394205509888
Time taken: 22.226519346237183 seconds
Memory usage: 19.7%

Starting clustering for k=95
Metrics for k = 95
Silhouette: 0.2592910670968547
Davies-Bouldin: 1.2141806887087294
Calinski-Harabasz: 2080.236295541497
Gap: 1.319355897987279
p-value: 0.0
Weighted Score: 0.7318200406845723
Time taken: 25.210952281951904 seconds
Memory usage: 19.7%

Starting clustering for k=115
Metrics for k = 115
Silhouette: 0.2609848786300438
Davies-Bouldin: 1.1629335753147048
Calinski-Harabasz: 6363.417516779815
Gap: 1.445160743586742
p-value: 0.0
Weighted Score: 0.7770433643166952
Time taken: 25.59364676475525 seconds
Memory usage: 19.8%

Starting clustering for k=139
Metrics for k = 139
Silhouette: 0.28269255257012543
Davies-Bouldin: 1.1007690838653734
Calinski-Harabasz: 3342.2869195149738
Gap: 1.595505250756121
p-value: 0.0
Weighted Score: 0.7720689898278638
Time taken: 27.84455633163452 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 139. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=31
Metrics for k = 31
Silhouette: 0.3326794915207368
Davies-Bouldin: 1.1303125491257437
Calinski-Harabasz: 5198.700195332594
Gap: 0.5238733789233763
p-value: 0.0
Weighted Score: 0.6608870187560983
Time taken: 16.564164876937866 seconds
Memory usage: 19.7%

Starting clustering for k=37
Metrics for k = 37
Silhouette: 0.3515640561784081
Davies-Bouldin: 1.0854352052599867
Calinski-Harabasz: 4493.391229423317
Gap: 0.6469241556190539
p-value: 0.0
Weighted Score: 0.6829540039875543
Time taken: 17.419426679611206 seconds
Memory usage: 19.7%

Starting clustering for k=45
Metrics for k = 45
Silhouette: 0.2862532352663683
Davies-Bouldin: 1.178461900102786
Calinski-Harabasz: 3726.2600556110983
Gap: 0.8057261461687215
p-value: 0.0
Weighted Score: 0.706087902534364
Time taken: 18.929052352905273 seconds
Memory usage: 19.7%

Starting clustering for k=54
Metrics for k = 54
Silhouette: 0.30324300008686755
Davies-Bouldin: 1.1291132882474006
Calinski-Harabasz: 3079.364643165692
Gap: 0.9260884332682746
p-value: 0.0
Weighted Score: 0.7247457937657412
Time taken: 19.40356731414795 seconds
Memory usage: 19.7%

Starting clustering for k=65
Metrics for k = 65
Silhouette: 0.28903123096248823
Davies-Bouldin: 1.1320551939289403
Calinski-Harabasz: 2940.515521590834
Gap: 1.0669873284179605
p-value: 0.0
Weighted Score: 0.7437288012250166
Time taken: 20.53294348716736 seconds
Memory usage: 19.7%

Starting clustering for k=79
Metrics for k = 79
Silhouette: 0.25241010630889943
Davies-Bouldin: 1.1617404237557305
Calinski-Harabasz: 6343.82249457714
Gap: 1.191277601995976
p-value: 0.0
Weighted Score: 0.7424024172512101
Time taken: 22.90779185295105 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 79. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=31
Metrics for k = 31
Silhouette: 0.3326794915207368
Davies-Bouldin: 1.1303125491257437
Calinski-Harabasz: 5198.700195332594
Gap: 0.5242272318156598
p-value: 0.0
Weighted Score: 0.6683824359019398
Time taken: 16.15998911857605 seconds
Memory usage: 19.7%

Starting clustering for k=37
Metrics for k = 37
Silhouette: 0.3515640561784081
Davies-Bouldin: 1.0854352052599867
Calinski-Harabasz: 4493.391229423317
Gap: 0.6475531317004801
p-value: 0.0
Weighted Score: 0.6926891557285434
Time taken: 17.72299075126648 seconds
Memory usage: 19.7%

Starting clustering for k=45
Metrics for k = 45
Silhouette: 0.2862532352663683
Davies-Bouldin: 1.178461900102786
Calinski-Harabasz: 3726.2600556110983
Gap: 0.8055039779975566
p-value: 0.0
Weighted Score: 0.7194374307715513
Time taken: 18.5836021900177 seconds
Memory usage: 19.7%

Starting clustering for k=54
Metrics for k = 54
Silhouette: 0.30324300008686755
Davies-Bouldin: 1.1291132882474006
Calinski-Harabasz: 3079.364643165692
Gap: 0.9261122634503209
p-value: 0.0
Weighted Score: 0.7408123830777124
Time taken: 18.801812648773193 seconds
Memory usage: 19.7%

Starting clustering for k=65
Metrics for k = 65
Silhouette: 0.28903123096248823
Davies-Bouldin: 1.1320551939289403
Calinski-Harabasz: 2940.515521590834
Gap: 1.0673584353210384
p-value: 0.0
Weighted Score: 0.7635066433992741
Time taken: 21.01963210105896 seconds
Memory usage: 19.7%

Starting clustering for k=79
Metrics for k = 79
Silhouette: 0.25241010630889943
Davies-Bouldin: 1.1617404237557305
Calinski-Harabasz: 6343.82249457714
Gap: 1.1906943490105633
p-value: 0.0
Weighted Score: 0.768475665036998
Time taken: 24.09129786491394 seconds
Memory usage: 19.7%

Starting clustering for k=95
Metrics for k = 95
Silhouette: 0.2592910670968547
Davies-Bouldin: 1.2141806887087294
Calinski-Harabasz: 2080.236295541497
Gap: 1.3198518779242736
p-value: 0.0
Weighted Score: 0.8249531400404535
Time taken: 24.80540919303894 seconds
Memory usage: 19.8%

Starting clustering for k=115
Metrics for k = 115
Silhouette: 0.2609848786300438
Davies-Bouldin: 1.1629335753147048
Calinski-Harabasz: 6363.417516779815
Gap: 1.4458198038961
p-value: 0.0
Weighted Score: 0.8250903126542101
Time taken: 25.822075128555298 seconds
Memory usage: 19.7%

Starting clustering for k=139
Metrics for k = 139
Silhouette: 0.28269255257012543
Davies-Bouldin: 1.1007690838653734
Calinski-Harabasz: 3342.2869195149738
Gap: 1.5965329653473948
p-value: 0.0
Weighted Score: 0.8581515142445534
Time taken: 27.793015480041504 seconds
Memory usage: 19.7%

Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.7456587867331592
p-value: 0.0
Weighted Score: 0.8676550666960702
Time taken: 31.364935636520386 seconds
Memory usage: 19.7%

Updated weights: {'silhouette': 0.13294435182178566, 'davies_bouldin': 0.15690475824584185, 'calinski_harabasz': -0.023331255944148848, 'gap': 0.7346569409291693, 'p_value': -0.0011747950526479263}
New best score: 0.8677 with k = 168
Contribution of silhouette: 0.0000
Contribution of davies_bouldin: 0.1432
Contribution of calinski_harabasz: -0.0067
Contribution of gap: 0.7347
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0034
Best k is the highest value tested. Expanding search to higher values.
Updated k_values for next iteration: [168 178 190 202 216 230 245 261 278 296 315 336]
Iteration 13/100
Current k_values: [168 178 190 202 216 230 245 261 278 296 315 336]
Current best_k: 168, decreasing_k: None
Current temperature: 0.2824
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.7464908119076075
p-value: 0.0
Weighted Score: 0.9528162185089545
Time taken: 31.874239444732666 seconds
Memory usage: 19.6%

Starting clustering for k=178
Metrics for k = 178
Silhouette: 0.25053981563601474
Davies-Bouldin: 1.1797557590820715
Calinski-Harabasz: 2503.5172830436836
Gap: 1.7825044744105458
p-value: 0.0
Weighted Score: 0.9526769438858583
Time taken: 31.299279928207397 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 178. Stopping further evaluations.
Updated weights: {'silhouette': 0.03927352002413922, 'davies_bouldin': 0.17385568780702695, 'calinski_harabasz': -0.02585180714033113, 'gap': 0.8140243112788579, 'p_value': -0.0013017119696929929}
New best score: 0.9528 with k = 168
Contribution of silhouette: 0.0000
Contribution of davies_bouldin: 0.1587
Contribution of calinski_harabasz: -0.0075
Contribution of gap: 0.8140
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0124
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.7466021526719029
p-value: 0.0
Weighted Score: 0.8566691204487467
Time taken: 30.374233961105347 seconds
Memory usage: 19.7%

Starting clustering for k=178
Metrics for k = 178
Silhouette: 0.25053981563601474
Davies-Bouldin: 1.1797557590820715
Calinski-Harabasz: 2503.5172830436836
Gap: 1.7838033646155278
p-value: 0.0
Weighted Score: 0.8649835721345148
Time taken: 32.06727933883667 seconds
Memory usage: 19.7%

Starting clustering for k=190
Metrics for k = 190
Silhouette: 0.26016493027669935
Davies-Bouldin: 1.1921107318712918
Calinski-Harabasz: 4292.9491108882485
Gap: 1.8307627455754805
p-value: 0.0
Weighted Score: 0.8679977922160222
Time taken: 33.51038980484009 seconds
Memory usage: 19.7%

Starting clustering for k=202
Metrics for k = 202
Silhouette: 0.23939002458087594
Davies-Bouldin: 1.2152093287587817
Calinski-Harabasz: 1832.8453378225372
Gap: 1.8845294319321848
p-value: 0.0
Weighted Score: 0.8743643473406075
Time taken: 33.80215096473694 seconds
Memory usage: 19.7%

Starting clustering for k=216
Metrics for k = 216
Silhouette: 0.2624187053316361
Davies-Bouldin: 1.1900232565646718
Calinski-Harabasz: 2201.4162951688677
Gap: 1.9271620749614993
p-value: 0.0
Weighted Score: 0.8771591722099492
Time taken: 34.45160984992981 seconds
Memory usage: 19.7%

Starting clustering for k=230
Metrics for k = 230
Silhouette: 0.25839423454630056
Davies-Bouldin: 1.1438878131438805
Calinski-Harabasz: 3771.1536209322867
Gap: 1.9798366367194102
p-value: 0.0
Weighted Score: 0.8579830912984577
Time taken: 36.23720026016235 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 230. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.747248797446769
p-value: 0.0
Weighted Score: 0.9708702084706636
Time taken: 31.712217092514038 seconds
Memory usage: 19.7%

Starting clustering for k=178
Metrics for k = 178
Silhouette: 0.25053981563601474
Davies-Bouldin: 1.1797557590820715
Calinski-Harabasz: 2503.5172830436836
Gap: 1.7837094138580536
p-value: 0.0
Weighted Score: 1.0027170687360314
Time taken: 31.260462045669556 seconds
Memory usage: 19.7%

Starting clustering for k=190
Metrics for k = 190
Silhouette: 0.26016493027669935
Davies-Bouldin: 1.1921107318712918
Calinski-Harabasz: 4292.9491108882485
Gap: 1.831768843410197
p-value: 0.0
Weighted Score: 0.9901501109627133
Time taken: 33.06909894943237 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 190. Stopping further evaluations.
Updated weights: {'silhouette': 0.04351636567771659, 'davies_bouldin': 0.19263788122662268, 'calinski_harabasz': -0.13667790264856636, 'gap': 0.901965995876851, 'p_value': -0.001442340132623815}
New best score: 1.0027 with k = 178
Contribution of silhouette: 0.0018
Contribution of davies_bouldin: 0.1685
Contribution of calinski_harabasz: -0.0109
Contribution of gap: 0.8576
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0143
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.745935227781013
p-value: 0.0
Weighted Score: 0.9076753432972416
Time taken: 29.778773307800293 seconds
Memory usage: 19.7%

Starting clustering for k=178
Metrics for k = 178
Silhouette: 0.25053981563601474
Davies-Bouldin: 1.1797557590820715
Calinski-Harabasz: 2503.5172830436836
Gap: 1.7825955000431914
p-value: 0.0
Weighted Score: 0.9160404746926986
Time taken: 32.44896221160889 seconds
Memory usage: 19.8%

Starting clustering for k=190
Metrics for k = 190
Silhouette: 0.26016493027669935
Davies-Bouldin: 1.1921107318712918
Calinski-Harabasz: 4292.9491108882485
Gap: 1.8300305648610227
p-value: 0.0
Weighted Score: 0.9243620125113636
Time taken: 32.32883048057556 seconds
Memory usage: 19.7%

Starting clustering for k=202
Metrics for k = 202
Silhouette: 0.23939002458087594
Davies-Bouldin: 1.2152093287587817
Calinski-Harabasz: 1832.8453378225372
Gap: 1.8851901396346697
p-value: 0.0
Weighted Score: 0.9489080694303296
Time taken: 33.26551604270935 seconds
Memory usage: 19.7%

Starting clustering for k=216
Metrics for k = 216
Silhouette: 0.2624187053316361
Davies-Bouldin: 1.1900232565646718
Calinski-Harabasz: 2201.4162951688677
Gap: 1.9259908509761008
p-value: 0.0
Weighted Score: 0.9514462685608837
Time taken: 34.400471210479736 seconds
Memory usage: 19.7%

Starting clustering for k=230
Metrics for k = 230
Silhouette: 0.25839423454630056
Davies-Bouldin: 1.1438878131438805
Calinski-Harabasz: 3771.1536209322867
Gap: 1.9807652425819153
p-value: 0.0
Weighted Score: 0.9418156507776635
Time taken: 35.072654008865356 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 230. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.7472328351475817
p-value: 0.0
Weighted Score: 0.986837930460206
Time taken: 30.054664134979248 seconds
Memory usage: 19.7%

Starting clustering for k=178
Metrics for k = 178
Silhouette: 0.25053981563601474
Davies-Bouldin: 1.1797557590820715
Calinski-Harabasz: 2503.5172830436836
Gap: 1.78219362663177
p-value: 0.0
Weighted Score: 1.0208017040963557
Time taken: 30.87332248687744 seconds
Memory usage: 19.7%

Starting clustering for k=190
Metrics for k = 190
Silhouette: 0.26016493027669935
Davies-Bouldin: 1.1921107318712918
Calinski-Harabasz: 4292.9491108882485
Gap: 1.8314240769090127
p-value: 0.0
Weighted Score: 1.0058352703194422
Time taken: 31.337659120559692 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 190. Stopping further evaluations.
Updated weights: {'silhouette': 0.048217579698300934, 'davies_bouldin': 0.21344917587437415, 'calinski_harabasz': -0.15144365944439486, 'gap': 0.8913750646834914, 'p_value': -0.0015981608117715401}
New best score: 1.0208 with k = 178
Contribution of silhouette: 0.0020
Contribution of davies_bouldin: 0.1867
Contribution of calinski_harabasz: -0.0121
Contribution of gap: 0.8470
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0028
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=168
Metrics for k = 168
Silhouette: 0.24638469523828757
Davies-Bouldin: 1.2011020583620957
Calinski-Harabasz: 4357.295454833163
Gap: 1.7483408494725676
p-value: 0.0
Weighted Score: 0.982743180550102
Time taken: 30.66683554649353 seconds
Memory usage: 19.7%

Starting clustering for k=178
Metrics for k = 178
Silhouette: 0.25053981563601474
Davies-Bouldin: 1.1797557590820715
Calinski-Harabasz: 2503.5172830436836
Gap: 1.7840866811461478
p-value: 0.0
Weighted Score: 1.0145407164027287
Time taken: 31.692529916763306 seconds
Memory usage: 19.7%

Starting clustering for k=190
Metrics for k = 190
Silhouette: 0.26016493027669935
Davies-Bouldin: 1.1921107318712918
Calinski-Harabasz: 4292.9491108882485
Gap: 1.8302903730550462
p-value: 0.0
Weighted Score: 1.0014287106222708
Time taken: 33.33097839355469 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 190. Stopping further evaluations.
Broadened search space: [167 194 227 265 310 361 422 493 575 672]
Iteration 14/100
Current k_values: [167 194 227 265 310 361 422 493 575 672]
Current best_k: 178, decreasing_k: None
Current temperature: 0.2542
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.7369431461612024
p-value: 0.0
Weighted Score: 1.1090156812853522
Time taken: 29.64404010772705 seconds
Memory usage: 19.6%

Starting clustering for k=194
Metrics for k = 194
Silhouette: 0.24836513241659816
Davies-Bouldin: 1.1850340096454925
Calinski-Harabasz: 5254.05224068945
Gap: 1.8496527678221444
p-value: 0.0
Weighted Score: 1.0721764764175714
Time taken: 32.67036056518555 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 194. Stopping further evaluations.
Updated weights: {'silhouette': -0.059573666297773574, 'davies_bouldin': 0.21452043730818188, 'calinski_harabasz': -0.15220372680512798, 'gap': 0.9988631374762634, 'p_value': -0.001606181681543749}
New best score: 1.1090 with k = 167
Contribution of silhouette: -0.0023
Contribution of davies_bouldin: 0.1879
Contribution of calinski_harabasz: -0.0000
Contribution of gap: 0.9379
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0144
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.7368161824998225
p-value: 0.0
Weighted Score: 1.0087689215486582
Time taken: 31.003005027770996 seconds
Memory usage: 19.7%

Starting clustering for k=194
Metrics for k = 194
Silhouette: 0.24836513241659816
Davies-Bouldin: 1.1850340096454925
Calinski-Harabasz: 5254.05224068945
Gap: 1.8486466295124817
p-value: 0.0
Weighted Score: 0.9746565865358434
Time taken: 33.099011182785034 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 194. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.735313740500759
p-value: 0.0
Weighted Score: 1.2277119846580278
Time taken: 31.216856956481934 seconds
Memory usage: 19.7%

Starting clustering for k=194
Metrics for k = 194
Silhouette: 0.24836513241659816
Davies-Bouldin: 1.1850340096454925
Calinski-Harabasz: 5254.05224068945
Gap: 1.8497922824625501
p-value: 0.0
Weighted Score: 1.1397223236118121
Time taken: 33.62107491493225 seconds
Memory usage: 19.6%

Score consistently decreasing for k = 194. Stopping further evaluations.
Updated weights: {'silhouette': -0.0660096025460095, 'davies_bouldin': 0.23769577541072787, 'calinski_harabasz': -0.2766800297009729, 'gap': 1.1067735595304857, 'p_value': -0.0017797026942313013}
New best score: 1.2277 with k = 167
Contribution of silhouette: -0.0025
Contribution of davies_bouldin: 0.2082
Contribution of calinski_harabasz: -0.0000
Contribution of gap: 1.0387
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0166
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.7358904506100306
p-value: 0.0
Weighted Score: 1.1129415756059895
Time taken: 30.07760715484619 seconds
Memory usage: 19.7%

Starting clustering for k=194
Metrics for k = 194
Silhouette: 0.24836513241659816
Davies-Bouldin: 1.1850340096454925
Calinski-Harabasz: 5254.05224068945
Gap: 1.848049080353265
p-value: 0.0
Weighted Score: 1.073564788412268
Time taken: 32.95856690406799 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 194. Stopping further evaluations.
Best k is the lowest value tested. Expanding search to lower values.
Updated k_values for next iteration: [166 169 172 175 179 182 186 190 194]
Iteration 15/100
Current k_values: [166 169 172 175 179 182 186 190 194]
Current best_k: 167, decreasing_k: None
Current temperature: 0.2288
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.7373882010033217
p-value: 0.0
Weighted Score: 1.2279643504542475
Time taken: 31.200329065322876 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7449251113526518
p-value: 0.0
Weighted Score: 1.2275539159151352
Time taken: 31.35175371170044 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 169. Stopping further evaluations.
Updated weights: {'silhouette': -0.17437413337454868, 'davies_bouldin': 0.23888872598612104, 'calinski_harabasz': -0.17505420664540838, 'gap': 1.1123282487225439, 'p_value': -0.001788634688708037}
New best score: 1.2280 with k = 166
Contribution of silhouette: -0.0150
Contribution of davies_bouldin: 0.2144
Contribution of calinski_harabasz: -0.0000
Contribution of gap: 1.0445
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0160
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.738264363801978
p-value: 0.0
Weighted Score: 1.1212554514992896
Time taken: 31.834859609603882 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7474538143217302
p-value: 0.0
Weighted Score: 1.118040343414914
Time taken: 30.781323671340942 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 169. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.737142208536035
p-value: 0.0
Weighted Score: 1.3605449109414436
Time taken: 30.250317573547363 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7461604942635773
p-value: 0.0
Weighted Score: 1.3508165038692397
Time taken: 30.66582465171814 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 169. Stopping further evaluations.
Updated weights: {'silhouette': -0.19321233614908445, 'davies_bouldin': 0.2646966492921009, 'calinski_harabasz': -0.30199912093674064, 'gap': 1.2324966744848134, 'p_value': -0.001981866691089238}
New best score: 1.3605 with k = 166
Contribution of silhouette: -0.0166
Contribution of davies_bouldin: 0.2376
Contribution of calinski_harabasz: -0.0000
Contribution of gap: 1.1573
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0178
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.7378671534265973
p-value: 0.0
Weighted Score: 1.2334307851023252
Time taken: 31.86542820930481 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7466094206828018
p-value: 0.0
Weighted Score: 1.2329037408952028
Time taken: 30.96518063545227 seconds
Memory usage: 19.8%

Score consistently decreasing for k = 169. Stopping further evaluations.
Best k is the lowest value tested. Expanding search to lower values.
Updated k_values for next iteration: [165 166 167 168 169]
Iteration 16/100
Current k_values: [165 166 167 168 169]
Current best_k: 166, decreasing_k: None
Current temperature: 0.2059
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=165
Metrics for k = 165
Silhouette: 0.24813684972359126
Davies-Bouldin: 1.1987153096507945
Calinski-Harabasz: 1323.9118138059268
Gap: 1.7313696182460117
p-value: 0.0
Weighted Score: 1.3741661821454965
Time taken: 32.19249391555786 seconds
Memory usage: 19.7%

Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.7384720142783916
p-value: 0.0
Weighted Score: 1.358550299191963
Time taken: 29.79911518096924 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 166. Stopping further evaluations.
Updated weights: {'silhouette': -0.3022152736719839, 'davies_bouldin': 0.26602511219613034, 'calinski_harabasz': -0.20050036965771884, 'gap': 1.2386823444423725, 'p_value': -0.001991813308800058}
New best score: 1.3742 with k = 165
Contribution of silhouette: -0.0097
Contribution of davies_bouldin: 0.2416
Contribution of calinski_harabasz: -0.0013
Contribution of gap: 1.1613
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0178
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=165
Metrics for k = 165
Silhouette: 0.24813684972359126
Davies-Bouldin: 1.1987153096507945
Calinski-Harabasz: 1323.9118138059268
Gap: 1.7339699215737063
p-value: 0.0
Weighted Score: 1.249294524070008
Time taken: 31.7470703125 seconds
Memory usage: 19.7%

Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.7368945778488545
p-value: 0.0
Weighted Score: 1.2389522361749532
Time taken: 32.21479415893555 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 166. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=165
Metrics for k = 165
Silhouette: 0.24813684972359126
Davies-Bouldin: 1.1987153096507945
Calinski-Harabasz: 1323.9118138059268
Gap: 1.7326983358597978
p-value: 0.0
Weighted Score: 1.5230128137804941
Time taken: 32.22262191772461 seconds
Memory usage: 19.7%

Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.7368428666553832
p-value: 0.0
Weighted Score: 1.505369435863518
Time taken: 31.22094750404358 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 166. Stopping further evaluations.
Updated weights: {'silhouette': -0.3348645691656332, 'davies_bouldin': 0.29476466725333, 'calinski_harabasz': -0.33019431541021477, 'gap': 1.372501212678529, 'p_value': -0.002206995356011144}
New best score: 1.5230 with k = 165
Contribution of silhouette: -0.0107
Contribution of davies_bouldin: 0.2677
Contribution of calinski_harabasz: -0.0021
Contribution of gap: 1.2872
Contribution of p_value: -0.0000
Contribution of regularization_term: -0.0191
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=165
Metrics for k = 165
Silhouette: 0.24813684972359126
Davies-Bouldin: 1.1987153096507945
Calinski-Harabasz: 1323.9118138059268
Gap: 1.732604318593408
p-value: 0.0
Weighted Score: 1.3810801740754053
Time taken: 31.155048608779907 seconds
Memory usage: 19.7%

Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.7361728996947647
p-value: 0.0
Weighted Score: 1.3643154315701438
Time taken: 31.27557110786438 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 166. Stopping further evaluations.
Random restart with k_values: [167 169 165 166 168]
Iteration 17/100
Current k_values: [167 169 165 166 168]
Current best_k: 165, decreasing_k: None
Current temperature: 0.1853
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.7375555143722092
p-value: 0.0
Weighted Score: 1.5161692010614487
Time taken: 30.413487911224365 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7468265603115647
p-value: 0.0
Weighted Score: 1.5103561120507298
Time taken: 30.331194162368774 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 169. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.7367374457516185
p-value: 0.0
Weighted Score: 1.2470082352173524
Time taken: 30.469367027282715 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7457784914321444
p-value: 0.0
Weighted Score: 1.2444492092764978
Time taken: 31.53173542022705 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 169. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.6%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.736996966028718
p-value: 0.0
Weighted Score: 1.5198482098570611
Time taken: 30.303804874420166 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7459257979383889
p-value: 0.0
Weighted Score: 1.5063791966450968
Time taken: 30.334426403045654 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 169. Stopping further evaluations.
X_scaled_df shape: (26193, 18)
Memory usage: 19.7%
Starting clustering for k=167
Metrics for k = 167
Silhouette: 0.24982732754121492
Davies-Bouldin: 1.1803235402616319
Calinski-Harabasz: 1337.8441091682132
Gap: 1.737699520150068
p-value: 0.0
Weighted Score: 1.2449483967791162
Time taken: 30.66981077194214 seconds
Memory usage: 19.7%

Starting clustering for k=169
Metrics for k = 169
Silhouette: 0.2532310572629465
Davies-Bouldin: 1.209588822252241
Calinski-Harabasz: 2073.76926870658
Gap: 1.7461300175853935
p-value: 0.0
Weighted Score: 1.2487034227870928
Time taken: 31.4293053150177 seconds
Memory usage: 19.7%

Starting clustering for k=165
Metrics for k = 165
Silhouette: 0.24813684972359126
Davies-Bouldin: 1.1987153096507945
Calinski-Harabasz: 1323.9118138059268
Gap: 1.7328294178359354
p-value: 0.0
Weighted Score: 1.253326181342946
Time taken: 30.992268562316895 seconds
Memory usage: 19.7%

Starting clustering for k=166
Metrics for k = 166
Silhouette: 0.2627930725895195
Davies-Bouldin: 1.1927100271446498
Calinski-Harabasz: 1267.8958950737701
Gap: 1.738219880764266
p-value: 0.0
Weighted Score: 1.2380532591802145
Time taken: 30.55031442642212 seconds
Memory usage: 19.7%

Score consistently decreasing for k = 166. Stopping further evaluations.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-19-8d3db60325e2> in <cell line: 849>()
    847
    848 # Refine weights and find the best clustering
--> 849 best_k, best_score, final_weights, best_clustering = refine_weights(
    850     X_scaled, broad_range, current_zoning_data, proposed_zoning_data, params,
    851     initial_weights, W, , , max_iter, metric_targets, normalization_ranges,

2 frames
<ipython-input-19-8d3db60325e2> in determine_num_samples(range_size, base_samples, max_samples)
    608     '''
    609     scaling_factor = np.log1p(range_size) / np.log1p(1000)  # log1p is log(1 + x)
--> 610     num_samples = base_samples + int((max_samples - base_samples) * scaling_factor)
    611     return min(max_samples, max(base_samples, num_samples))
    612

ValueError: cannot convert float NaN to integer

"""

import numpy as np
import pickle  # For saving and loading the state

def refine_weights(X_scaled_df, k_values, current_zoning_data, proposed_zoning_data, params,
                   initial_weights, W, , , max_iter, metric_targets, normalization_ranges,
                   previous_evaluations, min_k=2, max_k=None, state_file=None):
    # Load previous state if provided
    if state_file:
        with open(state_file, 'rb') as f:
            state = pickle.load(f)
        best_evaluation = state.get('best_evaluation', None)
        best_k = state.get('best_k', None)
        decreasing_k = state.get('decreasing_k', None)
        weights = state.get('weights', normalize_weights(initial_weights.copy()))
        weights_previous = state.get('weights_previous', weights.copy())
        peak_score = state.get('peak_score', float('-inf'))
        previous_best_k = state.get('previous_best_k', None)
        historical_max_k = state.get('historical_max_k', max(k_values))
        max_possible_k = state.get('max_possible_k', len(X_scaled_df))
        no_improvement_iterations = state.get('no_improvement_iterations', 0)
        max_broaden_restarts = state.get('max_broaden_restarts', 10)
        broaden_restarts_count = state.get('broaden_restarts_count', 0)
        tried_k_values_sets = state.get('tried_k_values_sets', set([tuple(k_values)]))
        k_history = state.get('k_history', {})
        initial_temperature = state.get('initial_temperature', 1.0)
        final_temperature = state.get('final_temperature', 0.01)
        alpha = state.get('alpha', 0.9)
        temperature = state.get('temperature', initial_temperature)
        iter_start = state.get('iter_start', 0)
    else:
        best_evaluation = None
        best_k = None
        decreasing_k = None
        weights = normalize_weights(initial_weights.copy())
        weights_previous = weights.copy()
        peak_score = float('-inf')
        previous_best_k = None
        historical_max_k = max(k_values)
        max_possible_k = len(X_scaled_df)
        no_improvement_iterations = 0
        max_broaden_restarts = 10  # Maximum number of times to broaden or restart
        broaden_restarts_count = 0
        tried_k_values_sets = set([tuple(k_values)])
        k_history = {}
        initial_temperature = 1.0
        final_temperature = 0.01
        alpha = 0.9  # Cooling rate
        temperature = initial_temperature
        iter_start = 0

    if max_k is None:
        max_k = max_possible_k

    # Adjust k_values to the provided min_k and max_k
    k_values = np.unique(np.geomspace(min_k, max_k, len(k_values)).astype(int))
    tried_k_values_sets.add(tuple(k_values))

    for iter in range(iter_start, max_iter):
        print(f"Iteration {iter + 1}/{max_iter}")
        print(f"Current k_values: {k_values}")
        print(f"Current best_k: {best_k}, decreasing_k: {decreasing_k}")
        print(f"Current temperature: {temperature:.4f}")

        iteration_best_score = float('-inf')
        iteration_best_k = None
        best_iteration_weights = weights.copy()

        all_targets_reached = check_all_targets_reached(weights, metric_targets)

        for metric in weights:
            if check_metric_target_reached(metric, weights[metric], metric_targets):
                continue

            for adjustment in [-W, W]:
                temp_weights = weights.copy()
                temp_weights[metric] += adjustment
                temp_weights = normalize_weights(temp_weights)

                evaluation, normalization_ranges = evaluate_adjustment(
                    X_scaled_df, k_values, temp_weights, weights, , iter, adjustment, metric, metric_targets, normalization_ranges
                )

                delta_score = evaluation.score - iteration_best_score

                if delta_score > 0 or np.exp(delta_score / temperature) > np.random.rand():
                    iteration_best_score = evaluation.score
                    iteration_best_k = evaluation.k
                    best_iteration_weights = temp_weights.copy()

                if best_evaluation is None or evaluation.score > best_evaluation.score:
                    best_evaluation = evaluation
                    best_k = evaluation.k
                    weights = temp_weights
                    weights_previous = weights.copy()

                    print(f"Updated weights: {weights}")
                    print(f"New best score: {best_evaluation.score:.4f} with k = {best_k}")
                    for key, value in best_evaluation.contributions.items():
                        print(f"Contribution of {key}: {value:.4f}")

        weights = best_iteration_weights.copy()

        # Update decreasing_k between iterations
        if iteration_best_score < peak_score:
            decreasing_k = iteration_best_k
            print(f"Score decreased. Setting decreasing_k to {decreasing_k}")
        else:
            peak_score = iteration_best_score
            decreasing_k = None  # Reset decreasing_k if score didn't decrease

        # Check for convergence
        if all_targets_reached and previous_best_k == best_k:
            print("Convergence reached.")
            break

        # Update previous_best_k for the next iteration
        previous_best_k = best_k

        # Periodically broaden the search space or do random restarts only if no improvement
        if no_improvement_iterations >= 3 or (iter + 1) % 10 == 0:
            if broaden_restarts_count < max_broaden_restarts:
                if np.random.rand() < 0.5:  # 50% chance
                    if min(k_values) > 2 or max(k_values) < max_possible_k:
                        new_k_values = broaden_search_space(k_values, max_possible_k)
                        print("Attempting to broaden search space:", new_k_values)
                    else:
                        print("Search space is already at the boundaries, not broadening further.")
                else:
                    new_k_values = np.random.choice(k_values, len(k_values), replace=False)
                    print("Attempting random restart with k_values:", new_k_values)

                # Ensure new_k_values is unique
                if tuple(new_k_values) in tried_k_values_sets:
                    print("New k_values set has been tried before, generating a new set.")
                    new_k_values = broaden_search_space(k_values, max_possible_k)
                else:
                    k_values = new_k_values
                    tried_k_values_sets.add(tuple(new_k_values))
                no_improvement_iterations = 0  # Reset no improvement counter
                broaden_restarts_count += 1  # Increment broaden/restart counter

        # Update k_values for the next iteration
        if best_k is not None and best_k in k_values:
            best_k_index = np.where(k_values == best_k)[0][0]

            if best_k_index > 0 and best_k_index + 1 < len(k_values):
                k_min = k_values[best_k_index - 1]
                k_max = k_values[best_k_index + 1]
                range_size = k_max - k_min
                num_samples = determine_num_samples(range_size)
                new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))
                print(f"Updated k_values for next iteration: {new_k_values}")
            elif best_k_index == 0:
                print("Best k is the lowest value tested. Expanding search to lower values.")
                k_min = max(2, k_values[0] - 1)
                k_max = k_values[1]
                range_size = k_max - k_min
                num_samples = determine_num_samples(range_size)
                new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))
            elif best_k_index == len(k_values) - 1:
                print("Best k is the highest value tested. Expanding search to higher values.")
                k_min = k_values[best_k_index]
                k_max = min(historical_max_k * 2, max_possible_k)
                range_size = k_max - k_min
                num_samples = determine_num_samples(range_size)
                new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))

            k_values = new_k_values
            tried_k_values_sets.add(tuple(k_values))
        else:
            print("k_values not updated: best_k is None or not in current k_values")
            no_improvement_iterations += 1

        # Cooling schedule for simulated annealing
        temperature = max(final_temperature, alpha * temperature)

        # Save current state to a file
        if state_file:
            state = {
                'best_evaluation': best_evaluation,
                'best_k': best_k,
                'decreasing_k': decreasing_k,
                'weights': weights,
                'weights_previous': weights_previous,
                'peak_score': peak_score,
                'previous_best_k': previous_best_k,
                'historical_max_k': historical_max_k,
                'max_possible_k': max_possible_k,
                'no_improvement_iterations': no_improvement_iterations,
                'max_broaden_restarts': max_broaden_restarts,
                'broaden_restarts_count': broaden_restarts_count,
                'tried_k_values_sets': tried_k_values_sets,
                'k_history': k_history,
                'initial_temperature': initial_temperature,
                'final_temperature': final_temperature,
                'alpha': alpha,
                'temperature': temperature,
                'iter_start': iter + 1
            }
            with open(state_file, 'wb') as f:
                pickle.dump(state, f)

    # Final calculation of actual values for all metrics
    if best_evaluation:
        calculate_final_metrics(X_scaled_df, best_evaluation)

    return best_evaluation.k if best_evaluation else None, best_evaluation.score if best_evaluation else float('-inf'), weights, best_evaluation.clustering if best_evaluation else None

# Helper function to update k_values
def update_k_values(k_values, best_k, historical_max_k, max_possible_k, determine_num_samples, k_history, X_scaled_df):
    if best_k is not None and best_k in k_values:
        k_min, k_max = determine_k_range(k_values, best_k, max_possible_k)
        range_size = k_max - k_min
        num_samples = determine_num_samples(range_size)
        new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))
        print(f"Updated k_values for next iteration: {new_k_values}")
    else:
        print("k_values not updated: best_k is None or not in current k_values")
        # Revert to the best historical k if current best_k is None or not in k_values
        if k_history:
            best_k = max(k_history, key=k_history.get)
            print(f"Reverted to best historical k: {best_k}")
            best_k_index = np.where(k_values == best_k)[0][0] if best_k in k_values else None

            if best_k_index is not None:
                k_min, k_max = determine_k_range(k_values, best_k, max_possible_k)
                range_size = k_max - k_min
                num_samples = determine_num_samples(range_size)
                new_k_values = np.unique(np.geomspace(k_min, k_max, num_samples).astype(int))
                print(f"Updated k_values for next iteration: {new_k_values}")
            else:
                k_values = broaden_search_space(k_values, max_possible_k)
                print("Broadened search space due to missing best_k:", k_values)
        else:
            k_values = broaden_search_space(k_values, max_possible_k)
            print("Broadened search space due to empty k_history:", k_values)
    return new_k_values

def determine_k_range(k_values, best_k, max_possible_k):
    best_k_index = np.where(k_values == best_k)[0][0]
    if best_k_index > 0 and best_k_index + 1 < len(k_values):
        k_min = k_values[best_k_index - 1]
        k_max = k_values[best_k_index + 1]
    elif best_k_index == 0:
        print("Best k is the lowest value tested. Expanding search to lower values.")
        k_min = max(2, k_values[0] - 1)
        k_max = k_values[1]
    elif best_k_index == len(k_values) - 1:
        print("Best k is the highest value tested. Expanding search to higher values.")
        k_min = k_values[best_k_index]
        k_max = min(historical_max_k * 2, max_possible_k)
    return k_min, k_max

def calculate_weighted_k_values(k_history, num_samples, max_possible_k):
    weighted_scores = np.array(list(k_history.values()))
    total_weight = np.sum(weighted_scores)
    probabilities = weighted_scores / total_weight
    k_values = np.array(list(k_history.keys()))
    new_k_values = np.random.choice(k_values, num_samples, p=probabilities, replace=False)
    return np.unique(np.clip(new_k_values, 2, max_possible_k))

# Define a simple broaden search space function as a placeholder
def broaden_search_space(k_values, max_possible_k):
    # Simple example implementation
    new_k_values = np.unique(np.geomspace(2, max_possible_k, len(k_values)).astype(int))
    return new_k_values

# Example function for normalization
def normalize_weights(weights):
    total = sum(weights.values())
    return {k: v / total for k, v in weights.items()}

# Example functions for placeholders
def check_all_targets_reached(weights, metric_targets):
    return all(weights[metric] >= target for metric, target in metric_targets.items())

def check_metric_target_reached(metric, weight, metric_targets):
    return weight >= metric_targets.get(metric, float('inf'))

def evaluate_adjustment(X_scaled_df, k_values, temp_weights, weights, , iter, adjustment, metric, metric_targets, normalization_ranges):
    # Dummy implementation, replace with actual evaluation logic
    evaluation = lambda: None
    evaluation.score = np.random.rand()  # Replace with actual score calculation
    evaluation.k = np.random.choice(k_values)
    evaluation.contributions = {metric: np.random.rand() for metric in weights}
    return evaluation, normalization_ranges

def determine_num_samples(range_size):
    return max(2, int(range_size / 10))  # Example heuristic for number of samples

def calculate_final_metrics(X_scaled_df, best_evaluation):
    # Placeholder for final metric calculation
    pass

# Perform K-Means clustering with k=165
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.stats import f_oneway

# Assuming merged_gdf_r1_r5 is your dataframe
# List of relevant features for scaling
features = [
    'LotArea', 'proposed_floor_area',
    'max_height', 'LotFront', 'LotDepth', 'Block', 'max_floors', 'max_floors_current',
    'max_residential_far_current', 'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'min_rear_yard_depth_proposed',
    'min_side_yard_width_proposed', 'max_building_height_proposed'
]

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(merged_gdf_r1_r5[features])

# Perform K-Means clustering
k = 80
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_scaled)

# Calculate metrics
silhouette_avg = silhouette_score(X_scaled, labels)
davies_bouldin_avg = davies_bouldin_score(X_scaled, labels)
calinski_harabasz_avg = calinski_harabasz_score(X_scaled, labels)

# Calculate p-value using ANOVA
combined_results = merged_gdf_r1_r5.copy()
combined_results['cluster'] = labels
npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
anova_result_npv = f_oneway(*npv_clusters)
p_value = anova_result_npv.pvalue

# Print results
print(f"Results for K-Means clustering with k={k}:")
print(f"Silhouette Score: {silhouette_avg:.4f}")
print(f"Davies-Bouldin Score: {davies_bouldin_avg:.4f}")
print(f"Calinski-Harabasz Score: {calinski_harabasz_avg:.4f}")
print(f"ANOVA p-value: {p_value:.4e}")

# Add cluster labels to the original dataframe
merged_gdf_r1_r5['cluster'] = labels

# Display the first few rows of the dataframe with cluster labels
print("\nFirst few rows of the dataframe with cluster labels:")
print(merged_gdf_r1_r5[['LotArea', 'proposed_floor_area', 'cluster']].head())

# Display cluster sizes
cluster_sizes = merged_gdf_r1_r5['cluster'].value_counts().sort_index()
print("\nCluster sizes:")
print(cluster_sizes)

import numpy as np
import pandas as pd
import statsmodels.stats.power as smp
from sklearn.utils import resample
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming merged_gdf_r1_r5 and current_zoning_data, proposed_zoning_data are already loaded
def analyze_redevelopment_potential_single(gdf, zoning_data, params):
    try:
        # Check if gdf is None or empty
        if gdf is None or gdf.empty:
            raise ValueError("Input GeoDataFrame is None or empty")

        # Ensure gdf is a GeoDataFrame and has at least one row
        if not isinstance(gdf, gpd.GeoDataFrame) or len(gdf) == 0:
            raise ValueError("Input must be a non-empty GeoDataFrame")

        # Get the first row of the GeoDataFrame
        row = gdf.iloc[0]

        zone_dist = row.get('ZoneDist1')
        if zone_dist is None or zone_dist not in zoning_data['Residential FAR (max)']:
            raise ValueError(f"Invalid or missing zoning district: {zone_dist}")

        proposed_far = zoning_data['Residential FAR (max)'].get(zone_dist)
        if proposed_far is None:
            raise ValueError(f"Missing FAR value for zone: {zone_dist}")

        front_yard = zoning_data['Front Yard Depth (min)'].get(zone_dist, 0)
        rear_yard = zoning_data['Rear Yard Depth (min)'].get(zone_dist, 0)
        side_yards = zoning_data['Side Yards (total width)'].get(zone_dist, 0) / 2
        max_height = zoning_data['Building Height (max)'].get(zone_dist, np.inf)

        buildable_width = max(0, row['LotFront'] - 2 * side_yards)
        buildable_depth = max(0, row['LotDepth'] - front_yard - rear_yard)
        buildable_footprint = buildable_width * buildable_depth

        max_floors = calculate_max_floors(max_height, params['floor_height'])[0]
        proposed_floor_area = min(row['LotArea'] * proposed_far, buildable_footprint * max_floors)

        land_value = row['LotArea'] * params['land_value_per_sqft'][0]
        demolition_cost = row['BldgArea'] * params['demolition_cost_per_sqft'][0]
        construction_cost = proposed_floor_area * params['construction_cost_per_sqft'][0]
        total_development_cost = land_value + demolition_cost + construction_cost

        current_value = row['BldgArea'] * params['market_value_per_sqft'][0]
        redeveloped_value = proposed_floor_area * params['market_value_per_sqft'][0]

        equity_investment = total_development_cost * (1 - params['debt_ratio'][0])
        debt = total_development_cost * params['debt_ratio'][0]

        annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
        annual_debt_service = debt * (params['interest_rate'][0] * (1 + params['interest_rate'][0])**params['holding_period_years'][0]) / ((1 + params['interest_rate'][0])**params['holding_period_years'][0] - 1)

        sale_price = redeveloped_value * (1 + params['appreciation_rate'][0])**params['holding_period_years'][0]

        cashflows = np.column_stack((
            -equity_investment,
            np.tile(annual_noi - annual_debt_service, (params['holding_period_years'][0] - 1, 1)).T,
            (annual_noi - annual_debt_service + sale_price - debt).reshape(-1, 1)
        ))

        if np.any(np.isinf(cashflows)) or np.any(np.isnan(cashflows)):
            print("Cashflows contain infs or NaNs")
            return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[row.name])

        npv_value = safe_npv(params['discount_rate'][0], cashflows)
        irr_value = safe_irr(cashflows)
        equity_multiple = np.sum(cashflows, axis=1) / equity_investment

        return pd.DataFrame({
            'current_value': current_value,
            'redevelopment_cost': total_development_cost,
            'redeveloped_value': redeveloped_value,
            'npv': npv_value,
            'irr': irr_value,
            'equity_multiple': equity_multiple,
            'proposed_floor_area': proposed_floor_area,
            'max_floors': max_floors
        }, index=[row.name])
    except Exception as e:
        print(f"Error in analyze_redevelopment_potential_single: {e}")
        return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.index[0] if len(gdf) > 0 else 0])

# Function to calculate weighted CV for a cluster
def calculate_weighted_cv(cluster, feature_importances):
    weighted_variances = []
    for feature, importance in feature_importances.items():
        if feature in cluster.columns:
            mean = np.mean(cluster[feature])
            std = np.std(cluster[feature])
            if mean != 0 and not np.isnan(mean) and not np.isnan(std):
                cv = std / mean
                weighted_variances.append((cv * importance) ** 2)
    if not weighted_variances:
        raise ValueError("No valid features found in the cluster data")
    weighted_cv = np.sqrt(np.sum(weighted_variances))
    return weighted_cv

# Function to determine weighted CVs for clusters
# Function to determine weighted CVs for clusters
def determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column='cluster'):
    cluster_weighted_cvs = {}
    for cluster_id in clustered_data[cluster_column].unique():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        try:
            weighted_cv = calculate_weighted_cv(cluster_data, feature_importances)
            cluster_weighted_cvs[cluster_id] = weighted_cv
        except ValueError as e:
            print(f"Cluster {cluster_id}: {e}")
    return cluster_weighted_cvs

# Function to calculate sample size using power analysis
def calculate_sample_size_power(effect_size, alpha=0.05, power=0.80):
    return int(np.ceil(smp.tt_ind_solve_power(effect_size=effect_size, alpha=alpha, power=power)))

# Function to perform Monte Carlo simulations
def monte_carlo_simulation(cluster_data, sample_size, num_simulations=100, current_zoning_data=None, proposed_zoning_data=None):
    results = []
    for _ in tqdm(range(num_simulations), desc="Monte Carlo simulation"):
        sample = resample(cluster_data, n_samples=sample_size)
        sim_result = analyze_redevelopment_potential_single(sample, current_zoning_data, proposed_zoning_data)
        results.append(sim_result['equity_multiple'].mean())
    return np.mean(results), np.std(results)

# Function to determine sample sizes for each cluster
def determine_cluster_sample_sizes(clustered_data, feature_importances, cluster_column='cluster', alpha=0.05, power=0.80, min_effect_size=0.01):
    cluster_sample_sizes = {}
    cluster_weighted_cvs = determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column)

    for cluster_id, weighted_cv in tqdm(cluster_weighted_cvs.items(), desc="Calculating sample sizes"):
        effect_size = max(weighted_cv, min_effect_size)  # Ensure effect size is non-zero
        if effect_size == 0:
            print(f"Skipping cluster {cluster_id} due to zero effect size.")
            continue
        sample_size = calculate_sample_size_power(effect_size, alpha=alpha, power=power)
        cluster_sample_sizes[cluster_id] = min(sample_size, len(clustered_data[clustered_data[cluster_column] == cluster_id]))

    return cluster_sample_sizes

def analyze_clusters(clustered_data, sample_sizes, cluster_column='cluster', current_zoning_data=None, proposed_zoning_data=None, min_samples=100):
    equity_multiples = []

    for cluster_id, sample_size in sample_sizes.items():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]

        # Ensure that we sample at least the minimum number of times
        num_samples = max(sample_size, min_samples)

        # Run the Monte Carlo simulation with the determined sample size
        dist_samples = []
        for _ in range(num_samples):
            sample = resample(cluster_data, n_samples=sample_size)
            sim_result = analyze_redevelopment_potential_single(sample, current_zoning_data, proposed_zoning_data)
            if sim_result is not None and 'equity_multiple' in sim_result.columns and not sim_result['equity_multiple'].isna().all():
                dist_samples.extend(sim_result['equity_multiple'].values.tolist())
            else:
                print(f"Warning: Simulation for cluster {cluster_id} returned invalid result")

        if dist_samples:
            # Calculate the mean equity multiple for this cluster
            mean_em = np.mean(dist_samples)
            equity_multiples.append((cluster_id, mean_em))

            # Store the distribution statistics
            clustered_data.loc[clustered_data[cluster_column] == cluster_id, 'equity_multiple_mean'] = mean_em
            clustered_data.loc[clustered_data[cluster_column] == cluster_id, 'equity_multiple_std'] = np.std(dist_samples)
            clustered_data.loc[clustered_data[cluster_column] == cluster_id, 'equity_multiple_min'] = np.min(dist_samples)
            clustered_data.loc[clustered_data[cluster_column] == cluster_id, 'equity_multiple_max'] = np.max(dist_samples)
        else:
            print(f"Warning: No valid samples for cluster {cluster_id}")

    return sorted(equity_multiples, key=lambda x: x[1], reverse=True)







# Function to visualize equity multiple distributions
def visualize_equity_multiple_distribution(clustered_data, selected_clusters, cluster_column='cluster'):
    fig, ax = plt.subplots(figsize=(10, 6))
    for cluster_id in selected_clusters:
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        mean = cluster_data['equity_multiple_mean'].iloc[0]
        std = cluster_data['equity_multiple_std'].iloc[0]
        min_val = cluster_data['equity_multiple_min'].iloc[0]
        max_val = cluster_data['equity_multiple_max'].iloc[0]

        # Generate a normal distribution based on mean and std
        x = np.linspace(min_val, max_val, 100)
        y = stats.norm.pdf(x, mean, std)

        ax.plot(x, y, label=f'Cluster {cluster_id}')
        ax.axvline(mean, color='k', linestyle='dashed', linewidth=1)

    ax.set_title('Estimated Equity Multiple Distribution for Selected Clusters')
    ax.set_xlabel('Equity Multiple')
    ax.set_ylabel('Density')
    ax.legend()
    plt.show()

# Example feature importances dictionary
feature_importances = {
    'max_residential_far_proposed': 0.69,
    'max_residential_far_current': 0.69,
    'max_building_height_proposed': 0.59,
    'max_floors': 0.59,
    'max_building_height_current': 0.59,
    'min_front_yard_depth_proposed': 0.59,
    'min_front_yard_depth_current': 0.59,
    'max_floors_current': 0.58,
    'LotFront': 0.23,
    'LotArea': 0.22,
    'proposed_floor_area': 0.20,
    'Block': 0.16,
    'LotDepth': 0.10,
    'max_height': 0.06
}

# Determine sample sizes for each cluster
sample_sizes = determine_cluster_sample_sizes(merged_gdf_r1_r5, feature_importances)

# Analyze clusters and get the best performing ones by equity multiple
best_clusters = analyze_clusters(merged_gdf_r1_r5, sample_sizes)

# Select top clusters for visualization
top_clusters = [cluster_id for cluster_id, _ in best_clusters[:5]]

# Visualize the equity multiple distributions for the top clusters
visualize_equity_multiple_distribution(merged_gdf_r1_r5, top_clusters)

import pandas as pd
import geopandas as gpd
import numpy as np
import json

# Assuming merged_gdf_r1_r5, current_zoning_data, and proposed_zoning_data are already defined

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

def print_df_info(df, name):
    print(f"\n{name} info:")
    print(df)
    print(df.dtypes)

# Generate a single set of parameters
params = sample_parameters(1)

# Print the sampled parameters
print("\nSampled Parameters:")
for key, value in params.items():
    print(f"{key}: {value[0]}")

# Select a single row from merged_gdf_r1_r5 for testing
test_row = merged_gdf_r1_r5.iloc[[0]].copy()

# Assign example values to NaN columns
test_row['equity_multiple_distribution'] = json.dumps([1.5, 2.0, 2.5])  # Store as JSON string
test_row['equity_multiple_mean'] = 2.0
test_row['equity_multiple_std'] = 0.5
test_row['equity_multiple_min'] = 1.5
test_row['equity_multiple_max'] = 2.5

print("\nTest row after assigning values to NaN columns:")
print_df_info(test_row, "test_row")

# Function to run a single analysis with detailed logging
def run_single_analysis(row, zoning_data, params):
    print("\nStarting analyze_redevelopment_potential_single")
    print(f"Input row:\n{row}")
    print(f"Zoning data keys: {zoning_data.keys()}")
    print(f"Params: {params}")

    try:
        result = analyze_redevelopment_potential_single(row, zoning_data, params)
        print("\nAnalysis completed successfully")
        print(f"Result:\n{result}")
        return result
    except Exception as e:
        print(f"\nAn error occurred: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return None

# Run the analysis with current zoning data
print("\nRunning analysis with current zoning data:")
current_result = run_single_analysis(test_row, current_zoning_data, params)

# Run the analysis with proposed zoning data
print("\nRunning analysis with proposed zoning data:")
proposed_result = run_single_analysis(test_row, proposed_zoning_data, params)

# Compare results
if current_result is not None and proposed_result is not None:
    print("\nComparison of results:")
    for col in current_result.columns:
        current_val = current_result[col].iloc[0]
        proposed_val = proposed_result[col].iloc[0]
        diff = proposed_val - current_val
        print(f"{col}:")
        print(f"  Current: {current_val}")
        print(f"  Proposed: {proposed_val}")
        print(f"  Difference: {diff}")
        print(f"  Percentage Change: {(diff/current_val)*100 if current_val != 0 else 'N/A'}%")
        print()
else:
    print("\nUnable to compare results due to errors in analysis")

# Check if the original test_row was modified
print("\nChecking if original test_row was modified:")
print_df_info(test_row, "test_row after analysis")

import pandas as pd
import geopandas as gpd
import numpy as np
import json
from sklearn.utils import resample

# Assuming merged_gdf_r1_r5, current_zoning_data, and proposed_zoning_data are already defined

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

def print_df_info(df, name):
    print(f"\n{name} info:")
    print(df)
    print(df.dtypes)

# Function to calculate equity multiple distribution for a cluster
def calculate_equity_multiple_distribution(cluster_data, zoning_data, num_samples=100):
    equity_multiples = []
    for _ in range(num_samples):
        sample = resample(cluster_data, n_samples=1)
        params = sample_parameters(1)
        result = analyze_redevelopment_potential_single(sample, zoning_data, params)
        if result is not None and 'equity_multiple' in result.columns:
            equity_multiples.append(result['equity_multiple'].iloc[0])
    return equity_multiples

# Generate a single set of parameters
params = sample_parameters(1)

# Print the sampled parameters
print("\nSampled Parameters:")
for key, value in params.items():
    print(f"{key}: {value[0]}")

# Select a single row from merged_gdf_r1_r5 for testing
test_row = merged_gdf_r1_r5.iloc[[0]].copy()
cluster_id = test_row['cluster'].iloc[0]

# Get all rows for this cluster
cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster_id]

print(f"\nCalculating equity multiple distribution for cluster {cluster_id}")
equity_multiple_distribution = calculate_equity_multiple_distribution(cluster_data, current_zoning_data)

# Assign calculated values to the test row
test_row['equity_multiple_distribution'] = json.dumps(equity_multiple_distribution)
test_row['equity_multiple_mean'] = np.mean(equity_multiple_distribution)
test_row['equity_multiple_std'] = np.std(equity_multiple_distribution)
test_row['equity_multiple_min'] = np.min(equity_multiple_distribution)
test_row['equity_multiple_max'] = np.max(equity_multiple_distribution)

print("\nTest row after assigning calculated values:")
print_df_info(test_row, "test_row")

# Function to run a single analysis with detailed logging
def run_single_analysis(row, zoning_data, params):
    print("\nStarting analyze_redevelopment_potential_single")
    print(f"Input row:\n{row}")
    print(f"Zoning data keys: {zoning_data.keys()}")
    print(f"Params: {params}")

    try:
        result = analyze_redevelopment_potential_single(row, zoning_data, params)
        print("\nAnalysis completed successfully")
        print(f"Result:\n{result}")
        return result
    except Exception as e:
        print(f"\nAn error occurred: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return None

# Run the analysis with current zoning data
print("\nRunning analysis with current zoning data:")
current_result = run_single_analysis(test_row, current_zoning_data, params)

# Run the analysis with proposed zoning data
print("\nRunning analysis with proposed zoning data:")
proposed_result = run_single_analysis(test_row, proposed_zoning_data, params)

# Compare results
if current_result is not None and proposed_result is not None:
    print("\nComparison of results:")
    for col in current_result.columns:
        current_val = current_result[col].iloc[0]
        proposed_val = proposed_result[col].iloc[0]
        diff = proposed_val - current_val
        print(f"{col}:")
        print(f"  Current: {current_val}")
        print(f"  Proposed: {proposed_val}")
        print(f"  Difference: {diff}")
        print(f"  Percentage Change: {(diff/current_val)*100 if current_val != 0 else 'N/A'}%")
        print()
else:
    print("\nUnable to compare results due to errors in analysis")

# Check if the original test_row was modified
print("\nChecking if original test_row was modified:")
print_df_info(test_row, "test_row after analysis")

import matplotlib.pyplot as plt
import seaborn as sns
import json

# Assuming test_row is still available from the previous run
equity_multiple_distribution = json.loads(test_row['equity_multiple_distribution'].iloc[0])

# Create the plot
plt.figure(figsize=(10, 6))
sns.histplot(equity_multiple_distribution, kde=True)
plt.title(f"Equity Multiple Distribution for Cluster {test_row['cluster'].iloc[0]}")
plt.xlabel("Equity Multiple")
plt.ylabel("Frequency")

# Add vertical lines for mean and median
mean_em = np.mean(equity_multiple_distribution)
median_em = np.median(equity_multiple_distribution)
plt.axvline(mean_em, color='r', linestyle='--', label=f'Mean: {mean_em:.2f}')
plt.axvline(median_em, color='g', linestyle='-.', label=f'Median: {median_em:.2f}')

# Add text annotations for other statistics
plt.text(0.05, 0.95, f"Std Dev: {np.std(equity_multiple_distribution):.2f}\n"
                     f"Min: {np.min(equity_multiple_distribution):.2f}\n"
                     f"Max: {np.max(equity_multiple_distribution):.2f}",
         transform=plt.gca().transAxes, verticalalignment='top')

plt.legend()
plt.show()

# Print the number of samples
print(f"Number of samples in equity multiple distribution: {len(equity_multiple_distribution)}")

import pandas as pd
import geopandas as gpd
import numpy as np
import json
from sklearn.utils import resample
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

def calculate_equity_multiple_distribution(cluster_data, zoning_data, num_samples=100):
    equity_multiples = []
    for _ in range(num_samples):
        sample = resample(cluster_data, n_samples=1)
        params = sample_parameters(1)
        result = analyze_redevelopment_potential_single(sample, zoning_data, params)
        if result is not None and 'equity_multiple' in result.columns:
            equity_multiples.append(result['equity_multiple'].iloc[0])
    return equity_multiples

# Calculate equity multiple distributions for all clusters
def calculate_all_cluster_distributions(data, zoning_data, num_samples=100):
    cluster_distributions = {}
    for cluster_id in tqdm(data['cluster'].unique(), desc="Processing clusters"):
        cluster_data = data[data['cluster'] == cluster_id]
        distribution = calculate_equity_multiple_distribution(cluster_data, zoning_data, num_samples)
        cluster_distributions[cluster_id] = distribution
    return cluster_distributions

# Plot distributions for all clusters
def plot_all_distributions(cluster_distributions):
    num_clusters = len(cluster_distributions)
    num_cols = 3
    num_rows = (num_clusters - 1) // num_cols + 1

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 4*num_rows))
    fig.suptitle("Equity Multiple Distributions for All Clusters", fontsize=16)

    for idx, (cluster_id, distribution) in enumerate(cluster_distributions.items()):
        row = idx // num_cols
        col = idx % num_cols
        ax = axes[row, col] if num_rows > 1 else axes[col]

        sns.histplot(distribution, kde=True, ax=ax)
        ax.set_title(f"Cluster {cluster_id}")
        ax.set_xlabel("Equity Multiple")
        ax.set_ylabel("Frequency")

        mean_em = np.mean(distribution)
        median_em = np.median(distribution)
        ax.axvline(mean_em, color='r', linestyle='--', label=f'Mean: {mean_em:.2f}')
        ax.axvline(median_em, color='g', linestyle='-.', label=f'Median: {median_em:.2f}')

        ax.text(0.05, 0.95, f"N: {len(distribution)}\n"
                             f"Std: {np.std(distribution):.2f}\n"
                             f"Min: {np.min(distribution):.2f}\n"
                             f"Max: {np.max(distribution):.2f}",
                transform=ax.transAxes, verticalalignment='top', fontsize=8)

        if idx == 0:  # Only show legend for the first subplot
            ax.legend(fontsize=8)

    # Remove any unused subplots
    for idx in range(len(cluster_distributions), num_rows * num_cols):
        row = idx // num_cols
        col = idx % num_cols
        fig.delaxes(axes[row, col] if num_rows > 1 else axes[col])

    plt.tight_layout()
    plt.show()

# Calculate distributions for all clusters
print("Calculating equity multiple distributions for all clusters...")
all_cluster_distributions = calculate_all_cluster_distributions(merged_gdf_r1_r5, current_zoning_data)

# Plot distributions for all clusters
print("Plotting equity multiple distributions for all clusters...")
plot_all_distributions(all_cluster_distributions)

# Print summary statistics for all clusters
print("\nSummary Statistics for All Clusters:")
for cluster_id, distribution in all_cluster_distributions.items():
    print(f"\nCluster {cluster_id}:")
    print(f"  Number of samples: {len(distribution)}")
    print(f"  Mean: {np.mean(distribution):.4f}")
    print(f"  Median: {np.median(distribution):.4f}")
    print(f"  Std Dev: {np.std(distribution):.4f}")
    print(f"  Min: {np.min(distribution):.4f}")
    print(f"  Max: {np.max(distribution):.4f}")

import pandas as pd
import geopandas as gpd
import numpy as np
import json
from sklearn.utils import resample
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# ... (keep the previous functions: sample_parameters, calculate_equity_multiple_distribution, plot_all_distributions)

def calculate_and_assign_distributions(gdf, zoning_data, num_samples=100):
    cluster_distributions = {}
    for cluster_id in tqdm(gdf['cluster'].unique(), desc="Processing clusters"):
        cluster_data = gdf[gdf['cluster'] == cluster_id]
        distribution = calculate_equity_multiple_distribution(cluster_data, zoning_data, num_samples)
        cluster_distributions[cluster_id] = distribution

        # Assign the distribution and statistics to all rows in this cluster
        gdf.loc[gdf['cluster'] == cluster_id, 'equity_multiple_distribution'] = json.dumps(distribution)
        gdf.loc[gdf['cluster'] == cluster_id, 'equity_multiple_mean'] = np.mean(distribution)
        gdf.loc[gdf['cluster'] == cluster_id, 'equity_multiple_std'] = np.std(distribution)
        gdf.loc[gdf['cluster'] == cluster_id, 'equity_multiple_min'] = np.min(distribution)
        gdf.loc[gdf['cluster'] == cluster_id, 'equity_multiple_max'] = np.max(distribution)

    return gdf, cluster_distributions

# Calculate distributions, assign to GDF, and save
print("Calculating and assigning equity multiple distributions...")
merged_gdf_r1_r5, all_cluster_distributions = calculate_and_assign_distributions(merged_gdf_r1_r5, current_zoning_data)

# Save the updated GDF to Google Drive
save_path = '/content/drive/MyDrive/updated_merged_gdf_r1_r5.gpkg'
print(f"Saving updated GeoDataFrame to {save_path}...")
merged_gdf_r1_r5.to_file(save_path, driver='GPKG')
print("Save completed.")

# Plot distributions for all clusters
print("Plotting equity multiple distributions for all clusters...")
plot_all_distributions(all_cluster_distributions)

# Print summary statistics for all clusters
print("\nSummary Statistics for All Clusters:")
for cluster_id, distribution in all_cluster_distributions.items():
    print(f"\nCluster {cluster_id}:")
    print(f"  Number of samples: {len(distribution)}")
    print(f"  Mean: {np.mean(distribution):.4f}")
    print(f"  Median: {np.median(distribution):.4f}")
    print(f"  Std Dev: {np.std(distribution):.4f}")
    print(f"  Min: {np.min(distribution):.4f}")
    print(f"  Max: {np.max(distribution):.4f}")

# Verify the changes in the GDF
print("\nVerifying changes in the GeoDataFrame:")
sample_cluster = merged_gdf_r1_r5['cluster'].iloc[0]
sample_row = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == sample_cluster].iloc[0]
print(f"Sample row from cluster {sample_cluster}:")
print(f"  equity_multiple_distribution: {sample_row['equity_multiple_distribution'][:100]}...")  # Show first 100 chars
print(f"  equity_multiple_mean: {sample_row['equity_multiple_mean']}")
print(f"  equity_multiple_std: {sample_row['equity_multiple_std']}")
print(f"  equity_multiple_min: {sample_row['equity_multiple_min']}")
print(f"  equity_multiple_max: {sample_row['equity_multiple_max']}")

import geopandas as gpd
import json
from google.colab import drive
from google.colab import files

# Mount Google Drive
drive.mount('/content/drive')

def convert_geopackage_to_csv(input_path, output_path):
    # Read the GeoPackage file
    gdf = gpd.read_file(input_path)

    # Convert geometry to GeoJSON string
    gdf['geometry'] = gdf['geometry'].apply(lambda geom: json.dumps(geom.__geo_interface__))

    # Select the columns we need
    columns_to_keep = ['geometry', 'equity_multiple_mean', 'cluster']
    gdf = gdf[columns_to_keep]

    # Save to CSV
    gdf.to_csv(output_path, index=False)
    print(f"Converted {input_path} to {output_path}")

# Paths (adjust these to match your Google Drive folder structure)
input_geopackage = '/content/drive/MyDrive/updated_merged_gdf_r1_r5.gpkg'
output_csv = '/content/drive/MyDrive/updated_merged_gdf_r1_r5.csv'

# Run the conversion
convert_geopackage_to_csv(input_geopackage, output_csv)

# Download the CSV file
files.download(output_csv)

import geopandas as gpd
import pandas as pd
from shapely.ops import unary_union, polygonize
from shapely.geometry import Polygon, MultiPolygon
from tqdm import tqdm

def create_gapless_gdf(gdf):
    print("Creating gapless geometry GeoDataFrame...")

    # Create the convex hull of the entire dataset
    convex_hull = gdf.unary_union.convex_hull

    # Create a union of all geometries
    all_geoms = unary_union(gdf.geometry)

    # Find the difference between the convex hull and the union of all geometries
    gaps = convex_hull.difference(all_geoms)

    # Create a list to store new geometries
    new_geoms = []

    # Iterate through each geometry in the original GDF
    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc="Processing geometries"):
        original_geom = row.geometry

        # Buffer the original geometry slightly to ensure it intersects with gaps
        buffered_geom = original_geom.buffer(0.01)

        # Find the intersection of the buffered geometry with the gaps
        intersection = buffered_geom.intersection(gaps)

        # Union the original geometry with the intersection
        new_geom = original_geom.union(intersection)

        # If the new geometry is a MultiPolygon, take the largest polygon
        if isinstance(new_geom, MultiPolygon):
            new_geom = max(new_geom, key=lambda x: x.area)

        new_geoms.append(new_geom)

    # Create a new GeoDataFrame with the new geometries
    gapless_gdf = gdf.copy()
    gapless_gdf['geometry'] = new_geoms

    print("Gapless geometry GeoDataFrame created.")
    return gapless_gdf

# Create the gapless GeoDataFrame
gapless_gdf = create_gapless_gdf(merged_gdf_r1_r5)

# Save the gapless GeoDataFrame
save_path = '/content/drive/MyDrive/gapless_merged_gdf_r1_r5.gpkg'
print(f"Saving gapless GeoDataFrame to {save_path}...")
gapless_gdf.to_file(save_path, driver='GPKG')
print("Save completed.")

# Print some information about the new GeoDataFrame
print("\nGapless GeoDataFrame Information:")
print(f"Number of geometries: {len(gapless_gdf)}")
print(f"CRS: {gapless_gdf.crs}")

# Check for any remaining gaps
total_area = gapless_gdf.unary_union.area
convex_hull_area = gapless_gdf.unary_union.convex_hull.area
gap_percentage = (convex_hull_area - total_area) / convex_hull_area * 100

print(f"\nPercentage of area covered: {100 - gap_percentage:.4f}%")
print(f"Remaining gap percentage: {gap_percentage:.4f}%")

# Sample comparison
print("\nSample geometry comparison:")
sample_idx = gapless_gdf.index[0]
original_area = merged_gdf_r1_r5.loc[sample_idx, 'geometry'].area
new_area = gapless_gdf.loc[sample_idx, 'geometry'].area
print(f"Sample index: {sample_idx}")
print(f"Original area: {original_area:.2f}")
print(f"New area: {new_area:.2f}")
print(f"Area increase: {(new_area - original_area) / original_area * 100:.2f}%")

import geopandas as gpd
import pandas as pd
from shapely.ops import unary_union
from shapely.geometry import Polygon, MultiPolygon, box
from multiprocessing import Pool
from tqdm import tqdm

def create_gapless_section(gdf_section):
    """
    Function to create gapless geometries for a section of the GeoDataFrame.
    """
    # Create the convex hull of the section
    convex_hull = gdf_section.unary_union.convex_hull

    # Create a union of all geometries
    all_geoms = unary_union(gdf_section.geometry)

    # Find the difference between the convex hull and the union of all geometries
    gaps = convex_hull.difference(all_geoms)

    # Create a list to store new geometries
    new_geoms = []

    for idx, row in tqdm(gdf_section.iterrows(), total=len(gdf_section), desc="Processing geometries"):
        original_geom = row.geometry

        # Buffer the original geometry slightly to ensure it intersects with gaps
        buffered_geom = original_geom.buffer(0.01)

        # Find the intersection of the buffered geometry with the gaps
        intersection = buffered_geom.intersection(gaps)

        # Union the original geometry with the intersection
        new_geom = original_geom.union(intersection)

        # If the new geometry is a MultiPolygon, merge it with the original one
        if isinstance(new_geom, MultiPolygon):
            new_geom = unary_union(new_geom)

        new_geoms.append(new_geom)

    # Update the geometry column
    gdf_section['geometry'] = new_geoms

    return gdf_section

def divide_gdf_into_sections(gdf, n_sections=8):
    """
    Divides a GeoDataFrame into `n_sections` spatially contiguous sections.
    """
    # Get the bounds of the convex hull
    convex_hull = gdf.unary_union.convex_hull
    minx, miny, maxx, maxy = convex_hull.bounds

    # Determine the size of each section
    x_intervals = pd.cut([minx, maxx], n_sections, retbins=True)[1]

    sections = []
    for i in range(len(x_intervals) - 1):
        # Create a bounding box for the section
        bbox = box(x_intervals[i], miny, x_intervals[i+1], maxy)

        # Subset the GeoDataFrame to include only geometries within the bounding box
        section = gdf[gdf.intersects(bbox)]

        if not section.empty:
            sections.append(section)

    return sections

def process_gapless_gdf_in_parallel(gdf, n_sections=8):
    """
    Process the GeoDataFrame in parallel by dividing it into sections and creating gapless geometries.
    """
    # Divide the GeoDataFrame into spatially contiguous sections
    sections = divide_gdf_into_sections(gdf, n_sections)

    # Process each section in parallel
    with Pool(processes=n_sections) as pool:
        processed_sections = pool.map(create_gapless_section, sections)

    # Combine the processed sections into one GeoDataFrame
    gapless_gdf = gpd.GeoDataFrame(pd.concat(processed_sections, ignore_index=True), crs=gdf.crs)

    return gapless_gdf

# Divide the GeoDataFrame and process in parallel
gapless_gdf = process_gapless_gdf_in_parallel(merged_gdf_r1_r5, n_sections=8)

# Save the gapless GeoDataFrame
save_path = '/content/drive/MyDrive/gapless_merged_gdf_r1_r5.gpkg'
gapless_gdf.to_file(save_path, driver='GPKG')
print("Save completed.")

import geopandas as gpd
import matplotlib.pyplot as plt

def visualize_gapless_gdf(gapless_gdf, save_path='gapless_map.png'):
    """
    Visualizes the gapless GeoDataFrame using geopandas' built-in plotting.
    """
    # Set up the plot
    fig, ax = plt.subplots(figsize=(12, 12))

    # Plot the gapless GeoDataFrame
    gapless_gdf.plot(ax=ax, color='lightblue', edgecolor='black')

    # Set the title and axis labels
    ax.set_title('Gapless GeoDataFrame Visualization', fontsize=16)
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')

    # Save the plot as an image
    plt.savefig(save_path)
    plt.show()
    print(f"Map saved to {save_path}")

# Visualize the gapless GeoDataFrame
visualize_gapless_gdf(gapless_gdf)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import MiniBatchKMeans
import matplotlib.pyplot as plt
import seaborn as sns

# List of relevant features for scaling
features = [
    'LotArea', 'proposed_floor_area',
    'max_height', 'LotFront', 'LotDepth', 'Block', 'max_floors', 'max_floors_current',
    'max_residential_far_current', 'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'min_rear_yard_depth_proposed',
    'min_side_yard_width_proposed', 'max_building_height_proposed'
]

# Extract and normalize the features
X = merged_gdf_r1_r5[features]

# Normalize numerical columns
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X.select_dtypes(include=[np.number]))

# Fit the PCA model
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Fit the MiniBatchKMeans model
best_k = 165  # Assuming 18 is the chosen number of clusters
minibatch_kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=1000)
minibatch_kmeans.fit(X_scaled)

# Create DataFrame for cluster centroids using the scaled feature columns
scaled_columns = X.select_dtypes(include=[np.number]).columns
cluster_centroids = pd.DataFrame(minibatch_kmeans.cluster_centers_, columns=scaled_columns)

print("Cluster Centroids")
print(cluster_centroids)

# Add cluster labels to the original DataFrame
merged_gdf_r1_r5['cluster'] = minibatch_kmeans.labels_

# Summary statistics for each cluster
summary_stats = merged_gdf_r1_r5.groupby('cluster').describe()
print("Summary Statistics for Each Cluster")
print(summary_stats)

# Extract PCA components (loadings)
pca_components = pd.DataFrame(pca.components_, columns=scaled_columns)
print("PCA Components (Loadings)")
print(pca_components)

# Identify the top features contributing to each PCA component
top_features_pc1 = pca_components.iloc[0].abs().sort_values(ascending=False).index[:3]
top_features_pc2 = pca_components.iloc[1].abs().sort_values(ascending=False).index[:3]

# Create labels for the PCA axes
pc1_label = "PCA 1: " + ", ".join(top_features_pc1)
pc2_label = "PCA 2: " + ", ".join(top_features_pc2)

# Visualizing clusters using PCA for dimensionality reduction
plt.figure(figsize=(12, 8))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=merged_gdf_r1_r5['cluster'], palette='viridis', legend='full')
plt.title("PCA of Clusters")
plt.xlabel(pc1_label)
plt.ylabel(pc2_label)
plt.legend(title='Cluster')
plt.show()

# Distribution of cluster values
print(merged_gdf_r1_r5['cluster'].value_counts())

# Verify the first few rows
print(merged_gdf_r1_r5.head())

print("Unique clusters:", merged_gdf_r1_r5['cluster'].unique())
print("Cluster counts:\n", merged_gdf_r1_r5['cluster'].value_counts())

# Check for missing cluster values
missing_clusters = merged_gdf_r1_r5['cluster'].isnull().sum()
print(f"Missing cluster values: {missing_clusters}")

# Check for missing geometry
missing_geometry = merged_gdf_r1_r5['geometry'].isnull().sum()
print(f"Missing geometry values: {missing_geometry}")

# Check for invalid geometry
invalid_geometry = merged_gdf_r1_r5[~merged_gdf_r1_r5.is_valid]
print(f"Invalid geometry values: {len(invalid_geometry)}")

# Verify color assignment
cmap = plt.get_cmap('viridis')  # You can use any colormap available in matplotlib
colors_assigned = merged_gdf_r1_r5['cluster'].apply(lambda x: cmap(x))
print("Colors assigned to clusters:\n", colors_assigned.value_counts())

print(merged_gdf_r1_r5[['geometry', 'cluster']].head(20))

# Ensure cluster values are integers for consistent coloring
merged_gdf_r1_r5['cluster'] = merged_gdf_r1_r5['cluster'].astype(int)

colors_assigned

import matplotlib.pyplot as plt
import geopandas as gpd
import matplotlib.colors as mcolors
import random

# Assuming merged_gdf_r1_r5 is already loaded with cluster assignments
gdf = gpd.GeoDataFrame(merged_gdf_r1_r5)

# Define the clusters based on your previous data
clusters = gdf['cluster'].unique()

# Generate random colors for each cluster
cluster_colors = {cluster: (random.random(), random.random(), random.random(), 1.0) for cluster in clusters}

# Create a new plot with a figure size of 15x15 inches
fig, ax = plt.subplots(figsize=(15, 15))

# Plot each cluster with its corresponding color
for cluster in clusters:
    gdf[gdf['cluster'] == cluster].plot(ax=ax, color=cluster_colors[cluster], edgecolor='none')

# Add legend
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cluster_colors[cluster], markersize=10, label=f'Cluster {cluster}') for cluster in clusters])

# Show plot
plt.show()

import matplotlib.pyplot as plt
import geopandas as gpd
import numpy as np

# Assuming merged_gdf_r1_r5 is already loaded with cluster assignments
gdf = gpd.GeoDataFrame(merged_gdf_r1_r5)

# Get the value counts of each cluster
cluster_counts = gdf['cluster'].value_counts()

# Print the value counts
print("Cluster value counts:\n", cluster_counts)

# Define custom bins
bins = np.linspace(cluster_counts.min(), cluster_counts.max(), 20)

# Plot the histogram of cluster counts with custom bins
fig, ax = plt.subplots(figsize=(10, 6))
cluster_counts.plot(kind='hist', bins=bins, ax=ax, color='skyblue', edgecolor='black')
ax.set_xlabel('Number of Members in Cluster')
ax.set_ylabel('Number of Clusters')
ax.set_title('Distribution of Cluster Sizes')
plt.grid(axis='y')
plt.show()

import matplotlib.pyplot as plt
import geopandas as gpd
import matplotlib.colors as mcolors

# Assuming merged_gdf_r1_r5 is already loaded with cluster assignments
gdf = gpd.GeoDataFrame(merged_gdf_r1_r5)

# Define the clusters and cluster_colors based on your previous data
clusters = [4, 9, 5, 17, 12, 2, 13, 1, 15, 16, 14, 7, 3, 10, 8, 6, 11, 0]
cluster_colors = [
    (0.17254901960784313, 0.6274509803921569, 0.17254901960784313, 1.0),
    (0.5490196078431373, 0.33725490196078434, 0.29411764705882354, 1.0),
    (0.596078431372549, 0.8745098039215686, 0.5411764705882353, 1.0),
    (0.6196078431372549, 0.8549019607843137, 0.8980392156862745, 1.0),
    (0.4980392156862745, 0.4980392156862745, 0.4980392156862745, 1.0),
    (1.0, 0.4980392156862745, 0.054901960784313725, 1.0),
    (0.7803921568627451, 0.7803921568627451, 0.7803921568627451, 1.0),
    (0.6823529411764706, 0.7803921568627451, 0.9098039215686274, 1.0),
    (0.8588235294117647, 0.8588235294117647, 0.5529411764705883, 1.0),
    (0.09019607843137255, 0.7450980392156863, 0.8117647058823529, 1.0),
    (0.7372549019607844, 0.7411764705882353, 0.13333333333333333, 1.0),
    (0.5803921568627451, 0.403921568627451, 0.7411764705882353, 1.0),
    (1.0, 0.7333333333333333, 0.47058823529411764, 1.0),
    (0.7686274509803922, 0.611764705882353, 0.5803921568627451, 1.0),
    (0.7725490196078432, 0.6901960784313725, 0.8352941176470589, 1.0),
    (1.0, 0.596078431372549, 0.5882352941176471, 1.0),
    (0.8901960784313725, 0.4666666666666667, 0.7607843137254902, 1.0),
    (0.12156862745098039, 0.4666666666666667, 0.7058823529411765, 1.0)
]

# Create a new plot with a figure size of 15x15 inches
fig, ax = plt.subplots(figsize=(15, 15))

# Define the color mapping
color_map = dict(zip(clusters, cluster_colors))

# Plot each cluster with its corresponding color
for cluster in clusters:
    gdf[gdf['cluster'] == cluster].plot(ax=ax, color=color_map[cluster], edgecolor='none')

# Add legend
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[cluster], markersize=10, label=f'Cluster {cluster}') for cluster in clusters])

# Show plot
plt.show()

import pandas as pd
import numpy as np
from sklearn.metrics import pairwise_distances_argmin_min

# Assuming merged_gdf_r1_r5 is already loaded with cluster assignments and geometry
gdf = gpd.GeoDataFrame(merged_gdf_r1_r5)

# Extract the cluster centroids from the clustering algorithm
centroids = minibatch_kmeans.cluster_centers_

# Find the representative property for each cluster
representative_properties = []
for cluster in range(len(centroids)):
    cluster_data = gdf[gdf['cluster'] == cluster]
    centroid = centroids[cluster].reshape(1, -1)
    closest, _ = pairwise_distances_argmin_min(centroid, cluster_data[features])
    representative_properties.append(cluster_data.iloc[closest[0]])

# Convert the representative properties to a DataFrame
rep_properties_df = pd.DataFrame(representative_properties)

# Select the relevant features, including zoning
features_with_zoning = [
    'LotArea', 'proposed_floor_area', 'max_height', 'LotFront', 'LotDepth', 'Block', 'max_floors',
    'max_floors_current', 'max_residential_far_current', 'min_front_yard_depth_current',
    'min_rear_yard_depth_current', 'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'min_rear_yard_depth_proposed',
    'min_side_yard_width_proposed', 'max_building_height_proposed'
]

# Extract PCA components or loadings
# Assuming `pca` was fitted on the scaled data
pca_components = pd.DataFrame(pca.components_, columns=features_with_zoning)

# Calculate feature importance as the sum of absolute values of the loadings for each feature
feature_importance = pca_components.abs().sum(axis=0)

# Sort features by their importance
sorted_features = feature_importance.sort_values(ascending=False)

# Create a new DataFrame for the table with sorted features
sorted_feature_names = [f"{feat} ({imp:.2f})" for feat, imp in sorted_features.items()]
table = rep_properties_df[sorted_features.index]

# Remove columns with the same value for all rows
table = table.loc[:, (table != table.iloc[0]).any()]

# Update the sorted feature names based on the filtered columns
sorted_feature_names = [f"{feat} ({sorted_features[feat]:.2f})" for feat in table.columns]

# Display the table with sorted features
table.columns = sorted_feature_names
print(table.to_string(index=False))

merged_gdf_r1_r5.columns

import plotly.express as px
import geopandas as gpd
import pandas as pd

# Assuming merged_gdf_r1_r5 is already loaded with cluster assignments and geometry
gdf = gpd.GeoDataFrame(merged_gdf_r1_r5)

# Create a DataFrame for plotly
df = pd.DataFrame(gdf)

# Define the color mapping
color_map = {
    0: "rgb(31, 120, 180)",
    1: "rgb(51, 160, 44)",
    2: "rgb(227, 26, 28)",
    3: "rgb(255, 127, 0)",
    4: "rgb(106, 61, 154)",
    5: "rgb(177, 89, 40)",
    6: "rgb(255, 255, 51)",
    7: "rgb(166, 206, 227)",
    8: "rgb(178, 223, 138)",
    9: "rgb(251, 154, 153)",
    10: "rgb(253, 191, 111)",
    11: "rgb(202, 178, 214)",
    12: "rgb(255, 255, 153)",
    13: "rgb(31, 120, 180)",
    14: "rgb(51, 160, 44)",
    15: "rgb(227, 26, 28)",
    16: "rgb(255, 127, 0)",
    17: "rgb(106, 61, 154)",
}

# Map the color to the clusters
df["color"] = df["cluster"].map(color_map)

# Plot using Plotly Express
fig = px.choropleth_mapbox(df,
                           geojson=gdf.geometry,
                           locations=df.index,
                           color="color",
                           color_discrete_map=color_map,
                           mapbox_style="carto-positron",
                           center={"lat": gdf.geometry.centroid.y.mean(), "lon": gdf.geometry.centroid.x.mean()},
                           zoom=10,
                           opacity=0.5,
                           labels={"cluster": "Cluster"},
                           title="Clustered Properties")

fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

import numpy as np
import pandas as pd
from scipy import stats

def calculate_weighted_cv(cluster, feature_importances):
    """
    Calculate the weighted coefficient of variation (CV) for a cluster based on multiple features.
    """
    weighted_variances = []
    for feature, importance in feature_importances.items():
        if feature in cluster.columns:
            mean = np.mean(cluster[feature])
            std = np.std(cluster[feature])
            if mean != 0 and not np.isnan(mean) and not np.isnan(std):  # Avoid division by zero and NaN values
                cv = std / mean
                weighted_variances.append((cv * importance)**2)
            else:
                print(f"Skipping feature '{feature}' due to zero or NaN mean in cluster.")

    if not weighted_variances:
        raise ValueError("No valid features found in the cluster data")

    weighted_cv = np.sqrt(np.sum(weighted_variances))
    return weighted_cv

def determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column='cluster'):
    """
    Determine the weighted CV for each cluster.
    """
    cluster_weighted_cvs = {}
    for cluster_id in clustered_data[cluster_column].unique():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        try:
            weighted_cv = calculate_weighted_cv(cluster_data, feature_importances)
            cluster_weighted_cvs[cluster_id] = weighted_cv
        except ValueError as e:
            print(f"Cluster {cluster_id}: {e}")
    return cluster_weighted_cvs

def cluster_statistics(cluster_data, feature_importances):
    """
    Print statistics for the given cluster data.
    """
    print(f"Cluster size: {len(cluster_data)}")
    for feature, importance in feature_importances.items():
        if feature in cluster_data.columns:
            mean = np.mean(cluster_data[feature])
            std = np.std(cluster_data[feature])
            print(f"Feature: {feature}, Mean: {mean:.4f}, Std: {std:.4f}")

def calculate_sample_size(cluster, feature_importances, confidence_level=0.95, margin_of_error=0.05):
    """
    Calculate the required sample size for a cluster based on multiple features.
    """
    z_score = stats.norm.ppf((1 + confidence_level) / 2)
    weighted_variances = []
    for feature, importance in feature_importances.items():
        if feature in cluster.columns:
            mean = np.mean(cluster[feature])
            std = np.std(cluster[feature])
            if mean != 0 and not np.isnan(mean) and not np.isnan(std):  # Avoid division by zero and NaN values
                cv = std / mean
                weighted_variances.append((cv * importance)**2)

    if not weighted_variances:
        raise ValueError("No valid features found in the cluster data")

    weighted_cv = np.sqrt(np.sum(weighted_variances))
    sample_size = ((z_score * weighted_cv) / margin_of_error)**2

    return int(np.ceil(sample_size))

def determine_cluster_sample_sizes(clustered_data, feature_importances, cluster_column='cluster', **kwargs):
    """
    Determine the sample size for each cluster.
    """
    cluster_sample_sizes = {}
    for cluster_id in clustered_data[cluster_column].unique():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        sample_size = calculate_sample_size(cluster_data, feature_importances, **kwargs)
        cluster_sample_sizes[cluster_id] = sample_size

    return cluster_sample_sizes

# Example usage
# Assuming 'merged_gdf_r1_r5' is your DataFrame with cluster assignments

# Define feature importances based on the provided information
feature_importances = {
    'max_residential_far_proposed': 0.69,
    'max_residential_far_current': 0.69,
    'max_building_height_proposed': 0.59,
    'max_floors': 0.59,
    'max_building_height_current': 0.59,
    'min_front_yard_depth_proposed': 0.59,
    'min_front_yard_depth_current': 0.59,
    'max_floors_current': 0.58,
    'LotFront': 0.23,
    'LotArea': 0.22,
    'proposed_floor_area': 0.20,
    'Block': 0.16,
    'LotDepth': 0.10,
    'max_height': 0.06
}

# Calculate weighted CVs for clusters
cluster_weighted_cvs = determine_cluster_weighted_cvs(merged_gdf_r1_r5, feature_importances, cluster_column='cluster')

# Filter out clusters with NaN weighted CVs
cluster_weighted_cvs = {k: v for k, v in cluster_weighted_cvs.items() if not np.isnan(v)}

# Sort clusters by weighted CV in descending order and get the worst 3
worst_clusters = sorted(cluster_weighted_cvs.items(), key=lambda item: item[1], reverse=True)[:3]

print("Worst 3 clusters by weighted CV:")
for cluster_id, weighted_cv in worst_clusters:
    print(f"Cluster {cluster_id}: Weighted CV = {weighted_cv:.4f}")

# Get sample sizes for all clusters
cluster_sample_sizes = determine_cluster_sample_sizes(
    merged_gdf_r1_r5,
    feature_importances,
    cluster_column='cluster',
    confidence_level=0.95,
    margin_of_error=0.05
)

print("\nSample sizes for each cluster:")
for cluster_id, sample_size in cluster_sample_sizes.items():
    print(f"Cluster {cluster_id}: {sample_size} samples")

# Calculate total samples
total_samples = sum(cluster_sample_sizes.values())
print(f"\nTotal samples across all clusters: {total_samples}")

# Calculate the proportion of each cluster that needs to be sampled
cluster_proportions = merged_gdf_r1_r5['cluster'].value_counts(normalize=True)
print("\nProportion of each cluster that needs to be sampled:")
for cluster_id, sample_size in cluster_sample_sizes.items():
    cluster_size = (merged_gdf_r1_r5['cluster'] == cluster_id).sum()
    proportion = sample_size / cluster_size
    print(f"Cluster {cluster_id}: {proportion:.2%}")

# Identify clusters that require sampling more than 50% of their data
oversampled_clusters = {cluster_id: proportion for cluster_id, proportion in cluster_proportions.items() if cluster_sample_sizes[cluster_id] > 0.5 * (merged_gdf_r1_r5['cluster'] == cluster_id).sum()}

if oversampled_clusters:
    print("\nClusters requiring more than 50% sampling:")
    for cluster_id, proportion in oversampled_clusters.items():
        print(f"Cluster {cluster_id}: {proportion:.2%}")
    print("\nConsider adjusting the margin of error or confidence level for these clusters.")

import numpy as np
import pandas as pd
from scipy import stats

def calculate_weighted_cv(cluster, feature_importances):
    """
    Calculate the weighted coefficient of variation (CV) for a cluster based on multiple features.
    """
    weighted_variances = []
    for feature, importance in feature_importances.items():
        if feature in cluster.columns:
            mean = np.mean(cluster[feature])
            std = np.std(cluster[feature])
            if mean != 0 and not np.isnan(mean) and not np.isnan(std):  # Avoid division by zero and NaN values
                cv = std / mean
                weighted_variances.append((cv * importance)**2)
            else:
                print(f"Skipping feature '{feature}' due to zero or NaN mean in cluster.")

    if not weighted_variances:
        raise ValueError("No valid features found in the cluster data")

    weighted_cv = np.sqrt(np.sum(weighted_variances))
    return weighted_cv

def determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column='cluster'):
    """
    Determine the weighted CV for each cluster.
    """
    cluster_weighted_cvs = {}
    for cluster_id in clustered_data[cluster_column].unique():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        try:
            weighted_cv = calculate_weighted_cv(cluster_data, feature_importances)
            cluster_weighted_cvs[cluster_id] = weighted_cv
        except ValueError as e:
            print(f"Cluster {cluster_id}: {e}")
    return cluster_weighted_cvs

def cluster_statistics(cluster_data, feature_importances):
    """
    Print statistics for the given cluster data.
    """
    print(f"Cluster size: {len(cluster_data)}")
    for feature, importance in feature_importances.items():
        if feature in cluster_data.columns:
            mean = np.mean(cluster_data[feature])
            std = np.std(cluster_data[feature])
            print(f"Feature: {feature}, Mean: {mean:.4f}, Std: {std:.4f}")

def calculate_sample_size(cluster, feature_importances, confidence_level=0.95, margin_of_error=0.05, max_sample_size=None):
    """
    Calculate the required sample size for a cluster based on multiple features.
    """
    z_score = stats.norm.ppf((1 + confidence_level) / 2)
    weighted_variances = []
    for feature, importance in feature_importances.items():
        if feature in cluster.columns:
            mean = np.mean(cluster[feature])
            std = np.std(cluster[feature])
            if mean != 0 and not np.isnan(mean) and not np.isnan(std):  # Avoid division by zero and NaN values
                cv = std / mean
                weighted_variances.append((cv * importance)**2)

    if not weighted_variances:
        raise ValueError("No valid features found in the cluster data")

    weighted_cv = np.sqrt(np.sum(weighted_variances))
    sample_size = ((z_score * weighted_cv) / margin_of_error)**2

    # Ensure the sample size is capped at the cluster size or max_sample_size, whichever is smaller
    if max_sample_size is not None:
        return min(int(np.ceil(sample_size)), len(cluster), max_sample_size)
    else:
        return min(int(np.ceil(sample_size)), len(cluster))

def determine_cluster_sample_sizes(clustered_data, feature_importances, cluster_column='cluster', **kwargs):
    """
    Determine the sample size for each cluster.
    """
    cluster_sample_sizes = {}
    for cluster_id in clustered_data[cluster_column].unique():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        sample_size = calculate_sample_size(cluster_data, feature_importances, **kwargs)
        cluster_sample_sizes[cluster_id] = sample_size

    return cluster_sample_sizes

# Example usage
# Assuming 'merged_gdf_r1_r5' is your DataFrame with cluster assignments

# Define feature importances based on the provided information
feature_importances = {
    'max_residential_far_proposed': 0.69,
    'max_residential_far_current': 0.69,
    'max_building_height_proposed': 0.59,
    'max_floors': 0.59,
    'max_building_height_current': 0.59,
    'min_front_yard_depth_proposed': 0.59,
    'min_front_yard_depth_current': 0.59,
    'max_floors_current': 0.58,
    'LotFront': 0.23,
    'LotArea': 0.22,
    'proposed_floor_area': 0.20,
    'Block': 0.16,
    'LotDepth': 0.10,
    'max_height': 0.06
}

# Calculate weighted CVs for clusters
cluster_weighted_cvs = determine_cluster_weighted_cvs(merged_gdf_r1_r5, feature_importances, cluster_column='cluster')

# Filter out clusters with NaN weighted CVs
cluster_weighted_cvs = {k: v for k, v in cluster_weighted_cvs.items() if not np.isnan(v)}

# Sort clusters by weighted CV in descending order and get the worst 3
worst_clusters = sorted(cluster_weighted_cvs.items(), key=lambda item: item[1], reverse=True)[:3]

print("Worst 3 clusters by weighted CV:")
for cluster_id, weighted_cv in worst_clusters:
    print(f"Cluster {cluster_id}: Weighted CV = {weighted_cv:.4f}")

# Get sample sizes for all clusters
cluster_sample_sizes = determine_cluster_sample_sizes(
    merged_gdf_r1_r5,
    feature_importances,
    cluster_column='cluster',
    confidence_level=0.95,
    margin_of_error=0.05
)

print("\nDetails of the worst 3 clusters:")
for cluster_id, weighted_cv in worst_clusters:
    print(f"\nCluster {cluster_id} (Weighted CV = {weighted_cv:.4f}):")
    cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster_id]
    cluster_statistics(cluster_data, feature_importances)
    recommended_sample_size = cluster_sample_sizes[cluster_id]
    print(f"Recommended sample size: {recommended_sample_size}")

import numpy as np
import pandas as pd
import statsmodels.stats.power as smp
from sklearn.utils import resample
from joblib import Parallel, delayed
import multiprocessing
import numpy_financial as npf
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import pairwise_distances_argmin_min

# Assuming merged_gdf_r1_r5 and current_zoning_data, proposed_zoning_data are already loaded

# Function to calculate weighted CV
def calculate_weighted_cv(cluster, feature_importances):
    weighted_variances = []
    for feature, importance in feature_importances.items():
        if feature in cluster.columns:
            mean = np.mean(cluster[feature])
            std = np.std(cluster[feature])
            if mean != 0 and not np.isnan(mean) and not np.isnan(std):
                cv = std / mean
                weighted_variances.append((cv * importance)**2)
    if not weighted_variances:
        raise ValueError("No valid features found in the cluster data")
    weighted_cv = np.sqrt(np.sum(weighted_variances))
    return weighted_cv

# Function to determine weighted CVs for clusters
def determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column='cluster'):
    cluster_weighted_cvs = {}
    for cluster_id in clustered_data[cluster_column].unique():
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        try:
            weighted_cv = calculate_weighted_cv(cluster_data, feature_importances)
            cluster_weighted_cvs[cluster_id] = weighted_cv
        except ValueError as e:
            print(f"Cluster {cluster_id}: {e}")
    return cluster_weighted_cvs

# Function to calculate sample size using power analysis
def calculate_sample_size_power(effect_size, alpha=0.05, power=0.80, alternative='two-sided'):
    sample_size = smp.tt_ind_solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative=alternative)
    return int(np.ceil(sample_size))

# Monte Carlo simulation function with early stopping and progress monitoring
def monte_carlo_simulation(cluster_data, sample_size, num_simulations=100, tol=0.01, min_simulations=30, current_zoning_data=None, proposed_zoning_data=None):
    all_means = []
    available_sample_size = min(sample_size, len(cluster_data))
    prev_mean = 0
    with tqdm(total=num_simulations, desc="Monte Carlo simulation for cluster") as pbar:
        for i in range(num_simulations):
            sampled_params = sample_parameters(available_sample_size)
            simulations = [
                analyze_redevelopment_potential_single(cluster_data.iloc[i], current_zoning_data, {k: v[i] for k, v in sampled_params.items()})
                for i in range(available_sample_size)
            ]
            mean_market_value = np.mean([sim['redeveloped_value'].iloc[0] for sim in simulations if not sim.empty])
            all_means.append(mean_market_value)

            current_mean = np.mean(all_means)
            current_std = np.std(all_means)

            if i >= min_simulations and abs(current_mean - prev_mean) / current_mean < tol:
                pbar.update(num_simulations - i)
                print(f"Early stopping at simulation {i + 1}")
                break

            prev_mean = current_mean
            pbar.update(1)

    estimated_mean = np.mean(all_means)
    estimated_std = np.std(all_means)
    return estimated_mean, estimated_std

# Bootstrap sampling function
def bootstrap_sample(cluster_data, num_samples=100):
    bootstrap_means = []
    for _ in tqdm(range(num_samples), desc="Bootstrap sampling"):
        sampled_params = sample_parameters(len(cluster_data))
        market_values = sampled_params['market_value_per_sqft']
        sample = resample(market_values)
        bootstrap_means.append(np.mean(sample))
    return np.mean(bootstrap_means), np.std(bootstrap_means)

# Determine appropriate sample sizes for each cluster
def determine_cluster_sample_sizes(clustered_data, feature_importances, cluster_column='cluster', alpha=0.05, power=0.80, margin_of_error=0.05, max_sample_size=None, current_zoning_data=None, proposed_zoning_data=None):
    cluster_sample_sizes = {}
    cluster_weighted_cvs = determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column)

    for cluster_id, weighted_cv in tqdm(cluster_weighted_cvs.items(), desc="Calculating sample sizes for clusters"):
        effect_size = weighted_cv
        initial_sample_size = calculate_sample_size_power(effect_size, alpha=alpha, power=power)

        # Validate with Monte Carlo simulations
        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        estimated_mean, estimated_std = monte_carlo_simulation(cluster_data, initial_sample_size, current_zoning_data=current_zoning_data, proposed_zoning_data=proposed_zoning_data)

        # Validate with Bootstrap sampling
        bootstrap_mean, bootstrap_std = bootstrap_sample(cluster_data)

        final_sample_size = max(initial_sample_size, int((estimated_std / margin_of_error) ** 2), int((bootstrap_std / margin_of_error) ** 2))

        if max_sample_size:
            final_sample_size = min(final_sample_size, max_sample_size)

        cluster_sample_sizes[cluster_id] = min(final_sample_size, len(cluster_data))

    return cluster_sample_sizes

# Identify representative properties for each cluster
def identify_representative_properties(gdf, centroids, features):
    representative_properties = []
    for cluster in range(len(centroids)):
        cluster_data = gdf[gdf['cluster'] == cluster]
        centroid = centroids[cluster].reshape(1, -1)
        closest, _ = pairwise_distances_argmin_min(centroid, cluster_data[features])
        representative_properties.append(cluster_data.iloc[closest[0]])
    rep_properties_df = pd.DataFrame(representative_properties)
    return rep_properties_df

# Apply sample sizes to compute desired parameters for each cluster
def analyze_clusters(rep_properties_df, sample_sizes, current_zoning_data, proposed_zoning_data):
    results = []
    for cluster_id, sample_size in sample_sizes.items():
        row = rep_properties_df[rep_properties_df['cluster'] == cluster_id].iloc[0]
        sampled_params = sample_parameters(sample_size)

        for i in range(sample_size):
            params = {k: v[i] for k, v in sampled_params.items()}
            current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
            proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
            results.append((current_sim, proposed_sim))

    return results

# Example usage
feature_importances = {
    'max_residential_far_proposed': 0.69,
    'max_residential_far_current': 0.69,
    'max_building_height_proposed': 0.59,
    'max_floors': 0.59,
    'max_building_height_current': 0.59,
    'min_front_yard_depth_proposed': 0.59,
    'min_front_yard_depth_current': 0.59,
    'max_floors_current': 0.58,
    'LotFront': 0.23,
    'LotArea': 0.22,
    'proposed_floor_area': 0.20,
    'Block': 0.16,
    'LotDepth': 0.10,
    'max_height': 0.06
}

# Extract representative properties
centroids = minibatch_kmeans.cluster_centers_
rep_properties_df = identify_representative_properties(merged_gdf_r1_r5, centroids, features_with_zoning)

# Calculate sample sizes for each cluster
sample_sizes = determine_cluster_sample_sizes(
    merged_gdf_r1_r5,
    feature_importances,
    cluster_column='cluster',
    alpha=0.05,
    power=0.80,
    margin_of_error=0.05,
    max_sample_size=1000,
    current_zoning_data=current_zoning_data,
    proposed_zoning_data=proposed_zoning_data
)

# Print sample sizes for each cluster
print("\nSample sizes for each cluster:")
for cluster_id, sample_size in sample_sizes.items():
    print(f"Cluster {cluster_id}: {sample_size} samples")

# Perform analysis on the sampled data
results = analyze_clusters(rep_properties_df, sample_sizes, current_zoning_data, proposed_zoning_data)

# Combine the results
current_simulations = pd.concat([result[0] for result in results], ignore_index=True)
proposed_simulations = pd.concat([result[1] for result in results], ignore_index=True)

print("Current Standards Simulation Results:")
print(current_simulations.head())

print("Proposed Standards Simulation Results:")
print(proposed_simulations.head())

# Visualization function
def visualize_results(current_simulations, proposed_simulations):
    fig, axs = plt.subplots(3, 2, figsize=(12, 18))

    sns.histplot(current_simulations['irr'], bins=50, kde=True, ax=axs[0, 0])
    axs[0, 0].set_title('Current IRR Distribution')
    sns.histplot(proposed_simulations['irr'], bins=50, kde=True, ax=axs[0, 1])
    axs[0, 1].set_title('Proposed IRR Distribution')

    sns.histplot(current_simulations['equity_multiple'], bins=50, kde=True, ax=axs[1, 0])
    axs[1, 0].set_title('Current Equity Multiple Distribution')
    sns.histplot(proposed_simulations['equity_multiple'], bins=50, kde=True, ax=axs[1, 1])
    axs[1, 1].set_title('Proposed Equity Multiple Distribution')

    sns.histplot(current_simulations['npv'], bins=50, kde=True, ax=axs[2, 0])
    axs[2, 0].set_title('Current NPV Distribution')
    sns.histplot(proposed_simulations['npv'], bins=50, kde=True, ax=axs[2, 1])
    axs[2, 1].set_title('Proposed NPV Distribution')

    plt.tight_layout()
    plt.show()

visualize_results(current_simulations, proposed_simulations)

merged_gdf_r1_r5.columns

import matplotlib.pyplot as plt
import seaborn as sns

# Define relevant clusters
relevant_clusters = [10, 15, 14, 6, 11, 0]

# Define key features based on feature importances
key_features = [
    'max_residential_far_proposed', 'max_residential_far_current', 'max_building_height_proposed',
    'max_floors', 'max_building_height_current', 'min_front_yard_depth_proposed',
    'min_front_yard_depth_current', 'max_floors_current', 'LotFront', 'LotArea',
    'proposed_floor_area', 'Block', 'LotDepth', 'max_height'
]

# Plot histograms and box plots for each key feature in relevant clusters
for cluster_id in relevant_clusters:
    cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster_id]

    for feature in key_features:
        if feature in cluster_data.columns:
            plt.figure(figsize=(12, 5))

            plt.subplot(1, 2, 1)
            sns.histplot(cluster_data[feature], kde=True)
            plt.title(f'Cluster {cluster_id} - {feature} Histogram')

            plt.subplot(1, 2, 2)
            sns.boxplot(x=cluster_data[feature])
            plt.title(f'Cluster {cluster_id} - {feature} Box Plot')

            plt.show()

def label_outliers(cluster_data, features):
    """
    Label outliers in the cluster data for the given features.

    :param cluster_data: DataFrame containing the cluster data
    :param features: List of features to check for outliers
    :return: DataFrame with an additional column 'is_outlier' indicating outliers
    """
    cluster_data = cluster_data.copy()
    cluster_data['is_outlier'] = False

    for feature in features:
        if feature in cluster_data.columns:
            Q1 = cluster_data[feature].quantile(0.25)
            Q3 = cluster_data[feature].quantile(0.75)
            IQR = Q3 - Q1
            outlier_condition = (cluster_data[feature] < (Q1 - 1.5 * IQR)) | (cluster_data[feature] > (Q3 + 1.5 * IQR))
            cluster_data['is_outlier'] = cluster_data['is_outlier'] | outlier_condition

    return cluster_data

# Apply outlier labeling for relevant clusters
labeled_clusters = {}

for cluster_id in relevant_clusters:
    cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster_id]
    labeled_cluster_data = label_outliers(cluster_data, key_features)
    labeled_clusters[cluster_id] = labeled_cluster_data

from sklearn.model_selection import train_test_split

def stratified_sample_with_outliers(cluster_data, key_features, sample_size):
    """
    Perform stratified sampling on a cluster, treating outliers as a separate stratum.

    :param cluster_data: DataFrame containing the cluster data
    :param key_features: List of key features for stratification
    :param sample_size: Desired sample size
    :return: DataFrame containing the stratified sample
    """
    # Ensure each stratum has at least 2 samples
    def ensure_min_samples_per_class(y):
        _, class_counts = np.unique(y, return_counts=True)
        return np.min(class_counts) >= 2

    # Calculate the fraction for the test size
    test_size_fraction = min(sample_size / len(cluster_data), 0.99)

    # Stratify by key features and outlier label
    stratify_cols = cluster_data[key_features + ['is_outlier']]

    # Check if stratification is possible
    if ensure_min_samples_per_class(stratify_cols):
        stratified_sample, _ = train_test_split(cluster_data, test_size=test_size_fraction, stratify=stratify_cols)
    else:
        stratified_sample = cluster_data.sample(n=sample_size)

    return stratified_sample

# Perform stratified sampling for each relevant cluster
stratified_samples = {}

for cluster_id in relevant_clusters:
    cluster_data = labeled_clusters[cluster_id]
    sample_size = cluster_sample_sizes[cluster_id]

    # Perform stratified sampling
    stratified_sample = stratified_sample_with_outliers(cluster_data, key_features, sample_size)
    stratified_samples[cluster_id] = stratified_sample

# Combine all stratified samples into one DataFrame
all_stratified_samples = pd.concat(stratified_samples.values())

print("Stratified samples for relevant clusters, with outliers handled, have been created.")

import pandas as pd
import numpy as np
from scipy import stats

# Identify clusters that require sampling more than 50% of their data
oversampled_clusters = {}
for cluster_id, sample_size in cluster_sample_sizes.items():
    cluster_size = (merged_gdf_r1_r5['cluster'] == cluster_id).sum()
    proportion = sample_size / cluster_size
    if proportion > 0.5:
        oversampled_clusters[cluster_id] = sample_size

print("Oversampled clusters:", oversampled_clusters)
def label_outliers(cluster_data, features):
    cluster_data = cluster_data.copy()
    cluster_data['is_outlier'] = False

    for feature in features:
        if feature in cluster_data.columns:
            Q1 = cluster_data[feature].quantile(0.25)
            Q3 = cluster_data[feature].quantile(0.75)
            IQR = Q3 - Q1
            outlier_condition = (cluster_data[feature] < (Q1 - 1.5 * IQR)) | (cluster_data[feature] > (Q3 + 1.5 * IQR))
            cluster_data['is_outlier'] = cluster_data['is_outlier'] | outlier_condition

    return cluster_data

# Apply outlier labeling for oversampled clusters
labeled_clusters = {}

for cluster_id in oversampled_clusters.keys():
    cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster_id]
    labeled_cluster_data = label_outliers(cluster_data, list(feature_importances.keys()))
    labeled_clusters[cluster_id] = labeled_cluster_data
def calculate_strata_sample_sizes(cluster_data, key_features, total_sample_size):
    strata_counts = cluster_data.groupby(key_features + ['is_outlier']).size()
    strata_sample_sizes = (strata_counts / strata_counts.sum()) * total_sample_size
    strata_sample_sizes = np.ceil(strata_sample_sizes).astype(int)  # Use ceiling function

    return strata_sample_sizes

# Calculate sample sizes for each stratum in oversampled clusters
strata_sample_sizes = {}

for cluster_id in labeled_clusters.keys():
    cluster_data = labeled_clusters[cluster_id]
    total_sample_size = oversampled_clusters[cluster_id]
    strata_sizes = calculate_strata_sample_sizes(cluster_data, list(feature_importances.keys()), total_sample_size)
    strata_sample_sizes[cluster_id] = strata_sizes
# Combine strata sample sizes into a single DataFrame for easier visualization
strata_sample_sizes_df = []

for cluster_id, sizes in strata_sample_sizes.items():
    strata_df = sizes.reset_index()
    strata_df['strata_id'] = ['strata_' + str(i) for i in range(len(strata_df))]
    strata_df['cluster'] = cluster_id
    strata_df['total_samples'] = sizes.sum()
    strata_sample_sizes_df.append(strata_df)

strata_sample_sizes_df = pd.concat(strata_sample_sizes_df, ignore_index=True)
strata_sample_sizes_df.columns = list(feature_importances.keys()) + ['is_outlier', 'sample_size', 'strata_id', 'cluster', 'total_samples']

# Calculate proportions
strata_sample_sizes_df['proportion'] = strata_sample_sizes_df['sample_size'] / strata_sample_sizes_df['total_samples']

# Summarize the number of strata and sample sizes for each oversampled cluster
summary_stats = strata_sample_sizes_df.groupby('cluster').agg(
    num_strata=('sample_size', 'size'),
    mean_sample_size=('sample_size', 'mean'),
    median_sample_size=('sample_size', 'median'),
    min_sample_size=('sample_size', 'min'),
    max_sample_size=('sample_size', 'max'),
    total_samples=('total_samples', 'first')
).reset_index()

# Calculate the total for all oversampled clusters
total_summary = summary_stats[['num_strata', 'mean_sample_size', 'median_sample_size', 'min_sample_size', 'max_sample_size']].sum().to_frame().T
total_summary['total_samples'] = summary_stats['total_samples'].sum()
total_summary['cluster'] = 'Total'

# Append the total row to the summary
summary_stats = pd.concat([summary_stats, total_summary], ignore_index=True)

# Print the summary of strata for oversampled clusters
print("\nSummary of strata for oversampled clusters:")
print(summary_stats)

# Print the total number of samples
total_samples = summary_stats[summary_stats['cluster'] == 'Total']['total_samples'].values[0]
print(f"\nTotal samples across all oversampled clusters: {total_samples}")

# Identify oversampled clusters
print("\nClusters requiring more than 50% sampling:")
for cluster_id, proportion in oversampled_clusters.items():
    cluster_size = (merged_gdf_r1_r5['cluster'] == cluster_id).sum()
    proportion = proportion / cluster_size
    print(f"Cluster {cluster_id}: {proportion:.2%}")

print("\nConsider adjusting the margin of error or confidence level for these clusters.")

import pandas as pd
import numpy as np
import numpy_financial as npf

def calculate_max_floors(max_height, floor_height):
    return max_height // floor_height, floor_height

def safe_npv(discount_rate, cashflows):
    try:
        return npf.npv(discount_rate, cashflows)
    except Exception as e:
        print(f"Error calculating NPV: {e}")
        return np.nan

def safe_irr(cashflows):
    try:
        return npf.irr(cashflows)
    except Exception as e:
        print(f"Error calculating IRR: {e}")
        return np.nan

def analyze_redevelopment_potential_single(gdf, zoning_data, params):
    try:
        # Ensure zoning_data is a DataFrame
        if isinstance(zoning_data, dict):
            zoning_data = pd.DataFrame(zoning_data)

        zone = gdf['ZoneDist1']

        if zone not in zoning_data.index:
            print(f"Zone {zone} not found in zoning data.")
            return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

        proposed_far = float(zoning_data.loc[zone, 'Residential FAR (max)'])
        front_yard = float(zoning_data.loc[zone, 'Front Yard Depth (min)']) if zoning_data.loc[zone, 'Front Yard Depth (min)'] is not None else 0
        rear_yard = float(zoning_data.loc[zone, 'Rear Yard Depth (min)']) if zoning_data.loc[zone, 'Rear Yard Depth (min)'] is not None else 0
        side_yards = float(zoning_data.loc[zone, 'Side Yards (total width)']) if zoning_data.loc[zone, 'Side Yards (total width)'] is not None else 0 / 2
        max_height = float(zoning_data.loc[zone, 'Building Height (max)']) if zoning_data.loc[zone, 'Building Height (max)'] is not None else np.inf

        buildable_width = max(0, gdf['LotFront'] - 2 * side_yards)
        buildable_depth = max(0, gdf['LotDepth'] - front_yard - rear_yard)
        buildable_footprint = buildable_width * buildable_depth

        # print(f"Buildable width: {buildable_width}, Buildable depth: {buildable_depth}, Buildable footprint: {buildable_footprint}")

        max_floors, effective_floor_height = calculate_max_floors(max_height, float(params['floor_height']))
        proposed_floor_area = min(gdf['LotArea'] * proposed_far, buildable_footprint * max_floors)

        # print(f"Max floors: {max_floors}, Proposed floor area: {proposed_floor_area}")

        land_value = gdf['LotArea'] * float(params['land_value_per_sqft'])
        demolition_cost = gdf['BldgArea'] * float(params['demolition_cost_per_sqft'])
        construction_cost = proposed_floor_area * float(params['construction_cost_per_sqft'])
        total_development_cost = land_value + demolition_cost + construction_cost

        # print(f"Land value: {land_value}, Demolition cost: {demolition_cost}, Construction cost: {construction_cost}, Total development cost: {total_development_cost}")

        current_value = gdf['BldgArea'] * float(params['market_value_per_sqft'])
        redeveloped_value = proposed_floor_area * float(params['market_value_per_sqft'])

        # print(f"Current value: {current_value}, Redeveloped value: {redeveloped_value}")

        equity_investment = total_development_cost * (1 - float(params['debt_ratio']))
        debt = total_development_cost * float(params['debt_ratio'])

        # print(f"Equity investment: {equity_investment}, Debt: {debt}")

        annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
        annual_debt_service = debt * (float(params['interest_rate']) * (1 + float(params['interest_rate']))**float(params['holding_period_years'])) / ((1 + float(params['interest_rate']))**float(params['holding_period_years']) - 1)

        sale_price = redeveloped_value * (1 + float(params['appreciation_rate']))**float(params['holding_period_years'])

        # print(f"Annual NOI: {annual_noi}, Annual debt service: {annual_debt_service}, Sale price: {sale_price}")

        cashflows = np.array([
            -equity_investment,
            *([annual_noi - annual_debt_service] * (int(params['holding_period_years']) - 1)),
            annual_noi - annual_debt_service + sale_price - debt
        ])

        # print(f"Cashflows: {cashflows}")

        if np.any(np.isinf(cashflows)) or np.any(np.isnan(cashflows)):
            print("Cashflows contain infs or NaNs")
            return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

        npv_value = safe_npv(float(params['discount_rate']), cashflows)
        irr_value = safe_irr(cashflows)
        equity_multiple = np.sum(cashflows) / equity_investment

        return pd.DataFrame({
            'current_value': current_value,
            'redevelopment_cost': total_development_cost,
            'redeveloped_value': redeveloped_value,
            'npv': npv_value,
            'irr': irr_value,
            'equity_multiple': equity_multiple,
            'proposed_floor_area': proposed_floor_area,
            'max_floors': max_floors
        }, index=[gdf.name])
    except Exception as e:
        print(f"Error in analyze_redevelopment_potential_single: {e}")
        return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

import pandas as pd
import numpy as np
from joblib import Parallel, delayed
import multiprocessing
import numpy_financial as npf
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.08, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(0.03, 0.05, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.80, num_samples),
        'interest_rate': np.random.uniform(0.05, 0.06, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples)
    }

def sample_strata(cluster_data, strata_sizes):
    sampled_data = pd.DataFrame()

    for index, size in strata_sizes.items():
        if size > 0:
            try:
                filter_conditions = (cluster_data['max_residential_far_proposed'] == index[0]) & \
                                    (cluster_data['max_residential_far_current'] == index[1]) & \
                                    (cluster_data['max_building_height_proposed'] == index[2]) & \
                                    (cluster_data['max_floors'] == index[3]) & \
                                    (cluster_data['max_building_height_current'] == index[4]) & \
                                    (cluster_data['min_front_yard_depth_proposed'] == index[5]) & \
                                    (cluster_data['min_front_yard_depth_current'] == index[6]) & \
                                    (cluster_data['max_floors_current'] == index[7]) & \
                                    (cluster_data['LotFront'] == index[8]) & \
                                    (cluster_data['LotArea'] == index[9]) & \
                                    (cluster_data['proposed_floor_area'] == index[10]) & \
                                    (cluster_data['Block'] == index[11]) & \
                                    (cluster_data['LotDepth'] == index[12]) & \
                                    (cluster_data['max_height'] == index[13]) & \
                                    (cluster_data['is_outlier'] == index[14])

                stratum_data = cluster_data[filter_conditions]

                if not stratum_data.empty:
                    sampled_stratum = stratum_data.sample(n=size, random_state=1)
                    sampled_data = pd.concat([sampled_data, sampled_stratum])
                else:
                    print(f"No data found for strata {index} in cluster data.")
            except KeyError as e:
                print(f"KeyError for strata {index} with size {size}: {e}")
            except Exception as e:
                print(f"Unexpected error for strata {index} with size {size}: {e}")

    return sampled_data

def analyze_property(idx, row, current_zoning_data, proposed_zoning_data, params):
    try:
        print(f"Analyzing property {idx} with params: {params}")
        current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
        print(f"Current sim for property {idx}: {current_sim}")
        proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
        print(f"Proposed sim for property {idx}: {proposed_sim}")
        return current_sim, proposed_sim
    except Exception as e:
        print(f"Error analyzing property {idx}: {e}")
        return pd.DataFrame(), pd.DataFrame()

print("Sampling parameters...")
params = sample_parameters(len(merged_gdf_r1_r5))

# Sample the data for non-oversampled clusters
non_oversampled_clusters = {k: v for k, v in cluster_sample_sizes.items() if k not in oversampled_clusters}
sampled_non_oversampled_clusters = {}

for cluster_id in non_oversampled_clusters.keys():
    cluster_data = merged_gdf_r1_r5[merged_gdf_r1_r5['cluster'] == cluster_id]
    sample_size = non_oversampled_clusters[cluster_id]
    sampled_non_oversampled_clusters[cluster_id] = cluster_data.sample(n=sample_size, random_state=1)

# Sample the data for oversampled clusters
sampled_oversampled_clusters = {}

for cluster_id in strata_sample_sizes.keys():
    cluster_data = labeled_clusters[cluster_id]
    strata_sizes = strata_sample_sizes[cluster_id]
    print(f"Sampling for oversampled cluster {cluster_id} with strata sizes:\n{strata_sizes}")
    sampled_oversampled_clusters[cluster_id] = sample_strata(cluster_data, strata_sizes)

# Combine the sampled data from non-oversampled and oversampled clusters
final_sampled_data = pd.concat([*sampled_non_oversampled_clusters.values(), *sampled_oversampled_clusters.values()], ignore_index=True)

print("Sampled data from all clusters:")
print(final_sampled_data)

# Assign parameters to the sampled data
sampled_params = pd.DataFrame(params)

# Ensure the correct number of parameters
sampled_params = sampled_params.head(len(final_sampled_data))

# Debug individual samples
sample_idx = 0  # Change this to inspect different samples
print(f"Sample {sample_idx} data:\n", final_sampled_data.iloc[sample_idx])
print(f"Sample {sample_idx} params:\n", sampled_params.iloc[sample_idx])

# Check data types
print("Data types of sample data:")
print(final_sampled_data.dtypes)

print("Data types of parameters:")
print(sampled_params.dtypes)

# Minimal example to debug analyze_redevelopment_potential_single
example_idx = 0
example_row = final_sampled_data.iloc[example_idx]
example_params = sampled_params.iloc[example_idx]

try:
    example_current_sim = analyze_redevelopment_potential_single(example_row, current_zoning_data, example_params)
    print(f"Example Current Simulation: {example_current_sim}")

    example_proposed_sim = analyze_redevelopment_potential_single(example_row, proposed_zoning_data, example_params)
    print(f"Example Proposed Simulation: {example_proposed_sim}")
except Exception as e:
    print(f"Error in minimal example: {e}")

# Perform analysis on the sampled data
num_cores = multiprocessing.cpu_count()
results = Parallel(n_jobs=num_cores)(
    delayed(analyze_property)(idx, row, current_zoning_data, proposed_zoning_data, sampled_params.iloc[idx])
    for idx, row in tqdm(final_sampled_data.iterrows(), total=len(final_sampled_data))
)

# Combine the results
current_simulations = pd.concat([result[0] for result in results], ignore_index=True)
proposed_simulations = pd.concat([result[1] for result in results], ignore_index=True)

print("Current Standards Simulation Results:")
print(current_simulations.head())

print("Proposed Standards Simulation Results:")
print(proposed_simulations.head())

# Visualization function
def visualize_results(current_simulations, proposed_simulations):
    fig, axs = plt.subplots(3, 2, figsize=(12, 18))

    sns.histplot(current_simulations['irr'], bins=50, kde=True, ax=axs[0, 0])
    axs[0, 0].set_title('Current IRR Distribution')
    sns.histplot(proposed_simulations['irr'], bins=50, kde=True, ax=axs[0, 1])
    axs[0, 1].set_title('Proposed IRR Distribution')

    sns.histplot(current_simulations['equity_multiple'], bins=50, kde=True, ax=axs[1, 0])
    axs[1, 0].set_title('Current Equity Multiple Distribution')
    sns.histplot(proposed_simulations['equity_multiple'], bins=50, kde=True, ax=axs[1, 1])
    axs[1, 1].set_title('Proposed Equity Multiple Distribution')

    sns.histplot(current_simulations['npv'], bins=50, kde=True, ax=axs[2, 0])
    axs[2, 0].set_title('Current NPV Distribution')
    sns.histplot(proposed_simulations['npv'], bins=50, kde=True, ax=axs[2, 1])
    axs[2, 1].set_title('Proposed NPV Distribution')

    plt.tight_layout()
    plt.show()

visualize_results(current_simulations, proposed_simulations)

current_simulations.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Function to create histograms and box plots for IRR, EM, and NPV
def plot_simulation_results(simulation_results, title_prefix):
    fig, axes = plt.subplots(3, 2, figsize=(15, 15))
    fig.suptitle(f'{title_prefix} Simulation Results', fontsize=16)

    # Histograms
    sns.histplot(simulation_results['irr'].dropna(), bins=50, kde=True, ax=axes[0, 0])
    axes[0, 0].set_title('IRR Distribution')
    sns.histplot(simulation_results['equity_multiple'].dropna(), bins=50, kde=True, ax=axes[1, 0])
    axes[1, 0].set_title('Equity Multiple Distribution')
    sns.histplot(simulation_results['npv'].dropna(), bins=50, kde=True, ax=axes[2, 0])
    axes[2, 0].set_title('NPV Distribution')

    # Box plots
    sns.boxplot(x=simulation_results['irr'], ax=axes[0, 1])
    axes[0, 1].set_title('IRR Box Plot')
    sns.boxplot(x=simulation_results['equity_multiple'], ax=axes[1, 1])
    axes[1, 1].set_title('Equity Multiple Box Plot')
    sns.boxplot(x=simulation_results['npv'], ax=axes[2, 1])
    axes[2, 1].set_title('NPV Box Plot')

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()

# Plot current standards simulation results
plot_simulation_results(current_simulations, 'Current Standards')

# Plot proposed standards simulation results
plot_simulation_results(proposed_simulations, 'Proposed Standards')

# Identify properties with NaN IRR in current and proposed simulations
nan_irr_current = current_simulations[current_simulations['irr'].isna()]
nan_irr_proposed = proposed_simulations[proposed_simulations['irr'].isna()]

print("Properties with NaN IRR in current simulations:")
print(nan_irr_current)

print("Properties with NaN IRR in proposed simulations:")
print(nan_irr_proposed)

# Define the get_cash_flows function based on your cash flow model
def get_cash_flows(property_data):
    initial_investment = property_data['redevelopment_cost']
    annual_return = property_data['redeveloped_value'] / 10  # Example: 10 years of returns
    cash_flows = [-initial_investment] + [annual_return] * 10  # 10 years of returns
    return cash_flows

# Investigate cash flows for properties with NaN IRR in current simulations
print("\nCash flows for properties with NaN IRR in current simulations:")
for idx, row in nan_irr_current.iterrows():
    cash_flows = get_cash_flows(row)
    print(f"Cash flows for property index {idx} (current simulation): {cash_flows}")

# Investigate cash flows for properties with NaN IRR in proposed simulations
print("\nCash flows for properties with NaN IRR in proposed simulations:")
for idx, row in nan_irr_proposed.iterrows():
    cash_flows = get_cash_flows(row)
    print(f"Cash flows for property index {idx} (proposed simulation): {cash_flows}")

import gc

# Clear specific variables

# Call garbage collector
gc.collect()

import pandas as pd

# Sample nested dictionaries
current_zoning_data = {
    'Residential FAR (max)': {'R4A': 1.0, 'R4': 1.0, 'R3-1': 0.75, 'R4-1': 1.0, 'R5': 1.5, 'R5B': 1.5, 'R4B': 1.0, 'R3X': 0.75, 'R3A': 0.75, 'R2': 0.75, 'R1-2': 0.75, 'R3-2': 0.75},
    'Front Yard Depth (min)': {'R4A': 10.0, 'R4': 10.0, 'R3-1': 15.0, 'R4-1': 10.0, 'R5': 10.0, 'R5B': 5.0, 'R4B': 5.0, 'R3X': 10.0, 'R3A': 10.0, 'R2': 15.0, 'R1-2': 20.0, 'R3-2': 15.0},
    'Rear Yard Depth (min)': {'R4A': 20.0, 'R4': 20.0, 'R3-1': 20.0, 'R4-1': 20.0, 'R5': 20.0, 'R5B': 20.0, 'R4B': 20.0, 'R3X': 20.0, 'R3A': 20.0, 'R2': 20.0, 'R1-2': 20.0, 'R3-2': 20.0},
    'Side Yards (total width)': {'R4A': 10.0, 'R4': 10.0, 'R3-1': 10.0, 'R4-1': 10.0, 'R5': 10.0, 'R5B': 10.0, 'R4B': 10.0, 'R3X': 10.0, 'R3A': 10.0, 'R2': 10.0, 'R1-2': 10.0, 'R3-2': 10.0},
    'Building Height (max)': {'R4A': 35.0, 'R4': 35.0, 'R3-1': 35.0, 'R4-1': 35.0, 'R5': 45.0, 'R5B': 35.0, 'R4B': 25.0, 'R3X': 35.0, 'R3A': 35.0, 'R2': 35.0, 'R1-2': 35.0, 'R3-2': 35.0}
}

proposed_zoning_data = {
    'Residential FAR (max)': {'R4A': 1.1, 'R4': 1.1, 'R3-1': 0.8, 'R4-1': 1.1, 'R5': 1.6, 'R5B': 1.6, 'R4B': 1.1, 'R3X': 0.8, 'R3A': 0.8, 'R2': 0.8, 'R1-2': 0.8, 'R3-2': 0.8},
    'Front Yard Depth (min)': {'R4A': 11.0, 'R4': 11.0, 'R3-1': 16.0, 'R4-1': 11.0, 'R5': 11.0, 'R5B': 6.0, 'R4B': 6.0, 'R3X': 11.0, 'R3A': 11.0, 'R2': 16.0, 'R1-2': 21.0, 'R3-2': 16.0},
    'Rear Yard Depth (min)': {'R4A': 21.0, 'R4': 21.0, 'R3-1': 21.0, 'R4-1': 21.0, 'R5': 21.0, 'R5B': 21.0, 'R4B': 21.0, 'R3X': 21.0, 'R3A': 21.0, 'R2': 21.0, 'R1-2': 21.0, 'R3-2': 21.0},
    'Side Yards (total width)': {'R4A': 11.0, 'R4': 11.0, 'R3-1': 11.0, 'R4-1': 11.0, 'R5': 11.0, 'R5B': 11.0, 'R4B': 11.0, 'R3X': 11.0, 'R3A': 11.0, 'R2': 11.0, 'R1-2': 11.0, 'R3-2': 11.0},
    'Building Height (max)': {'R4A': 36.0, 'R4': 36.0, 'R3-1': 36.0, 'R4-1': 36.0, 'R5': 46.0, 'R5B': 36.0, 'R4B': 26.0, 'R3X': 36.0, 'R3A': 36.0, 'R2': 36.0, 'R1-2': 36.0, 'R3-2': 36.0}
}

# Convert nested dictionaries to DataFrames
current_zoning_df = pd.DataFrame(current_zoning_data).reset_index().rename(columns={'index': 'ZoneDist1'})
proposed_zoning_df = pd.DataFrame(proposed_zoning_data).reset_index().rename(columns={'index': 'ZoneDist1'})

# Merge with current_zoning_df and proposed_zoning_df on 'ZoneDist1'
merged_with_current = merged_gdf_r1_r5.merge(current_zoning_df, on='ZoneDist1', how='left')
final_merged_gdf = merged_with_current.merge(proposed_zoning_df, on='ZoneDist1', how='left', suffixes=('_current', '_proposed'))

# Print the resulting DataFrame's columns to confirm
print(final_merged_gdf.columns)
print(final_merged_gdf.head())
merged_gdf_r1_r5 = final_merged_gdf

# Rename the columns for current and proposed zoning data
merged_gdf_r1_r5.rename(columns={
    'Residential FAR (max)_current': 'max_residential_far_current',
    'Front Yard Depth (min)_current': 'min_front_yard_depth_current',
    'Rear Yard Depth (min)_current': 'min_rear_yard_depth_current',
    'Side Yards (total width)_current': 'min_side_yard_width_current',
    'Building Height (max)_current': 'max_building_height_current',
    'Residential FAR (max)_proposed': 'max_residential_far_proposed',
    'Front Yard Depth (min)_proposed': 'min_front_yard_depth_proposed',
    'Rear Yard Depth (min)_proposed': 'min_rear_yard_depth_proposed',
    'Side Yards (total width)_proposed': 'min_side_yard_width_proposed',
    'Building Height (max)_proposed': 'max_building_height_proposed'
}, inplace=True)

# Drop the 'R4A', 'R4', etc. columns
columns_to_drop = [
    'R4A', 'R4', 'R3-1', 'R4-1', 'R5', 'R5B', 'R4B', 'R3X', 'R3A', 'R2', 'R1-2', 'R3-2',
    'R4A_proposed', 'R4_proposed', 'R3-1_proposed', 'R4-1_proposed', 'R5_proposed', 'R5B_proposed',
    'R4B_proposed', 'R3X_proposed', 'R3A_proposed', 'R2_proposed', 'R1-2_proposed', 'R3-2_proposed'
]

merged_gdf_r1_r5.drop(columns=columns_to_drop, inplace=True)

# List of relevant features for scaling
features = [
    'LotArea', 'LotFront', 'LotDepth', 'BldgFront', 'BldgDepth',
    'NumFloors', 'height', 'ZoneDist1', 'current_FAR', 'ResidFAR',
    'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed',
    'min_rear_yard_depth_proposed', 'min_side_yard_width_proposed',
    'max_building_height_proposed', 'CommFAR', 'FacilFAR', 'LandUse',
    'BldgClass', 'BldgArea', 'ResArea', 'FrontYardDepth', 'RearYardDepth',
    'LotCoverageRatio', 'BuildingAspectRatio', 'BuildableArea',
    'SetbackComplianceRatio', 'FloorAreaPotential',
    'Centroid', 'Area'
]

# Get the list of columns in merged_gdf_r1_r5
existing_columns = merged_gdf_r1_r5.columns.tolist()

# Find missing features
missing_features = [feature for feature in features if feature not in existing_columns]

# Find features that are present
present_features = [feature for feature in features if feature in existing_columns]

print("Missing features that remain to be computed:")
print(missing_features)

print("\nFeatures that are already present:")
print(present_features)

import geopandas as gpd
import numpy as np

# Function to compute missing derived features for current and proposed zoning standards
def compute_missing_features(gdf):
    # LotCoverageRatio: Ratio of building footprint to lot area
    gdf['LotCoverageRatio'] = gdf['BldgArea'] / gdf['LotArea']

    # BuildingAspectRatio: Ratio of building front to building depth
    gdf['BuildingAspectRatio'] = gdf['BldgFront'] / gdf['BldgDepth']

    # BuildableArea under current and proposed standards
    gdf['BuildableArea_current'] = gdf['LotArea'] - (
        gdf['FrontYardDepth'] + gdf['RearYardDepth'] + gdf['min_side_yard_width_current']
    )
    gdf['BuildableArea_proposed'] = gdf['LotArea'] - (
        gdf['FrontYardDepth'] + gdf['RearYardDepth'] + gdf['min_side_yard_width_proposed']
    )

    # SetbackComplianceRatio under current and proposed standards
    gdf['SetbackComplianceRatio_current'] = (
        (gdf['FrontYardDepth'] / gdf['min_front_yard_depth_current']) *
        (gdf['RearYardDepth'] / gdf['min_rear_yard_depth_current']) *
        (gdf['min_side_yard_width_current'] / gdf['min_side_yard_width_current'])
    )
    gdf['SetbackComplianceRatio_proposed'] = (
        (gdf['FrontYardDepth'] / gdf['min_front_yard_depth_proposed']) *
        (gdf['RearYardDepth'] / gdf['min_rear_yard_depth_proposed']) *
        (gdf['min_side_yard_width_proposed'] / gdf['min_side_yard_width_proposed'])
    )

    # FloorAreaPotential under current and proposed standards
    gdf['FloorAreaPotential_current'] = gdf['max_residential_far_current'] * gdf['LotArea']
    gdf['FloorAreaPotential_proposed'] = gdf['max_residential_far_proposed'] * gdf['LotArea']

    # Centroid: Coordinates of the centroid of the geometry
    gdf['Centroid'] = gdf['geometry'].centroid

    # Area: Area of the geometry
    gdf['Area'] = gdf['geometry'].area

    return gdf

# Compute missing features
merged_gdf_r1_r5 = compute_missing_features(merged_gdf_r1_r5)

# Split Centroid into x and y coordinates
merged_gdf_r1_r5['Centroid_x'] = merged_gdf_r1_r5['Centroid'].x
merged_gdf_r1_r5['Centroid_y'] = merged_gdf_r1_r5['Centroid'].y

# Drop the original Centroid feature if desired
merged_gdf_r1_r5.drop(columns=['Centroid'], inplace=True)

# Print the first few rows to verify
print(merged_gdf_r1_r5[['LotCoverageRatio', 'BuildingAspectRatio',
                        'BuildableArea_current', 'BuildableArea_proposed',
                        'SetbackComplianceRatio_current', 'SetbackComplianceRatio_proposed',
                        'FloorAreaPotential_current', 'FloorAreaPotential_proposed',
                        'Centroid_x', 'Centroid_y', 'Area']].head())

import numpy as np
import pandas as pd

def check_special_values(df, columns, special_values_numeric=[0, 1], special_values_string=['NaN', 'Null', 'None', 'N/A', 'Missing', 'UNKNOWN', 'inf', '-inf', '-0']):
    results = {}
    epsilon = np.finfo(float).eps
    custom_epsilon = 1e-10

    for column in columns:
        if column in df.columns:
            results[column] = {
                'NaN_count': df[column].isna().sum(),
                'inf_count': 0,
                'zero_count': 0,
                'neg_zero_count': 0,
                'one_count': 0,
                'near_zero_count': 0,
                'near_one_count': 0,
                'near_max_float_count': 0,
                'near_min_float_count': 0,
                'empty_string_count': 0,
                'unique_value_count': df[column].nunique()
            }

            # Add counts for special numeric values
            for value in special_values_numeric:
                results[column][f'{value}_count'] = 0

            # Add counts for special string values
            for value in special_values_string:
                results[column][f'{repr(value)}_count'] = 0

            # Numeric-specific checks
            if pd.api.types.is_numeric_dtype(df[column]):
                non_nan = df[column].dropna()
                results[column].update({
                    'inf_count': np.isinf(non_nan).sum(),
                    'zero_count': np.isclose(non_nan, 0, atol=custom_epsilon).sum(),
                    'neg_zero_count': (non_nan == -0).sum(),
                    'one_count': np.isclose(non_nan, 1, atol=epsilon).sum(),
                    'near_zero_count': ((np.abs(non_nan) > 0) & (np.abs(non_nan) < custom_epsilon)).sum(),
                    'near_one_count': ((np.abs(non_nan - 1) > 0) & (np.abs(non_nan - 1) < epsilon)).sum(),
                    'near_max_float_count': np.isclose(non_nan, np.finfo(float).max, atol=epsilon).sum(),
                    'near_min_float_count': np.isclose(non_nan, np.finfo(float).min, atol=epsilon).sum()
                })
                for value in special_values_numeric:
                    results[column][f'{value}_count'] = np.isclose(non_nan, value, atol=epsilon).sum()

            # String-specific checks
            if pd.api.types.is_string_dtype(df[column]) or pd.api.types.is_object_dtype(df[column]):
                non_nan = df[column].dropna().astype(str)
                normalized_column = non_nan.str.strip().str.lower()
                results[column]['empty_string_count'] = (normalized_column == '').sum()
                for value in special_values_string:
                    results[column][f'{repr(value)}_count'] = (normalized_column == value.lower()).sum()
        else:
            print(f"Warning: Column '{column}' not found in the DataFrame. Skipping this column.")

    return pd.DataFrame(results).T

# Step 1: Remove CommFAR from the features list since it's entirely zero
features_of_interest = [
    'LotArea', 'LotFront', 'LotDepth', 'BldgFront', 'BldgDepth',
    'NumFloors', 'height', 'ZoneDist1', 'current_FAR', 'ResidFAR',
    'min_front_yard_depth_current', 'min_rear_yard_depth_current',
    'min_side_yard_width_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed',
    'min_rear_yard_depth_proposed', 'min_side_yard_width_proposed',
    'max_building_height_proposed', 'FacilFAR', 'LandUse',
    'BldgClass', 'BldgArea', 'ResArea', 'FrontYardDepth', 'RearYardDepth',
    'LotCoverageRatio', 'BuildingAspectRatio', 'BuildableArea_current',
    'BuildableArea_proposed', 'SetbackComplianceRatio_current', 'SetbackComplianceRatio_proposed',
    'FloorAreaPotential_current', 'FloorAreaPotential_proposed',
    'Centroid_x', 'Centroid_y', 'Area'
]

# Run the updated function
special_values_report = check_special_values(df_sample, df_sample.columns)

# Display the special values report
print(special_values_report)

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression

# Create a copy of the dataframe to preserve original data
df = merged_gdf_r1_r5.copy()

# Check and remove CommFAR from features list if it's present
if 'CommFAR' in features_of_interest:
    features_of_interest.remove('CommFAR')

# Handle categorical variable LandUse by filling missing with "Unknown"
df['LandUse'].fillna('Unknown', inplace=True)

# One-hot encode LandUse
onehot = OneHotEncoder(sparse_output=False, dtype=bool)
landuse_encoded = pd.DataFrame(onehot.fit_transform(df[['LandUse']]),
                               columns=onehot.get_feature_names_out(['LandUse']),
                               index=df.index)

# Create binary flags for zoning compliance
df['is_FAR_compliant'] = (df['current_FAR'] <= df['max_residential_far_proposed']).astype(bool)
df['is_height_compliant'] = (df['height'] <= df['max_building_height_proposed']).astype(bool)
df['is_front_yard_compliant'] = (df['FrontYardDepth'] >= df['min_front_yard_depth_proposed']).astype(bool)

# Create binary flags for NumFloors and BuildingAspectRatio estimation
df['NumFloors_estimated_flag'] = df['NumFloors'].isnull().astype(bool)
df['BuildingAspectRatio_estimated_flag'] = df['BuildingAspectRatio'].isnull().astype(bool)

# Estimate NumFloors using other features (e.g., height, BldgArea, LotArea)
X_numfloors = df[['height', 'BldgArea', 'LotArea']].fillna(0)
y_numfloors = df['NumFloors']

# Drop rows where target (y) has NaNs
y_numfloors_non_nan = y_numfloors.dropna()
X_numfloors_non_nan = X_numfloors.loc[y_numfloors_non_nan.index]

# Fit the model to estimate NumFloors
model_numfloors = LinearRegression()
model_numfloors.fit(X_numfloors_non_nan, y_numfloors_non_nan)

# Predict NumFloors for missing entries only
predicted_numfloors = model_numfloors.predict(X_numfloors[df['NumFloors'].isnull()])

# Fill in the missing NumFloors with predicted values
df.loc[df['NumFloors'].isnull(), 'NumFloors'] = predicted_numfloors

# Estimate BuildingAspectRatio using other features (e.g., height, BldgArea, LotArea)
X_building_aspect = df[['height', 'BldgArea', 'LotArea']].fillna(0)
y_building_aspect = df['BuildingAspectRatio']

# Drop rows where target (y) has NaNs
y_building_aspect_non_nan = y_building_aspect.dropna()
X_building_aspect_non_nan = X_building_aspect.loc[y_building_aspect_non_nan.index]

# Fit the model to estimate BuildingAspectRatio
model_building_aspect = LinearRegression()
model_building_aspect.fit(X_building_aspect_non_nan, y_building_aspect_non_nan)

# Predict BuildingAspectRatio for missing entries only
predicted_building_aspect = model_building_aspect.predict(X_building_aspect[df['BuildingAspectRatio'].isnull()])

# Fill in the missing BuildingAspectRatio with predicted values
df.loc[df['BuildingAspectRatio'].isnull(), 'BuildingAspectRatio'] = predicted_building_aspect

# Ensure all features_of_interest are present in df
features_of_interest = [f for f in features_of_interest if f in df.columns]

# Combine features for clustering
df_final = pd.concat([df[features_of_interest], landuse_encoded,
                      df[['NumFloors_estimated_flag', 'BuildingAspectRatio_estimated_flag',
                          'is_FAR_compliant', 'is_height_compliant', 'is_front_yard_compliant']]],
                     axis=1)

# Update the features list to use for clustering
features_for_clustering = df_final.columns.tolist()

# Function to check variability in one-hot encoded features and report them
def report_low_variability_onehot(df, features, threshold=0.99):
    low_variability_features = []
    for feature in features:
        if df[feature].ndim == 1:  # Ensure it's a single column
            # Calculate the percentage of the most frequent value
            most_frequent_percentage = df[feature].value_counts(normalize=True).max()
            if most_frequent_percentage > threshold:
                low_variability_features.append((feature, most_frequent_percentage * 100))

    return low_variability_features

# Report low variability in one-hot encoded features without removing them
onehot_encoded_features = [feature for feature in df_final.columns if feature.startswith('LandUse_')]
low_variability_report = report_low_variability_onehot(df_final, onehot_encoded_features)

# Print the low variability one-hot encoded features
print("One-hot encoded features with low variability:")
for feature, percentage in low_variability_report:
    print(f"{feature}: {percentage:.2f}%")

# Print the updated features list for clustering
print("\nUpdated features list for clustering:")
print(features_for_clustering)

df_final.head()

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder

# Create a copy of the dataframe to preserve original data
df = merged_gdf_r1_r5.copy()

# Handle categorical variable LandUse by filling missing with "Unknown"
df['LandUse'].fillna('Unknown', inplace=True)

# One-hot encode LandUse
onehot = OneHotEncoder(sparse_output=False, dtype=int)
landuse_encoded = pd.DataFrame(onehot.fit_transform(df[['LandUse']]),
                               columns=onehot.get_feature_names_out(['LandUse']),
                               index=df.index)

# List of columns to keep
columns_to_keep = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_current', 'min_front_yard_depth_proposed',
    'max_building_height_proposed', 'max_residential_far_proposed',
    'BldgArea', 'BuildingAspectRatio', 'Centroid_x', 'Centroid_y', 'Area',
    'RearYardDepth'  # Keep RearYardDepth to calculate compliance, but it will be excluded later
]

# Keep only the specified columns
df = df[columns_to_keep]

# Calculate SideYardWidth assuming the building is centered on the lot
df['SideYardWidth'] = (df['LotFront'] - df['BldgFront']) / 2

# Create binary flags for missing NumFloors and BuildingAspectRatio before estimation
df['NumFloors_estimated_flag'] = df['NumFloors'].isnull().astype(int)
df['BuildingAspectRatio_estimated_flag'] = df['BuildingAspectRatio'].isnull().astype(int)

# Estimate NumFloors using other features
X_numfloors = df[['height', 'BldgArea', 'LotArea']].fillna(0)
y_numfloors = df['NumFloors']

# Drop rows where target (y) has NaNs
y_numfloors_non_nan = y_numfloors.dropna()
X_numfloors_non_nan = X_numfloors.loc[y_numfloors_non_nan.index]

# Fit the model to estimate NumFloors
model_numfloors = LinearRegression()
model_numfloors.fit(X_numfloors_non_nan, y_numfloors_non_nan)

# Predict NumFloors for missing entries only
predicted_numfloors = model_numfloors.predict(X_numfloors[df['NumFloors'].isnull()])

# Fill in the missing NumFloors with predicted values
df.loc[df['NumFloors'].isnull(), 'NumFloors'] = predicted_numfloors

# Estimate BuildingAspectRatio using other features
X_building_aspect = df[['height', 'BldgArea', 'LotArea']].fillna(0)
y_building_aspect = df['BuildingAspectRatio']

# Drop rows where target (y) has NaNs
y_building_aspect_non_nan = y_building_aspect.dropna()
X_building_aspect_non_nan = X_building_aspect.loc[y_building_aspect_non_nan.index]

# Fit the model to estimate BuildingAspectRatio
model_building_aspect = LinearRegression()
model_building_aspect.fit(X_building_aspect_non_nan, y_building_aspect_non_nan)

# Predict BuildingAspectRatio for missing entries only
predicted_building_aspect = model_building_aspect.predict(X_building_aspect[df['BuildingAspectRatio'].isnull()])

# Fill in the missing BuildingAspectRatio with predicted values
df.loc[df['BuildingAspectRatio'].isnull(), 'BuildingAspectRatio'] = predicted_building_aspect

# Create binary flags for zoning compliance
df['is_front_yard_compliant_current'] = (df['BldgFront'] >= df['min_front_yard_depth_current']).astype(int)
df['is_front_yard_compliant_proposed'] = (df['BldgFront'] >= df['min_front_yard_depth_proposed']).astype(int)
df['is_height_compliant'] = (df['height'] <= df['max_building_height_proposed']).astype(int)
df['is_FAR_compliant'] = (df['current_FAR'] <= df['max_residential_far_proposed']).astype(int)

# Add binary flags for rear and side yard compliance only if the relevant columns exist in merged_gdf_r1_r5
if 'RearYardDepth' in df.columns and 'min_rear_yard_depth_current' in merged_gdf_r1_r5.columns:
    df['is_rear_yard_compliant_current'] = (df['RearYardDepth'] >= merged_gdf_r1_r5['min_rear_yard_depth_current']).astype(int)
    df['is_rear_yard_compliant_proposed'] = (df['RearYardDepth'] >= merged_gdf_r1_r5['min_rear_yard_depth_proposed']).astype(int)

if 'SideYardWidth' in df.columns and 'min_side_yard_width_current' in merged_gdf_r1_r5.columns:
    df['is_side_yard_compliant_current'] = (df['SideYardWidth'] >= merged_gdf_r1_r5['min_side_yard_width_current']).astype(int)
    df['is_side_yard_compliant_proposed'] = (df['SideYardWidth'] >= merged_gdf_r1_r5['min_side_yard_width_proposed']).astype(int)

# Rename specific columns
df = df.rename(columns={
    'min_front_yard_depth_proposed': 'min_front_yard_depth_zoning',
    'max_building_height_proposed': 'max_building_height_zoning',
    'max_residential_far_proposed': 'max_residential_far_zoning'
})

# Combine features for clustering
df_final = pd.concat([df, landuse_encoded], axis=1)

# List of continuous columns (excluding the binary flags and SideYardWidth/RearYardDepth)
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_current', 'min_front_yard_depth_zoning',
    'max_building_height_zoning', 'max_residential_far_zoning', 'BldgArea',
    'BuildingAspectRatio', 'Centroid_x', 'Centroid_y', 'Area'
]

df_final = df_final.drop(columns=['RearYardDepth', 'SideYardWidth'])

# Print the shape of the final dataframe and the list of continuous columns
print("Shape of df_final:", df_final.shape)
print("Updated continuous columns:", updated_continuous_columns)
print("All columns in df_final:", df_final.columns.tolist())

# Print the number of compliant and non-compliant buildings for each zoning aspect
compliance_flags = [col for col in df_final.columns if col.startswith('is_') and col.endswith('_compliant')]

for flag in compliance_flags:
    compliant = df_final[flag].sum()
    non_compliant = len(df_final) - compliant
    print(f"\n{flag}:")
    print(f"Compliant: {compliant} ({compliant/len(df_final)*100:.2f}%)")
    print(f"Non-compliant: {non_compliant} ({non_compliant/len(df_final)*100:.2f}%)")

df_final.head()

# Ensure all relevant features are included in the list
features_of_interest = df_sample.columns.tolist()

# Run the updated function
special_values_report = check_special_values(df_sample, features_of_interest)

# Display the special values report
print(special_values_report)

import pandas as pd
import numpy as np

def check_feature_properties(df, features):
    results = {}
    epsilon = np.finfo(float).eps

    for feature in features:
        unique_values = df[feature].nunique()
        most_frequent_value_count = df[feature].value_counts().max()
        most_frequent_value_percentage = most_frequent_value_count / len(df) * 100
        sparsity = (df[feature] == 0).sum() / len(df) * 100  # Percentage of zero values
        variance = df[feature].var()
        std_dev = df[feature].std()
        mean = df[feature].mean()
        outliers = ((df[feature] < mean - 3 * std_dev) | (df[feature] > mean + 3 * std_dev)).sum()

        results[feature] = {
            'unique_values': unique_values,
            'most_frequent_value_percentage': most_frequent_value_percentage,
            'sparsity_percentage': sparsity,
            'variance': variance,
            'outliers_count': outliers
        }

    return pd.DataFrame(results).T

# Include all features including one-hot encoded features and binary flags
features_to_check = df_sample.columns.tolist()

# Run the checks
feature_properties_report = check_feature_properties(df_sample, features_to_check)

# Display the report
print(feature_properties_report)

df_sample.shape

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Step 1: Remove constant or near-constant features from features list
constant_features = ['min_rear_yard_depth_current', 'min_side_yard_width_current', 'min_rear_yard_depth_proposed', 'min_side_yard_width_proposed']
features_of_interest = [feature for feature in features_of_interest if feature not in constant_features]

# Step 2: Identify numeric features
numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()

# Step 3: Identify categorical features
categorical_features = df.select_dtypes(include=[object]).columns.tolist()

# Step 4: Remove categorical features from numeric scaling
features_to_scale = [feature for feature in features_of_interest if feature in numeric_features]

# Step 5: Scale numeric features
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[features_to_scale]), columns=features_to_scale)

# Step 6: Combine scaled numerical features, one-hot encoded categorical features, and binary flags
df_final = pd.concat([df_scaled, landuse_encoded, df[['NumFloors_estimated_flag', 'ResArea_estimated_flag', 'BuildingAspectRatio_estimated_flag']]], axis=1)

# Step 7: Update the features list to use for clustering
features_for_clustering = df_final.columns.tolist()

# Function to check variability in one-hot encoded features and report them
def report_low_variability_onehot(df, features, threshold=0.99):
    low_variability_features = []
    for feature in features:
        if df[feature].ndim == 1:  # Ensure it's a single column
            # Calculate the percentage of the most frequent value
            most_frequent_percentage = df[feature].value_counts(normalize=True).max()
            if most_frequent_percentage > threshold:
                low_variability_features.append((feature, most_frequent_percentage * 100))

    return low_variability_features

# Step 8: Report low variability in one-hot encoded features without removing them
onehot_encoded_features = [feature for feature in df_final.columns if feature.startswith('LandUse_')]
low_variability_report = report_low_variability_onehot(df_final, onehot_encoded_features)

# Print the low variability one-hot encoded features
print("One-hot encoded features with low variability:")
for feature, percentage in low_variability_report:
    print(f"{feature}: {percentage:.2f}%")

# Step 9: Print the updated features list for clustering
print("Updated features list for clustering:")
print(features_for_clustering)

import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Assuming df_final is your DataFrame
# If it's not defined, you'll need to load your data first

# Select only the continuous columns
X = df_final[continuous_columns]

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Sort the features by VIF in descending order
vif_data = vif_data.sort_values("VIF", ascending=False)

# Print the VIF values
print(vif_data)

# Identify features with high VIF (e.g., VIF > 5)
high_vif_features = vif_data[vif_data["VIF"] > 5]
print("\nFeatures with high VIF (>5):")
print(high_vif_features)

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Assuming df_final is your original DataFrame
# Create a copy of the original DataFrame
df_analysis = df_final.copy()

# List of columns to drop
columns_to_drop = [
    'LotDepth', 'BldgDepth', 'RearYardDepth', 'FrontYardDepth',
    'min_front_yard_depth_current', 'max_building_height_current',
    'ResidFAR', 'ResArea'
]

# Drop the specified columns
df_analysis = df_analysis.drop(columns=columns_to_drop)

# Rename columns for clarity
df_analysis = df_analysis.rename(columns={
    'min_front_yard_depth_proposed': 'min_front_yard_depth_zoning',
    'max_building_height_proposed': 'max_building_height_zoning',
    'max_residential_far_proposed': 'max_residential_far_zoning'
})

# List of remaining continuous columns
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_zoning', 'max_building_height_zoning',
    'max_residential_far_zoning', 'BldgArea', 'BuildingAspectRatio',
    'Centroid_x', 'Centroid_y', 'Area'
]

# Recalculate VIF for the updated feature set
X = df_analysis[updated_continuous_columns]

vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Sort the features by VIF in descending order
vif_data = vif_data.sort_values("VIF", ascending=False)

# Print the updated VIF values
print("Updated VIF values:")
print(vif_data)

# Identify features with high VIF (e.g., VIF > 5)
high_vif_features = vif_data[vif_data["VIF"] > 5]
print("\nFeatures with high VIF (>5):")
print(high_vif_features)

import pandas as pd
import numpy as np

# Assuming df_final is the original DataFrame with all columns
# and df_analysis is the DataFrame we're working with (after dropping some columns)

# Create binary flags for zoning compliance
df_analysis['is_FAR_compliant'] = df_analysis['current_FAR'] <= df_analysis['max_residential_far_zoning']
df_analysis['is_height_compliant'] = df_analysis['height'] <= df_analysis['max_building_height_zoning']

# For FrontYardDepth, we need to use df_final
df_analysis['is_front_yard_compliant'] = df_final['FrontYardDepth'] >= df_analysis['min_front_yard_depth_zoning']

# Create a function to bin FAR utilization
def categorize_far_utilization(row):
    if row['current_FAR'] < 0.7 * row['max_residential_far_zoning']:
        return 0  # Significantly under-utilized
    elif row['current_FAR'] < row['max_residential_far_zoning']:
        return 1  # Slightly under-utilized
    elif row['current_FAR'] == row['max_residential_far_zoning']:
        return 2  # Fully utilized
    else:
        return 3  # Over-utilized

# Apply the function to create a new feature
df_analysis['FAR_utilization_category'] = df_analysis.apply(categorize_far_utilization, axis=1)

# Print the first few rows of the new features
print(df_analysis[['is_FAR_compliant', 'is_height_compliant', 'is_front_yard_compliant', 'FAR_utilization_category']].head())

# Calculate the percentage of compliant properties for each feature
compliance_percentages = {
    'FAR_compliance_percentage': df_analysis['is_FAR_compliant'].mean() * 100,
    'Height_compliance_percentage': df_analysis['is_height_compliant'].mean() * 100,
    'Front_yard_compliance_percentage': df_analysis['is_front_yard_compliant'].mean() * 100
}

print("\nCompliance Percentages:")
for key, value in compliance_percentages.items():
    print(f"{key}: {value:.2f}%")

# Display the distribution of FAR utilization categories
far_utilization_distribution = df_analysis['FAR_utilization_category'].value_counts(normalize=True) * 100
print("\nFAR Utilization Distribution:")
print(far_utilization_distribution)

# Calculate the percentage of properties that are fully compliant
fully_compliant = (df_analysis['is_FAR_compliant'] &
                   df_analysis['is_height_compliant'] &
                   df_analysis['is_front_yard_compliant']).mean() * 100

print(f"\nPercentage of fully compliant properties: {fully_compliant:.2f}%")

# Calculate the percentage of properties that are non-compliant in at least one aspect
non_compliant = (~df_analysis['is_FAR_compliant'] |
                 ~df_analysis['is_height_compliant'] |
                 ~df_analysis['is_front_yard_compliant']).mean() * 100

print(f"Percentage of properties non-compliant in at least one aspect: {non_compliant:.2f}%")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df_final is the original DataFrame with all columns
# and df_analysis is the DataFrame we're working with (after dropping some columns)

# Calculate degree of non-compliance
df_analysis['FAR_excess'] = (df_analysis['current_FAR'] - df_analysis['max_residential_far_zoning']) / df_analysis['max_residential_far_zoning']
df_analysis['height_excess'] = (df_analysis['height'] - df_analysis['max_building_height_zoning']) / df_analysis['max_building_height_zoning']
df_analysis['front_yard_deficit'] = (df_analysis['min_front_yard_depth_zoning'] - df_final['FrontYardDepth']) / df_analysis['min_front_yard_depth_zoning']

# Function to calculate average degree of non-compliance
def avg_non_compliance(series):
    return series[series > 0].mean() * 100

# Calculate average degree of non-compliance for each feature
avg_far_excess = avg_non_compliance(df_analysis['FAR_excess'])
avg_height_excess = avg_non_compliance(df_analysis['height_excess'])
avg_front_yard_deficit = avg_non_compliance(df_analysis['front_yard_deficit'])

print("Average degree of non-compliance:")
print(f"FAR excess: {avg_far_excess:.2f}%")
print(f"Height excess: {avg_height_excess:.2f}%")
print(f"Front yard deficit: {avg_front_yard_deficit:.2f}%")

# Create histograms of the degree of non-compliance
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))

ax1.hist(df_analysis['FAR_excess'][df_analysis['FAR_excess'] > 0], bins=50)
ax1.set_title('Distribution of FAR Excess (Non-compliant properties)')
ax1.set_xlabel('FAR Excess (%)')
ax1.set_ylabel('Count')

ax2.hist(df_analysis['height_excess'][df_analysis['height_excess'] > 0], bins=50)
ax2.set_title('Distribution of Height Excess (Non-compliant properties)')
ax2.set_xlabel('Height Excess (%)')
ax2.set_ylabel('Count')

ax3.hist(df_analysis['front_yard_deficit'][df_analysis['front_yard_deficit'] > 0], bins=50)
ax3.set_title('Distribution of Front Yard Deficit (Non-compliant properties)')
ax3.set_xlabel('Front Yard Deficit (%)')
ax3.set_ylabel('Count')

plt.tight_layout()
plt.show()

# Calculate correlations between different types of non-compliance
non_compliance_corr = df_analysis[['FAR_excess', 'height_excess', 'front_yard_deficit']].corr()

print("\nCorrelations between different types of non-compliance:")
print(non_compliance_corr)

# Visualize correlations
plt.figure(figsize=(8, 6))
sns.heatmap(non_compliance_corr, annot=True, cmap='coolwarm')
plt.title('Correlations between Different Types of Non-compliance')
plt.show()

# Define small margin threshold
small_margin = 0.1  # 10% over the limit

# Separate the small margin non-compliance properties
small_far_excess = df_analysis['FAR_excess'][(df_analysis['FAR_excess'] > 0) & (df_analysis['FAR_excess'] <= small_margin)]
small_height_excess = df_analysis['height_excess'][(df_analysis['height_excess'] > 0) & (df_analysis['height_excess'] <= small_margin)]
small_front_yard_deficit = df_analysis['front_yard_deficit'][(df_analysis['front_yard_deficit'] > 0) & (df_analysis['front_yard_deficit'] <= small_margin)]

# Plot histograms for small margin non-compliance separately
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))

ax1.hist(small_far_excess, bins=20)
ax1.set_title('Distribution of FAR Excess (Small margin non-compliant properties)')
ax1.set_xlabel('FAR Excess (%)')
ax1.set_ylabel('Count')

ax2.hist(small_height_excess, bins=20)
ax2.set_title('Distribution of Height Excess (Small margin non-compliant properties)')
ax2.set_xlabel('Height Excess (%)')
ax2.set_ylabel('Count')

ax3.hist(small_front_yard_deficit, bins=20)
ax3.set_title('Distribution of Front Yard Deficit (Small margin non-compliant properties)')
ax3.set_xlabel('Front Yard Deficit (%)')
ax3.set_ylabel('Count')

plt.tight_layout()
plt.show()

# Calculate percentage of properties that are non-compliant by small margins
small_non_compliance = {
    'FAR': small_far_excess,
    'Height': small_height_excess,
    'Front Yard': small_front_yard_deficit
}

print("\nPercentage of properties non-compliant by small margins (<=10%):")
for feature, series in small_non_compliance.items():
    percentage = (series.count() / df_analysis.shape[0]) * 100
    print(f"{feature}: {percentage:.2f}%")

# Calculate percentage of properties that are non-compliant by larger margins
large_non_compliance = {
    'FAR': df_analysis['FAR_excess'] > small_margin,
    'Height': df_analysis['height_excess'] > small_margin,
    'Front Yard': df_analysis['front_yard_deficit'] > small_margin
}

print("\nPercentage of properties non-compliant by larger margins (>10%):")
for feature, mask in large_non_compliance.items():
    percentage = mask.mean() * 100
    print(f"{feature}: {percentage:.2f}%")

import pandas as pd

# Assuming df_final is your DataFrame and the relevant columns are present

# Step 1: Direct comparison for building height
df_final['building_height_equal'] = df_final['max_building_height_current'] == df_final['max_building_height_proposed']

# Step 2: View the results where they are not equal
height_differences = df_final[~df_final['building_height_equal']]

# Step 3: Proportion of equality
height_equality_proportion = df_final['building_height_equal'].mean()

# Display results
print(f"Proportion of rows where max_building_height_current equals max_building_height_proposed: {height_equality_proportion:.2%}")
print("Rows where the current and proposed building heights are different:")
print(height_differences[['max_building_height_current', 'max_building_height_proposed']])

# Identify the continuous columns from your list
continuous_columns = [
    'LotArea', 'LotFront', 'LotDepth', 'BldgFront', 'BldgDepth', 'NumFloors', 'height',
    'current_FAR', 'ResidFAR', 'min_front_yard_depth_current', 'max_building_height_current',
    'max_residential_far_proposed', 'min_front_yard_depth_proposed', 'max_building_height_proposed',
    'FacilFAR', 'BldgArea', 'ResArea', 'FrontYardDepth', 'RearYardDepth', 'LotCoverageRatio',
    'BuildingAspectRatio', 'BuildableArea_current', 'BuildableArea_proposed',
    'SetbackComplianceRatio_current', 'SetbackComplianceRatio_proposed',
    'FloorAreaPotential_current', 'FloorAreaPotential_proposed',
    'Centroid_x', 'Centroid_y', 'Area'
]

# Verify that all columns are present in df_final
continuous_columns = [col for col in continuous_columns if col in df_final.columns]

# Get indices of continuous features in df_final
continuous_indices = [df_final.columns.get_loc(col) for col in continuous_columns]
import numpy as np
import pandas as pd

# Preprocess the covariance matrix for the continuous features
def preprocess_covariance_matrix(X_scaled_df, continuous_indices):
    # Ensure X_scaled_df is a numpy array
    if isinstance(X_scaled_df, pd.DataFrame):
        X_scaled_df = X_scaled_df.values

    # Extract the relevant continuous features
    continuous_features_data = X_scaled_df[:, continuous_indices]

    # Debugging: Print shape of continuous_features_data
    print(f"Shape of continuous_features_data: {continuous_features_data.shape}")

    # Check if the continuous_features_data is 2D
    if continuous_features_data.ndim != 2:
        raise ValueError(f"Expected a 2D array, but got {continuous_features_data.ndim}D array instead.")

    # Calculate the Covariance Matrix for the continuous features
    cov_matrix = np.cov(continuous_features_data, rowvar=False)

    # Debugging: Print shape of covariance matrix
    print(f"Shape of covariance matrix: {cov_matrix.shape}")

    # Check if the Covariance Matrix is Singular
    det_cov_matrix = np.linalg.det(cov_matrix)
    if det_cov_matrix == 0:
        raise ValueError("Covariance matrix is singular, cannot invert.")
    else:
        print(f"Determinant of covariance matrix: {det_cov_matrix}")

    # Invert the Covariance Matrix
    inv_cov_matrix = np.linalg.inv(cov_matrix)

    # Check for Collinearity (Condition Number)
    cond_number = np.linalg.cond(cov_matrix)
    if cond_number > 1e10:
        raise ValueError("Covariance matrix is poorly conditioned (possible multicollinearity).")
    else:
        print(f"Condition number of covariance matrix: {cond_number}")

    return inv_cov_matrix

# Apply the preprocessing to the continuous columns
try:
    inv_cov_matrix = preprocess_covariance_matrix(df_final.values, continuous_indices)
    print("Covariance matrix inversion successful.")
except Exception as e:
    print(f"Error during covariance matrix preprocessing: {e}")

# Check if the covariance matrix is properly prepared
if 'inv_cov_matrix' in locals():
    print("Covariance matrix is successfully inverted and ready for use.")
else:
    print("Covariance matrix preprocessing failed.")

# Extract continuous features data correctly
continuous_features_data = df_final[continuous_columns].values

# 2. Check for constant columns (where variance is zero)
constant_columns = np.var(continuous_features_data, axis=0) == 0
if np.any(constant_columns):
    print(f"Constant columns detected: {np.array(continuous_columns)[constant_columns]}")

# Re-run the covariance matrix preprocessing function
try:
    inv_cov_matrix = preprocess_covariance_matrix(df_final.values, continuous_indices)
    print("Covariance matrix inversion successful.")
except Exception as e:
    print(f"Error during covariance matrix preprocessing: {e}")

# Check if the covariance matrix is properly prepared
if 'inv_cov_matrix' in locals():
    print("Covariance matrix is successfully inverted and ready for use.")
else:
    print("Covariance matrix preprocessing failed.")

# Step 1: Ensure the data for covariance matrix calculation is correct
print("First few rows of continuous_features_data:")
print(continuous_features_data[:5, :])  # Print the first 5 rows to inspect the data

# Step 2: Check for constant columns (where variance is zero)
constant_columns = np.var(continuous_features_data, axis=0) == 0
if np.any(constant_columns):
    print(f"Constant columns detected: {np.array(continuous_columns)[constant_columns]}")
else:
    print("No constant columns detected.")

# Step 3: Simplified covariance matrix calculation for debugging
try:
    # Try calculating the covariance matrix for a subset of columns
    subset_data = continuous_features_data[:, :5]  # Use only the first 5 continuous columns for testing
    print("Calculating covariance matrix for a subset of the data...")
    cov_matrix_subset = np.cov(subset_data, rowvar=False)
    print("Covariance matrix for the subset calculated successfully.")
    print("Covariance matrix subset:")
    print(cov_matrix_subset)

    # Attempt inversion on the subset covariance matrix
    inv_cov_matrix_subset = np.linalg.inv(cov_matrix_subset)
    print("Inversion of the subset covariance matrix successful.")
except Exception as e:
    print(f"Error during subset covariance matrix processing: {e}")

# Step 4: Now retry on the full dataset after ensuring the subset works
try:
    print("Attempting full covariance matrix calculation and inversion...")
    inv_cov_matrix = preprocess_covariance_matrix(df_final.values, continuous_indices)
    print("Covariance matrix inversion successful.")
except Exception as e:
    print(f"Error during full covariance matrix preprocessing: {e}")

# Step 5: Check if the full covariance matrix is properly prepared
if 'inv_cov_matrix' in locals():
    print("Covariance matrix is successfully inverted and ready for use.")
else:
    print("Covariance matrix preprocessing failed.")

# Re-import necessary libraries
import numpy as np

# Step 1: Debugging full covariance matrix calculation
try:
    # Ensure the correct data is being used
    print("Shape of continuous_features_data:", continuous_features_data.shape)

    # Calculate the full covariance matrix
    cov_matrix_full = np.cov(continuous_features_data, rowvar=False)
    print("Full covariance matrix calculated successfully.")
    print("Covariance matrix full:")
    print(cov_matrix_full)

    # Attempt inversion on the full covariance matrix
    inv_cov_matrix_full = np.linalg.inv(cov_matrix_full)
    print("Inversion of the full covariance matrix successful.")

except Exception as e:
    print(f"Error during full covariance matrix preprocessing: {e}")

# Step 2: Check if the full covariance matrix is properly prepared
if 'inv_cov_matrix_full' in locals():
    print("Full covariance matrix is successfully inverted and ready for use.")
else:
    print("Covariance matrix preprocessing failed.")

from google.colab import drive
drive.mount('/content/drive')

import pickle
import os

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to save the file in your Google Drive
save_path = '/content/drive/My Drive/important_variables.pkl'

# Initialize an empty dictionary to hold the variables
important_variables = {}
saved_variables = []

# Check if each variable is defined and add it to the dictionary if it exists
variables_to_check = [
    'df_final', 'continuous_columns', 'X_scaled_continuous', 'df_scaled', 'X_scaled_df',
    'initial_feature_weights', 'initial_metric_weights', 'normalization_ranges', 'inv_cov_matrix',
    'sample_tracker', 'metric_targets', 'params', 'merged_gdf_r1_r5', 'features_for_clustering',
    'W', '', '', 'max_iter', 'feature_importances'
]

for var_name in variables_to_check:
    if var_name in globals():
        important_variables[var_name] = globals()[var_name]
        saved_variables.append(var_name)

# Save the variables to your Google Drive only if the dictionary is not empty
if important_variables:
    with open(save_path, 'wb') as f:
        pickle.dump(important_variables, f)
    print("Important variables saved successfully to Google Drive.")
    print("The following variables were saved:")
    for var in saved_variables:
        print(f"- {var}")
else:
    print("No variables were found to save.")

import pickle
import os

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the pickle file in your Google Drive
load_path = '/content/drive/My Drive/important_variables.pkl'

# Load the variables from the pickle file
with open(load_path, 'rb') as f:
    important_variables = pickle.load(f)

# If you want to restore them into the current session, you can do so like this:
for var_name, var_value in important_variables.items():
    globals()[var_name] = var_value

print("Important variables loaded successfully.")
print("The following variables were restored:")
for var in important_variables:
    print(f"- {var}")

df_final.head()

import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.cluster import SpectralClustering
from scipy.stats import f_oneway
import warnings
import time
from concurrent.futures import ThreadPoolExecutor
import psutil
from scipy.spatial.distance import cdist
from sklearn.metrics.pairwise import pairwise_distances
import pandas as pd
import numpy_financial as npf
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.stats.power as smp
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
import skopt
from skopt.acquisition import gaussian_acquisition_1D
from skopt import Optimizer
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
from collections import deque
from sklearn.preprocessing import RobustScaler
from scipy.spatial import ConvexHull
from concurrent.futures import ProcessPoolExecutor, as_completed

warnings.filterwarnings('ignore')

def sample_parameters(num_samples):
    return {
        'market_value_per_sqft': np.random.uniform(360, 527, num_samples),
        'construction_cost_per_sqft': np.random.uniform(150, 400, num_samples),
        'land_value_per_sqft': np.random.uniform(35, 55, num_samples),
        'demolition_cost_per_sqft': np.random.uniform(5, 60, num_samples),
        'holding_period_years': np.random.randint(3, 11, num_samples),
        'discount_rate': np.random.uniform(0.07, 0.12, num_samples),
        'appreciation_rate': np.random.uniform(-0.147, 0.258, num_samples),
        'debt_ratio': np.random.uniform(0.65, 0.90, num_samples),
        'interest_rate': np.random.uniform(0.0503, 0.0672, num_samples),
        'floor_height': np.random.uniform(10, 13, num_samples),
        'cap_rate': np.random.uniform(0.0349, 0.08, num_samples)
    }

# Custom Distance Function for Spectral Clustering
def custom_distance_function(X, Y=None, continuous_indices=None, spatial_indices=None, categorical_indices=None, binary_indices=None, cov_matrix=None, weights=None, distance_ranges=None):
    Y = X if Y is None else Y
    distances = np.zeros((X.shape[0], Y.shape[0]))

    # Initialize distance matrices for each feature type
    cont_distances = np.zeros((X.shape[0], Y.shape[0])) if continuous_indices else None
    spatial_distances = np.zeros((X.shape[0], Y.shape[0])) if spatial_indices else None
    cat_distances = np.zeros((X.shape[0], Y.shape[0])) if categorical_indices else None
    binary_distances = np.zeros((X.shape[0], Y.shape[0])) if binary_indices else None

    # Calculate distances for continuous features
    if continuous_indices is not None:
        X_cont = X[:, continuous_indices]
        Y_cont = Y[:, continuous_indices]
        if cov_matrix is not None:
            cont_distances = cdist(X_cont, Y_cont, metric='mahalanobis', VI=cov_matrix)
        else:
            cont_distances = cdist(X_cont, Y_cont, metric='euclidean')

    # Calculate distances for spatial features
    if spatial_indices is not None:
        X_spatial = X[:, spatial_indices]
        Y_spatial = Y[:, spatial_indices]
        spatial_distances = cdist(X_spatial, Y_spatial, metric='euclidean')

    # Calculate distances for categorical features
    if categorical_indices is not None:
        X_cat = X[:, categorical_indices]
        Y_cat = Y[:, categorical_indices]
        cat_distances = cdist(X_cat, Y_cat, metric='hamming')

    # Calculate distances for binary features
    if binary_indices is not None:
        X_bin = X[:, binary_indices]
        Y_bin = Y[:, binary_indices]
        binary_distances = cdist(X_bin, Y_bin, metric='hamming')

    # Normalize distances
    cont_norm, spatial_norm, cat_norm, binary_norm = normalize_distances(
        cont_distances, spatial_distances, cat_distances, binary_distances, distance_ranges
    )

    # Update the distance normalization ranges
    updated_distance_ranges = update_distance_normalization_ranges(
        distance_ranges, [cont_norm, spatial_norm, cat_norm, binary_norm]
    )

    # Combine normalized distances with weights
    distances = (
        weights['continuous'] * cont_norm +
        weights['spatial'] * spatial_norm +
        weights['categorical'] * cat_norm +
        weights['binary'] * binary_norm
    )

    return distances, updated_distance_ranges

# Function to evaluate cluster metrics
def evaluate_cluster_metrics(X_scaled_df, labels):
    silhouette_avg = silhouette_score(X_scaled_df, labels)
    davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
    calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
    return silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg

# Memory usage function
def print_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")

# Function to dynamically calculate normalization ranges
def dynamic_normalization_ranges(X_scaled_df, k_values, metric_targets):
    print(f"X_scaled_df shape: {X_scaled_df.shape}")
    print_memory_usage()

    silhouette_values = []
    davies_bouldin_values = []
    calinski_harabasz_values = []
    p_value_values = []
    times = []

    for k in k_values:
        print(f"Starting clustering for k={k}")
        start_time = time.time()

        # Custom distance matrix using the defined distance function
        distance_matrix = custom_distance_function(X_scaled_df.values)

        # Spectral Clustering with custom similarity (negative distance)
        spectral_clustering = SpectralClustering(n_clusters=k, affinity='precomputed', random_state=42)
        labels = spectral_clustering.fit_predict(np.exp(-distance_matrix))

        silhouette_avg = silhouette_score(X_scaled_df, labels)
        davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
        calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)

        combined_results = merged_gdf_r1_r5.copy()
        combined_results['cluster'] = labels
        npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
        p_value = f_oneway(*npv_clusters).pvalue

        silhouette_values.append(silhouette_avg)
        davies_bouldin_values.append(davies_bouldin_avg)
        calinski_harabasz_values.append(calinski_harabasz_avg)
        p_value_values.append(p_value)

        end_time = time.time()
        times.append(end_time - start_time)

        print(f"Metrics for k = {k}")
        print(f"Silhouette: {silhouette_avg}")
        print(f"Davies-Bouldin: {davies_bouldin_avg}")
        print(f"Calinski-Harabasz: {calinski_harabasz_avg}")
        print(f"p-value: {p_value}")
        print(f"Time taken: {end_time - start_time} seconds")
        print_memory_usage()
        print()

    # Final range calculation
    silhouette_range = (min(silhouette_values), max(silhouette_values))
    davies_bouldin_range = (min(davies_bouldin_values), max(davies_bouldin_values))
    calinski_harabasz_range = (min(calinski_harabasz_values), max(calinski_harabasz_values))
    p_value_range = (min(p_value_values), max(p_value_values))

    print("Normalization Ranges:")
    print(f"Silhouette: {silhouette_range}")
    print(f"Davies-Bouldin: {davies_bouldin_range}")
    print(f"Calinski-Harabasz: {calinski_harabasz_range}")
    print(f"p-value: {p_value_range}")
    print_memory_usage()

    return silhouette_range, davies_bouldin_range, calinski_harabasz_range, p_value_range

class Evaluation:
    def __init__(self, k, metrics, metric_weights, financial_weights, lambda_value, clustering):
        self.k = k
        self.metrics = metrics
        self.metric_weights = metric_weights
        self.financial_weights = financial_weights
        self.lambda_value = lambda_value
        self.clustering = clustering
        self.score = None

previous_evaluations = []

# Function to calculate the weighted score with normalized metrics
def calculate_weighted_score(metrics, weights, weights_previous, , normalization_ranges):
    silhouette, davies_bouldin, calinski_harabasz, p_value = metrics
    silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, p_value_norm = normalize_metrics(
        silhouette, davies_bouldin, calinski_harabasz, p_value, normalization_ranges
    )

    regularization_term =  * np.linalg.norm(np.array(list(weights.values())) - np.array(list(weights_previous.values())))

    weighted_score = (
        weights['silhouette'] * silhouette_norm +
        weights['davies_bouldin'] * davies_bouldin_norm +
        weights['calinski_harabasz'] * calinski_harabasz_norm +
        weights['p_value'] * p_value_norm -
        regularization_term
    )

    contributions = {
        'silhouette': weights['silhouette'] * silhouette_norm,
        'davies_bouldin': weights['davies_bouldin'] * davies_bouldin_norm,
        'calinski_harabasz': weights['calinski_harabasz'] * calinski_harabasz_norm,
        'p_value': weights['p_value'] * p_value_norm,
        'regularization_term': -regularization_term
    }

    return weighted_score, contributions, (silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, p_value_norm)

# Function to compute the eigenvalue gap
def compute_eigenvalue_gap(affinity_matrix, k):
    # Compute eigenvalues of the affinity matrix
    eigenvalues = np.linalg.eigvalsh(affinity_matrix)

    # Sort eigenvalues in ascending order
    eigenvalues = np.sort(eigenvalues)

    # Compute the gap between the k-th and (k-1)-th eigenvalues
    eigenvalue_gap = eigenvalues[-k] - eigenvalues[-(k+1)]

    return eigenvalue_gap

# Function to calculate sample size using power analysis
def calculate_sample_size_power(effect_size, alpha=0.05, power=0.80, alternative='two-sided'):
    sample_size = smp.tt_ind_solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative=alternative)
    return int(np.ceil(sample_size))

# Function to dynamically adjust sample size based on weighted CVs and estimated needs
def determine_cluster_sample_sizes(clustered_data, feature_importances, cluster_column='cluster', alpha=0.05, power=0.80, margin_of_error=0.05, max_sample_size=None):
    cluster_sample_sizes = {}
    cluster_weighted_cvs = determine_cluster_weighted_cvs(clustered_data, feature_importances, cluster_column)

    for cluster_id, weighted_cv in cluster_weighted_cvs.items():
        effect_size = weighted_cv
        initial_sample_size = calculate_sample_size_power(effect_size, alpha=alpha, power=power)

        cluster_data = clustered_data[clustered_data[cluster_column] == cluster_id]
        final_sample_size = min(initial_sample_size, len(cluster_data))

        if max_sample_size:
            final_sample_size = min(final_sample_size, max_sample_size)

        cluster_sample_sizes[cluster_id] = final_sample_size

    return cluster_sample_sizes

# Function to normalize weights for feature types
def normalize_weights(weights):
    total_weight = sum(weights.values())
    if total_weight == 0:
        return {key: 0 for key in weights}
    return {key: value / total_weight for key, value in weights.items()}

def normalize_metrics(silhouette, davies_bouldin, calinski_harabasz, eigenvalue_gap, p_value, ranges):
    silhouette_range, davies_bouldin_range, calinski_harabasz_range, eigenvalue_gap_range, p_value_range = ranges

    silhouette_norm = (silhouette - silhouette_range[0]) / (silhouette_range[1] - silhouette_range[0]) if silhouette_range[1] != silhouette_range[0] else 0
    davies_bouldin_norm = (davies_bouldin - davies_bouldin_range[0]) / (davies_bouldin_range[1] - davies_bouldin_range[0]) if davies_bouldin_range[1] != davies_bouldin_range[0] else 0
    calinski_harabasz_norm = (calinski_harabasz - calinski_harabasz_range[0]) / (calinski_harabasz_range[1] - calinski_harabasz_range[0]) if calinski_harabasz_range[1] != calinski_harabasz_range[0] else 0
    eigenvalue_gap_norm = (eigenvalue_gap - eigenvalue_gap_range[0]) / (eigenvalue_gap_range[1] - eigenvalue_gap_range[0]) if eigenvalue_gap_range[1] != eigenvalue_gap_range[0] else 0
    p_value_norm = (p_value - p_value_range[0]) / (p_value_range[1] - p_value_range[0]) if p_value_range[1] != p_value_range[0] else 0

    # Safeguard against NaN values
    silhouette_norm = 0 if np.isnan(silhouette_norm) else silhouette_norm
    davies_bouldin_norm = 0 if np.isnan(davies_bouldin_norm) else davies_bouldin_norm
    calinski_harabasz_norm = 0 if np.isnan(calinski_harabasz_norm) else calinski_harabasz_norm
    eigenvalue_gap_norm = 0 if np.isnan(eigenvalue_gap_norm) else eigenvalue_gap_norm
    p_value_norm = 0 if np.isnan(p_value_norm) else p_value_norm

    return silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, eigenvalue_gap_norm, p_value_norm

def normalize_financial_metrics(npv, irr, em, ranges):
    npv_range, irr_range, em_range = ranges

    npv_norm = (npv - npv_range[0]) / (npv_range[1] - npv_range[0]) if npv_range[1] != npv_range[0] else 0
    irr_norm = (irr - irr_range[0]) / (irr_range[1] - irr_range[0]) if irr_range[1] != irr_range[0] else 0
    em_norm = (em - em_range[0]) / (em_range[1] - em_range[0]) if em_range[1] != em_range[0] else 0

    # Safeguard against NaN values
    npv_norm = 0 if np.isnan(npv_norm) else npv_norm
    irr_norm = 0 if np.isnan(irr_norm) else irr_norm
    em_norm = 0 if np.isnan(em_norm) else em_norm

    return npv_norm, irr_norm, em_norm

# Function to normalize distances dynamically
def normalize_distances(cont_distances, spatial_distances, cat_distances, binary_distances, ranges):
    cont_range, spatial_range, cat_range, binary_range = ranges

    cont_norm = (cont_distances - cont_range[0]) / (cont_range[1] - cont_range[0]) if cont_range[1] != cont_range[0] else np.zeros_like(cont_distances)
    spatial_norm = (spatial_distances - spatial_range[0]) / (spatial_range[1] - spatial_range[0]) if spatial_range[1] != spatial_range[0] else np.zeros_like(spatial_distances)
    cat_norm = (cat_distances - cat_range[0]) / (cat_range[1] - cat_range[0]) if cat_range[1] != cat_range[0] else np.zeros_like(cat_distances)
    binary_norm = (binary_distances - binary_range[0]) / (binary_range[1] - binary_range[0]) if binary_range[1] != binary_range[0] else np.zeros_like(binary_distances)

    return cont_norm, spatial_norm, cat_norm, binary_norm

# Function to update normalization ranges dynamically
def update_normalization_ranges(current_ranges, new_values):
    updated_ranges = []
    for (current_min, current_max), new_value in zip(current_ranges, new_values):
        updated_min = min(current_min, np.min(new_value))
        updated_max = max(current_max, np.max(new_value))
        updated_ranges.append((updated_min, updated_max))
    return updated_ranges

def calculate_proposed_floor_area(row, params):
    """
    Calculate the proposed floor area based on the zoning parameters and other relevant features.

    Arguments:
    row -- A row from the dataframe containing the features needed for calculation.
    params -- A dictionary containing any additional parameters needed for the calculation.

    Returns:
    proposed_floor_area -- The calculated proposed floor area.
    """

    # Example calculation (to be customized based on your specific zoning rules):
    max_residential_far_proposed = row['max_residential_far_zoning']
    lot_area = row['LotArea']

    # Proposed floor area is often calculated as max FAR * lot area
    proposed_floor_area = max_residential_far_proposed * lot_area

    return proposed_floor_area

def generate_cash_flows(row, params):
    # Calculate initial values
    current_value = row['BldgArea'] * float(params['market_value_per_sqft'])
    existing_revenue = current_value * params['cap_rate']
    holding_period_years = int(params['holding_period_years'])
    appreciation_rate = float(params['appreciation_rate'])

    # Redevelopment Strategy (Strategy 1)
    proposed_floor_area = calculate_proposed_floor_area(row, params)
    construction_cost = proposed_floor_area * float(params['construction_cost_per_sqft'])
    land_value = row['LotArea'] * float(params['land_value_per_sqft'])
    demolition_cost = row['BldgArea'] * float(params['demolition_cost_per_sqft'])
    total_development_cost = land_value + demolition_cost + construction_cost

    # Equity investment is the portion of the development cost not covered by debt
    equity_investment = total_development_cost * (1 - float(params['debt_ratio']))
    debt = total_development_cost * float(params['debt_ratio'])

    # Annual Net Operating Income (NOI)
    annual_noi = proposed_floor_area * params['market_value_per_sqft'] * params['cap_rate']

    # Annual Debt Service
    annual_debt_service = debt * (float(params['interest_rate']) * (1 + float(params['interest_rate']))**holding_period_years) / ((1 + float(params['interest_rate']))**holding_period_years - 1)

    # Sale price at the end of the holding period
    sale_price = proposed_floor_area * float(params['market_value_per_sqft']) * (1 + appreciation_rate)**holding_period_years

    # Calculate cash flows
    cash_flows = [
        -equity_investment,  # Initial investment (negative cash flow)
        *([annual_noi - annual_debt_service] * (holding_period_years - 1)),  # Annual cash flow during the holding period
        annual_noi - annual_debt_service + sale_price - debt  # Final year cash flow including sale proceeds
    ]

    # Calculate total contributions and total distributions
    total_contributions = equity_investment  # Initial equity investment
    total_distributions = sum(max(0, cf) for cf in cash_flows[1:])  # Sum of all positive cash flows (distributions)

    # Return a dictionary with the required values
    return {
        'cash_flows': ','.join(map(str, cash_flows)),  # Store as comma-separated string
        'total_contributions': total_contributions,
        'total_distributions': total_distributions
    }

def calculate_npv(discount_rate, cash_flows):
    # Convert discount_rate to float to ensure it's numeric
    discount_rate = float(discount_rate)
    # Ensure cash_flows is a numpy array of floats
    cash_flows = np.array(cash_flows, dtype=float)
    # Calculate NPV using numpy_financial's npv function
    return npf.npv(discount_rate, cash_flows)

def calculate_irr(cash_flows):
    # Calculate IRR using numpy_financial's irr function
    return npf.irr(cash_flows)

def calculate_em(total_contributions, total_distributions):
    # Calculate Equity Multiple based on total contributions and distributions
    return total_distributions / total_contributions

# Updated Monte Carlo simulation function with early stopping and progress monitoring
def monte_carlo_simulation(cluster_data, sample_size, num_simulations=100, tol=0.01, min_simulations=30, params=None):
    npv_means = []
    irr_means = []
    em_means = []
    available_sample_size = min(sample_size, len(cluster_data))
    prev_means = [0, 0, 0]  # Track previous means for npv, irr, em

    # Initialize columns to hold list data
    cluster_data['cash_flows'] = [[] for _ in range(len(cluster_data))]
    cluster_data['total_contributions'] = np.nan
    cluster_data['total_distributions'] = np.nan

    with tqdm(total=num_simulations, desc="Monte Carlo simulation for cluster") as pbar:
        for i in range(num_simulations):
            sampled_params = sample_parameters(available_sample_size)

            # Generate cash flows for each row in the cluster
            for j, row in cluster_data.iterrows():
                cash_flows_dict = generate_cash_flows(row, {k: v[j] for k, v in sampled_params.items()})
                cluster_data.at[j, 'cash_flows'] = cash_flows_dict['cash_flows'].tolist()
                cluster_data.at[j, 'total_contributions'] = cash_flows_dict['total_contributions']
                cluster_data.at[j, 'total_distributions'] = cash_flows_dict['total_distributions']

            npv_values = [
                calculate_npv(float(sampled_params['discount_rate'][j]), row['cash_flows'])
                for j, row in cluster_data.iterrows()
            ]
            irr_values = [calculate_irr(row['cash_flows']) for _, row in cluster_data.iterrows()]
            em_values = [calculate_em(row['total_contributions'], row['total_distributions']) for _, row in cluster_data.iterrows()]

            npv_means.append(np.mean(npv_values))
            irr_means.append(np.mean(irr_values))
            em_means.append(np.mean(em_values))

            current_means = [np.mean(npv_means), np.mean(irr_means), np.mean(em_means)]
            current_stds = [np.std(npv_means), np.std(irr_means), np.std(em_means)]

            # Early stopping based on tolerance
            if i >= min_simulations and all(abs(current_means[j] - prev_means[j]) / current_means[j] < tol for j in range(3)):
                pbar.update(num_simulations - i)
                print(f"Early stopping at simulation {i + 1}")
                break

            prev_means = current_means
            pbar.update(1)

    estimated_means = [np.mean(npv_means), np.mean(irr_means), np.mean(em_means)]
    estimated_stds = [np.std(npv_means), np.std(irr_means), np.std(em_means)]

    return estimated_means, estimated_stds

def bootstrap_sample(cluster_data, num_samples):
    npv_means = []
    irr_means = []
    em_means = []

    for _ in tqdm(range(num_samples), desc="Bootstrap sampling"):
        sampled_params = sample_parameters(len(cluster_data))

        # Generate cash flows and store them in the DataFrame
        for j, row in cluster_data.iterrows():
            cash_flows_dict = generate_cash_flows(row, {k: v[j] for k, v in sampled_params.items()})
            cluster_data.at[j, 'cash_flows'] = cash_flows_dict['cash_flows'].tolist()  # Store as list
            cluster_data.at[j, 'total_contributions'] = cash_flows_dict['total_contributions']
            cluster_data.at[j, 'total_distributions'] = cash_flows_dict['total_distributions']

        npv_values = [
            calculate_npv(float(sampled_params['discount_rate'][j]), row['cash_flows'])
            for j, row in cluster_data.iterrows()
        ]
        irr_values = [calculate_irr(row['cash_flows']) for _, row in cluster_data.iterrows()]
        em_values = [calculate_em(row['total_contributions'], row['total_distributions']) for _, row in cluster_data.iterrows()]

        npv_means.append(np.mean(npv_values))
        irr_means.append(np.mean(irr_values))
        em_means.append(np.mean(em_values))

    estimated_means = [np.mean(npv_means), np.mean(irr_means), np.mean(em_means)]
    estimated_stds = [np.std(npv_means), np.std(irr_means), np.std(em_means)]

    return estimated_means, estimated_stds

def perform_power_analysis(X_scaled_df, num_samples, effect_size=0.5, alpha=0.05, power=0.8):
    """
    Perform power analysis to determine the appropriate sample size.

    Parameters:
    X_scaled_df (pd.DataFrame): The scaled dataframe
    num_samples (int): The initial number of samples
    effect_size (float): The effect size to detect (default: 0.5, medium effect)
    alpha (float): The significance level (default: 0.05)
    power (float): The desired statistical power (default: 0.8)

    Returns:
    dict: A dictionary containing the results of the power analysis
    """
    from statsmodels.stats.power import TTestIndPower

    # Perform power analysis
    analysis = TTestIndPower()
    result = analysis.solve_power(effect_size=effect_size, power=power, nobs1=None, ratio=1.0, alpha=alpha)

    # Calculate the required sample size
    required_sample_size = int(np.ceil(result))

    # Determine if the current sample size is sufficient
    is_sufficient = num_samples >= required_sample_size

    return {
        'sample_size': required_sample_size,
        'is_sufficient': is_sufficient,
        'effect_size': effect_size,
        'alpha': alpha,
        'power': power
    }

# Set the margin of error as a constant value, e.g., 0.05
margin_of_error = 0.05

# Updated evaluate_spectral_clustering_with_sampling function
def evaluate_spectral_clustering_with_sampling(X_scaled_df, k, metric_weights, , previous_weights, num_samples, sample_tracker, metric_targets, distance_matrix=None, cov_matrix=None):
    # Step 1: Perform Spectral Clustering using RBF affinity
    spectral_clustering = SpectralClustering(n_clusters=k, affinity='rbf', random_state=42)
    labels = spectral_clustering.fit_predict(X_scaled_df)

    # Debug print: Check cluster labels distribution
    print(f"Iteration {iteration_count}, k={k}: Labels distribution: {np.unique(labels, return_counts=True)}")

    # Step 2: Calculate clustering metrics
    silhouette_avg = silhouette_score(X_scaled_df, labels)
    davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
    calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)
    eigenvalue_gap = compute_eigenvalue_gap(spectral_clustering.affinity_matrix_, k)

    # Debug print: Check calculated metrics
    print(f"Iteration {iteration_count}, k={k}: Silhouette: {silhouette_avg}, Davies-Bouldin: {davies_bouldin_avg}, Calinski-Harabasz: {calinski_harabasz_avg}, Eigenvalue Gap: {eigenvalue_gap}")

    # Step 3: Refine the sample size using power analysis
    if k not in sample_tracker:
        # Perform power analysis (hypothetical function, adjust as needed)
        power_analysis_results = perform_power_analysis(X_scaled_df, num_samples)

        # Use the results from power analysis to determine refined_sample_size
        refined_sample_size = max(power_analysis_results['sample_size'], 1)
        sample_tracker[k] = {'sample_size': refined_sample_size}
    else:
        refined_sample_size = sample_tracker[k]['sample_size']

    # Step 4: Generate new sampled_params and calculate financial metrics within each cluster
    combined_results = X_scaled_df.copy()
    sampled_params = sample_parameters(len(combined_results))  # New call to sample_parameters
    combined_results['cluster'] = labels
    combined_results['discount_rate'] = sampled_params['discount_rate']  # Add discount_rate to combined_results

    for j, row in combined_results.iterrows():
        row_params = {key: sampled_params[key][j] for key in sampled_params}
        cash_flows_dict = generate_cash_flows(row, row_params)
        combined_results.at[j, 'cash_flows'] = cash_flows_dict['cash_flows']
        combined_results.at[j, 'total_contributions'] = cash_flows_dict['total_contributions']
        combined_results.at[j, 'total_distributions'] = cash_flows_dict['total_distributions']

    # Calculate NPV, IRR, and EM
    combined_results['npv'] = combined_results.apply(lambda row: calculate_npv(row['discount_rate'], [float(cf) for cf in row['cash_flows'].split(',')]), axis=1)
    combined_results['irr'] = combined_results.apply(lambda row: calculate_irr([float(cf) for cf in row['cash_flows'].split(',')]), axis=1)
    combined_results['em'] = combined_results.apply(lambda row: calculate_em(row['total_contributions'], row['total_distributions']), axis=1)

    # Step 5: Perform ANOVA Test
    npv_clusters = [group['npv'].values for name, group in combined_results.groupby('cluster')]
    anova_result_npv = f_oneway(*npv_clusters)
    p_value = anova_result_npv.pvalue

    # Early stopping check based on metric targets
    early_stop_flags = {
        'silhouette': silhouette_avg >= metric_targets['silhouette'],
        'davies_bouldin': davies_bouldin_avg <= metric_targets['davies_bouldin'],
        'calinski_harabasz': calinski_harabasz_avg >= metric_targets['calinski_harabasz'],
        'p_value': p_value <= metric_targets['p_value'],
        'eigenvalue_gap': eigenvalue_gap >= metric_targets.get('eigenvalue_gap', float('-inf'))
    }

    # Return clustering metrics and financial metrics
    return silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value, combined_results[['npv', 'irr', 'em']].mean(), labels

def update_normalization_ranges_and_renormalize_scores(current_ranges, new_values, previous_scores, range_type):
    updated_ranges = []
    for (current_min, current_max), new_value in zip(current_ranges, new_values):
        updated_min = min(current_min, np.min(new_value))
        updated_max = max(current_max, np.max(new_value))
        updated_ranges.append((updated_min, updated_max))

    # Re-normalize all previous scores based on updated ranges
    for i in range(len(previous_scores)):
        previous_scores[i]['total_score'] = renormalize(previous_scores[i], updated_ranges, range_type)

    return updated_ranges, previous_scores

def renormalize(score, updated_ranges, score_type):
    if score_type == 'metrics':
        silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, eigenvalue_gap_norm, p_value_norm = normalize_metrics(
            score.get('silhouette', 0),
            score.get('davies_bouldin', 0),
            score.get('calinski_harabasz', 0),
            score.get('eigenvalue_gap', 0),
            score.get('p_value', 0),
            updated_ranges
        )
        new_weighted_score = (
            score['metric_weights'].get('silhouette', 0) * silhouette_norm +
            score['metric_weights'].get('davies_bouldin', 0) * davies_bouldin_norm +
            score['metric_weights'].get('calinski_harabasz', 0) * calinski_harabasz_norm +
            score['metric_weights'].get('eigenvalue_gap', 0) * eigenvalue_gap_norm +
            score['metric_weights'].get('p_value', 0) * p_value_norm
        )
        return new_weighted_score / sum(score['metric_weights'].values())

    elif score_type == 'financial':
        npv_norm, irr_norm, em_norm = normalize_financial_metrics(
            score.get('npv', 0),
            score.get('irr', 0),
            score.get('em', 0),
            updated_ranges
        )
        new_financial_score = (
            score['financial_weights'].get('npv', 0) * npv_norm +
            score['financial_weights'].get('irr', 0) * irr_norm +
            score['financial_weights'].get('em', 0) * em_norm
        )
        return new_financial_score / sum(score['financial_weights'].values())

    elif score_type == 'weighted_score':
        if 'weighted_score' in score:
            weighted_score_norm = (score['weighted_score'] - updated_ranges[0][0]) / (updated_ranges[0][1] - updated_ranges[0][0])
            return weighted_score_norm

    elif score_type == 'financial_score':
        if 'financial_score' in score:
            financial_score_norm = (score['financial_score'] - updated_ranges[0][0]) / (updated_ranges[0][1] - updated_ranges[0][0])
            return financial_score_norm

    elif score_type == 'pre_regularization_total_score':
        if 'pre_regularization_total_score' in score:
            pre_reg_total_score_norm = (score['pre_regularization_total_score'] - updated_ranges[0][0]) / (updated_ranges[0][1] - updated_ranges[0][0])
            return pre_reg_total_score_norm

    elif score_type == 'total_score':
        if 'total_score' in score:
            total_score_norm = (score['total_score'] - updated_ranges[0][0]) / (updated_ranges[0][1] - updated_ranges[0][0])
            return total_score_norm

    # Return 0 if the score type does not exist in the score dictionary
    return 0

# Apply cooling factor to the search space
def apply_cooling(search_space, cooling_factor):
    for i, param in enumerate(search_space[1:], start=1):  # Skip the first parameter (e.g., 'k')
        if isinstance(param, skopt.space.space.Real):
            # Adjust bounds of Real parameters
            new_low = param.low * cooling_factor
            new_high = param.high * cooling_factor
            search_space[i] = skopt.space.Real(new_low, new_high)
        elif isinstance(param, skopt.space.space.Integer):
            # Adjust bounds of Integer parameters
            new_low = int(param.low * cooling_factor)
            new_high = int(param.high * cooling_factor)
            search_space[i] = skopt.space.Integer(new_low, new_high)
        elif isinstance(param, skopt.space.space.Categorical):
            # For categorical, we don't apply cooling; skip or handle appropriately
            pass
        else:
            raise TypeError(f"Unsupported parameter type: {type(param)}")

    return search_space

# Updated refine_weights_spectral function with financial metric weights in range (-1, 1)
def refine_weights_spectral(X_scaled_df, initial_feature_weights, initial_metric_weights, , , max_iter, metric_targets, normalization_ranges=None, sample_tracker=None, feature_importances=None, cov_matrix=None, continuous_indices=None, spatial_indices=None, categorical_indices=None, binary_indices=None):
    global best_score, best_clustering, best_k, iteration_count, best_weights, previous_scores
    best_evaluation = None
    weights_previous = initial_feature_weights.copy()
    peak_score = float('-inf')
    sample_tracker = {}
    improvement_threshold = 0.001  # Reduce threshold for stopping optimization
    cooling_factor = 0.95  # Cooling factor for gradually reducing the exploration space

    # Track scores over the last n iterations for smoothing
    n_iterations = 10
    score_history = deque(maxlen=n_iterations)
    previous_scores = deque(maxlen=n_iterations)
    previous_weights = deque(maxlen=10)  # Track the last 10 sets of weights

    # Initialize global variables
    best_score = float('-inf')
    best_clustering = None
    best_k = None
    best_weights = None
    iteration_count = 0

    # Multi-start optimization loop
    best_overall_score = float('-inf')
    best_overall_params = None

    # Initialize or pass in normalization ranges
    if normalization_ranges is None:
        normalization_ranges = {
            'metrics': [(float('inf'), float('-inf'))] * 5,  # For silhouette, davies_bouldin, calinski_harabasz, p_value
            'financial': [(float('inf'), float('-inf'))] * 3#,  # For npv, irr, em
            #'distances': [(float('inf'), float('-inf'))] * 4  # For cont, spatial, cat, binary distances
        }

    # Define the search space for Bayesian Optimization
    initial_search_space = [
        Integer(25, 150, name='k'),  # Adjusted number of clusters
        #Real(0.0, 1.0, name='continuous_weight'),
        #Real(0.0, 1.0, name='spatial_weight'),
        #Real(0.0, 1.0, name='categorical_weight'),
        #Real(0.0, 1.0, name='binary_weight'),
        Real(-1.0, 1.0, name='silhouette_weight'),
        Real(-1.0, 1.0, name='davies_bouldin_weight'),
        Real(-1.0, 1.0, name='calinski_harabasz_weight'),
        Real(-1.0, 1.0, name='eigenvalue_gap_weight'),
        Real(-1.0, 1.0, name='p_value_weight'),
        Real(-1.0, 1.0, name='npv_weight'),  # Weight for NPV in the objective function
        Real(-1.0, 1.0, name='irr_weight'),  # Weight for IRR in the objective function
        Real(-1.0, 1.0, name='em_weight'),   # Weight for Equity Multiple in the objective function
        Real(0.0, 1.0, name=''),  # Regularization Term
        Real(0.01, 0.05, name=''),  # Exploration vs. Exploitation (EI)
        Integer(2000, 30000, name='sample_size'),  # Adjusted Total Sample Size
        Real(0.0, 0.05, name='weight_decay')  # Weight Decay
    ]

    # Copy the initial search space for dynamic updates
    search_space = initial_search_space.copy()

    # Initialize the Bayesian Optimizer
    optimizer = Optimizer(search_space, base_estimator="gp", acq_func="EI", acq_optimizer="sampling", n_initial_points=15)

    @use_named_args(search_space)
    def objective(**params):
        print("Params received by objective function:", params)  # Debugging line
        global best_score, best_clustering, best_k, iteration_count, best_weights, previous_scores
        iteration_count += 1

        k = params['k']
        # feature_weights = {
        #     'continuous': params['continuous_weight'],
        #     'spatial': params['spatial_weight'],
        #     'categorical': params['categorical_weight'],
        #     'binary': params['binary_weight']
        # }
        #feature_weights = normalize_weights(feature_weights)

        metric_weights = {
            'silhouette': params['silhouette_weight'],
            'davies_bouldin': params['davies_bouldin_weight'],
            'calinski_harabasz': params['calinski_harabasz_weight'],
            'eigenvalue_gap': params['eigenvalue_gap_weight'],
            'p_value': params['p_value_weight']
        }

        # Financial weights potentially in (-1, 1) range
        financial_weights = {
            'npv': params['npv_weight'],
            'irr': params['irr_weight'],
            'em': params['em_weight']
        }

        # Update  based on current parameters
         = params['']
        # Update total sample size
        total_sample_size = params['sample_size']

        # Recompute distance matrix for current feature weights
        # distance_matrix, updated_distance_ranges = custom_distance_function(
        #     X_scaled_df, weights=initial_feature_weights, cov_matrix=cov_matrix, distance_ranges=normalization_ranges['distances'], continuous_indices=continuous_indices, spatial_indices=spatial_indices, categorical_indices=categorical_indices, binary_indices=binary_indices
        # )

        # Update the distance normalization ranges with the new values
        #normalization_ranges['distances'] = updated_distance_ranges

        # Normalize distance metrics
        # cont_norm, spatial_norm, cat_norm, binary_norm = normalize_distances(
        #     distance_matrix['continuous'], distance_matrix['spatial'], distance_matrix['categorical'], distance_matrix['binary'], normalization_ranges['distances']
        # )

        # Perform clustering and calculate weighted score
        # weighted_score, silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value, early_stop_flags, financial_metrics = evaluate_spectral_clustering_with_sampling(
        #     X_scaled_df, k, metric_weights, , weights_previous, num_samples=total_sample_size, sample_tracker=sample_tracker, metric_targets=metric_targets, distance_matrix=distance_matrix
        # )
        # Perform clustering and calculate metrics
        silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value, financial_metrics, labels = evaluate_spectral_clustering_with_sampling(
            X_scaled_df, k, metric_weights, , None, num_samples=total_sample_size, sample_tracker=sample_tracker, metric_targets=metric_targets
        )

        # Update normalization ranges and re-normalize past scores
        normalization_ranges['metrics'], previous_scores = update_normalization_ranges_and_renormalize_scores(
            normalization_ranges['metrics'],
            [silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value],
            previous_scores,
            'metrics'
        )
        normalization_ranges['financial'], previous_scores = update_normalization_ranges_and_renormalize_scores(
            normalization_ranges['financial'],
            [financial_metrics['npv'], financial_metrics['irr'], financial_metrics['em']],
            previous_scores,
            'financial'
        )

        # Normalize the current metrics
        silhouette_norm, davies_bouldin_norm, calinski_harabasz_norm, eigenvalue_gap_norm, p_value_norm = normalize_metrics(
            silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value, normalization_ranges['metrics']
        )
        npv_norm, irr_norm, em_norm = normalize_financial_metrics(
            financial_metrics['npv'], financial_metrics['irr'], financial_metrics['em'], normalization_ranges['financial']
        )

        # Combine normalized metrics into weighted and financial scores
        weighted_score = (
            metric_weights['silhouette'] * silhouette_norm +
            metric_weights['davies_bouldin'] * davies_bouldin_norm +
            metric_weights['calinski_harabasz'] * calinski_harabasz_norm +
            metric_weights['eigenvalue_gap'] * eigenvalue_gap_norm +
            metric_weights['p_value'] * p_value_norm
        )
        financial_score = (
            financial_weights['npv'] * npv_norm +
            financial_weights['irr'] * irr_norm +
            financial_weights['em'] * em_norm
        )

        # Handling NaN values
        weighted_score = 0 if np.isnan(weighted_score) else weighted_score
        financial_score = 0 if np.isnan(financial_score) else financial_score

        # Normalize the weighted and financial scores against their ranges
        normalization_ranges['weighted_score'], previous_scores = update_normalization_ranges_and_renormalize_scores(
            normalization_ranges.get('weighted_score', [(float('inf'), float('-inf'))]),
            [weighted_score],
            previous_scores,
            'weighted_score'
        )
        weighted_score_norm = (weighted_score - normalization_ranges['weighted_score'][0][0]) / (normalization_ranges['weighted_score'][0][1] - normalization_ranges['weighted_score'][0][0])
        weighted_score_norm = 0 if np.isnan(weighted_score_norm) else weighted_score_norm

        normalization_ranges['financial_score'], previous_scores = update_normalization_ranges_and_renormalize_scores(
            normalization_ranges.get('financial_score', [(float('inf'), float('-inf'))]),
            [financial_score],
            previous_scores,
            'financial_score'
        )
        financial_score_norm = (financial_score - normalization_ranges['financial_score'][0][0]) / (normalization_ranges['financial_score'][0][1] - normalization_ranges['financial_score'][0][0])
        financial_score_norm = 0 if np.isnan(financial_score_norm) else financial_score_norm

        # Apply regularization if needed
        regularization_term = 0
        if previous_weights:
            last_weights = previous_weights[-1]
            regularization_term =  * np.linalg.norm(
                np.array(list(metric_weights.values())) - np.array(list(last_weights['metric_weights'].values()))
            )
            weighted_score_norm -= regularization_term
            financial_score_norm -= regularization_term

        # Calculate total score before and after regularization
        pre_regularization_total_score = (weighted_score_norm + financial_score_norm) / (sum(metric_weights.values()) + sum(financial_weights.values()))
        normalization_ranges['pre_regularization_total_score'], previous_scores = update_normalization_ranges_and_renormalize_scores(
            normalization_ranges.get('pre_regularization_total_score', [(float('inf'), float('-inf'))]),
            [pre_regularization_total_score],
            previous_scores,
            'pre_regularization_total_score'
        )
        pre_regularization_total_score_norm = (pre_regularization_total_score - normalization_ranges['pre_regularization_total_score'][0][0]) / (normalization_ranges['pre_regularization_total_score'][0][1] - normalization_ranges['pre_regularization_total_score'][0][0])

        total_score = pre_regularization_total_score_norm - regularization_term
        normalization_ranges['total_score'], previous_scores = update_normalization_ranges_and_renormalize_scores(
            normalization_ranges.get('total_score', [(float('inf'), float('-inf'))]),
            [total_score],
            previous_scores,
            'total_score'
        )
        total_score_norm = (total_score - normalization_ranges['total_score'][0][0]) / (normalization_ranges['total_score'][0][1] - normalization_ranges['total_score'][0][0])
        total_score_norm = 0 if np.isnan(total_score_norm) else total_score_norm

        # Track the current iteration's score and metrics
        current_score = {
            'silhouette': silhouette_avg,
            'davies_bouldin': davies_bouldin_avg,
            'calinski_harabasz': calinski_harabasz_avg,
            'eigenvalue_gap': eigenvalue_gap,
            'p_value': p_value,
            'npv': financial_metrics['npv'],
            'irr': financial_metrics['irr'],
            'em': financial_metrics['em'],
            'metric_weights': metric_weights,
            'financial_weights': financial_weights,
            'total_score': total_score_norm
        }
        previous_scores.append(current_score)

        # Update best scores and k if the current score is better
        if total_score_norm > best_score:
            best_score = total_score_norm
            best_k = k
            best_weights = {**metric_weights, **financial_weights}
            best_clustering = labels

        if len(previous_scores) == previous_scores.maxlen:
            max_difference = max(score['total_score'] for score in previous_scores) - min(score['total_score'] for score in previous_scores)
            if max_difference < improvement_threshold:
                print("Cooling down the search space as score improvement has stabilized.")
                nonlocal search_space
                search_space = apply_cooling(search_space, cooling_factor)  # Apply cooling
                optimizer = Optimizer(search_space, base_estimator="gp", acq_func="EI", acq_optimizer="sampling", n_initial_points=15)
                if max_difference < improvement_threshold / 10:
                    print("Early stopping for the optimizer: Minimal score improvement detected.")
                    return 1e10  # Return a large positive value to stop the optimization process

        # Update the best evaluation if the current score is better, including financial metrics
        nonlocal best_evaluation, peak_score
        if best_evaluation is None or total_score_norm > best_evaluation.score:  # Update to use normalized total score
            best_evaluation = Evaluation(k, (silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value), metric_weights, financial_weights, , labels)
            best_evaluation.score = total_score_norm  # Update to use normalized total score
            peak_score = total_score_norm  # Update to use normalized total score

        # Print details of the current iteration
        print(f"\nIteration {iteration_count}:")
        print(f"Number of clusters (k): {k}")
        print(f"Best k so far: {best_k}")
        print("\nMetric Performance vs Targets:")
        print(f"Silhouette: {silhouette_avg:.4f} (Target: {metric_targets['silhouette']})")
        print(f"Davies-Bouldin: {davies_bouldin_avg:.4f} (Target: {metric_targets['davies_bouldin']})")
        print(f"Calinski-Harabasz: {calinski_harabasz_avg:.4f} (Target: {metric_targets['calinski_harabasz']})")
        print(f"Eigenvalue gap: {eigenvalue_gap:.4f} (Target: {metric_targets['eigenvalue_gap']})")
        print(f"p-value: {p_value:.4f} (Target: {metric_targets['p_value']})")
        print(f"NPV: {financial_metrics['npv']:.4f} (Target: {metric_targets['npv']})")
        print(f"IRR: {financial_metrics['irr']:.4f} (Target: {metric_targets['irr']})")
        print(f"EM: {financial_metrics['em']:.4f} (Target: {metric_targets['em']})")

        print("\nCurrent Weights:")
        for key, value in {**metric_weights, **financial_weights}.items():
            print(f"{key}: {value:.4f}")

        # Print the normalized total score and best score so far
        print(f"\nTotal score (normalized): {total_score_norm:.4f}")
        print(f"Best score so far (normalized): {best_score:.4f}")
        print("-" * 50)

        print("Returning from objective function:", -total_score_norm)
        return -total_score_norm  # We minimize the negative normalized score

    # Perform the optimization with a maximum number of iterations
    for i in range(5):  # Number of multi-starts
        print(f"Multi-start iteration {i+1}")
        results = optimizer.run(objective, n_iter=max_iter)
        print("Results object:", results)
        print("Best score found:", results.fun)
        print("Best parameters found:", results.x)

        if results.fun < best_overall_score:
            best_overall_score = results.fun
            best_overall_params = results.x

    print(f"Best overall score after multi-start: {best_overall_score}")
    print(f"Best overall parameters: {best_overall_params}")

    # Extract the best parameters
    best_params = results.x
    best_k = best_params[0]
    # best_feature_weights = {
    #     'continuous': best_params[1],
    #     'spatial': best_params[2],
    #     'categorical': best_params[3],
    #     'binary': best_params[4]
    # }
    best_metric_weights = {
        'silhouette': best_params[5],
        'davies_bouldin': best_params[6],
        'calinski_harabasz': best_params[7],
        'eigenvalue_gap': best_params[8],
        'p_value': best_params[9]
    }
    best_financial_weights = {
        'npv': best_params[10],
        'irr': best_params[11],
        'em': best_params[12]
    }

    # After optimization
    print("\nOptimization completed!")
    print(f"Total iterations: {iteration_count}")
    print(f"Best score: {best_score:.4f}")
    print(f"Best number of clusters (k): {best_k}")
    print(f"Best cluster sizes: {np.bincount(best_clustering)}")
    print("\nBest Weights:")
    for key, value in best_weights.items():
        print(f"{key}: {value:.4f}")

    return best_k, best_evaluation.score, best_metric_weights, best_financial_weights, best_evaluation.clustering

# Step 1: Identify Continuous Features
# Continuous features (excluding spatial features)
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_zoning', 'max_building_height_zoning',
    'max_residential_far_zoning', 'BldgArea', 'BuildingAspectRatio', 'Area'
]

# Spatial features
spatial_columns = ['Centroid_x', 'Centroid_y']

# Categorical features (assuming these are one-hot encoded)
categorical_columns = [col for col in df_sample.columns if col.startswith('LandUse_')]

# Binary features
binary_columns = [
    'NumFloors_estimated_flag', 'BuildingAspectRatio_estimated_flag',
    'is_front_yard_compliant_current', 'is_front_yard_compliant_proposed',
    'is_height_compliant', 'is_FAR_compliant',
    'is_rear_yard_compliant_current', 'is_rear_yard_compliant_proposed',
    'is_side_yard_compliant_current', 'is_side_yard_compliant_proposed'
]

# Step 2: Apply StandardScaler only to continuous columns
scaler = RobustScaler()

# Scale continuous features
X_scaled_continuous = pd.DataFrame(scaler.fit_transform(df_sample[updated_continuous_columns]), columns=updated_continuous_columns)

# Scale spatial features
X_scaled_spatial = pd.DataFrame(scaler.fit_transform(df_sample[spatial_columns]), columns=spatial_columns)

# Step 3: Combine scaled features with the rest of the features
df_scaled = df_sample.copy()
df_scaled[updated_continuous_columns] = X_scaled_continuous  # Replace continuous columns with scaled values
df_scaled[spatial_columns] = X_scaled_spatial  # Replace spatial columns with scaled values

# Step 4: Prepare for the rest of the pipeline
# Define X_scaled as the entire dataframe with scaled continuous and spatial features
X_scaled = df_scaled

# Ensure that indices are properly initialized
continuous_indices = [df_scaled.columns.get_loc(col) for col in updated_continuous_columns]
spatial_indices = [df_scaled.columns.get_loc(col) for col in spatial_columns]
categorical_indices = [df_scaled.columns.get_loc(col) for col in categorical_columns]
binary_indices = [df_scaled.columns.get_loc(col) for col in binary_columns]

# Preprocess the covariance matrix for the continuous features
#inv_cov_matrix, condition_number = preprocess_covariance_matrix(X_scaled[:, continuous_indices])

# Now df_scaled contains the scaled continuous features, while categorical, binary, and other features remain unchanged

# Define initial weights, W, , , and max_iter
initial_feature_weights = {
    'continuous': 0.3,
    'spatial': 0.25,
    'categorical': 0.2,
    'binary': 0.25
}

# Use the weights from Mini-KMeans as the starting point
initial_metric_weights = {
    'silhouette': 0.0587,
    'davies_bouldin': 0.25,
    'calinski_harabasz': 0.9,
    'eigenvalue_gap': -0.0749,  # Add a weight for the eigenvalue gap
    'p_value': 0.25,
    'npv': 0.1371,  # Initial weight for NPV, moderate influence
    'irr': 0.0142,  # Initial weight for IRR, moderate influence
    'em': -0.0094   # Initial weight for Equity Multiple, slightly higher importance
}

W = 0.05
 = 0.1
 = 1e-3
max_iter = 5000

# Define numerical targets for early stopping
metric_targets = {
    'silhouette': 0.3,
    'davies_bouldin': 1.2,
    'calinski_harabasz': 1000,
    'eigenvalue_gap': 0.1,  # Example target, adjust as needed
    'p_value': 0.05,
    'npv': 0.0,  # Minimum NPV target, assuming any positive NPV is acceptable
    'irr': 0.12,  # Minimum IRR target, considering reasonable return
    'em': 1.5   # Equity Multiple should be at least 1.0 (breakeven)
}

# Calculate normalization ranges based on initial broad range
#normalization_ranges = dynamic_normalization_ranges(X_scaled, k_values, metric_targets)

# Call the refine_weights_spectral function with the correct indices
best_k, best_score, final_metric_weights, final_financial_weights, best_clustering = refine_weights_spectral(
    X_scaled, initial_feature_weights, initial_metric_weights, , , max_iter, metric_targets,
    normalization_ranges=None, sample_tracker=None, feature_importances=None,
    cov_matrix=None, continuous_indices=continuous_indices,
    spatial_indices=spatial_indices, categorical_indices=categorical_indices, binary_indices=binary_indices
)

if best_k is not None:
    print(f"Best number of clusters: {best_k} with Weighted Score: {best_score}")
    print(f"Final metric weights: {final_metric_weights}")
    print(f"Final financial weights: {final_financial_weights}")
else:
    print("No valid clustering solution found.")

# # Example usage of analyze_clusters_spectral with your parameters
# results, sample_sizes = analyze_clusters_spectral(
#     merged_gdf_r1_r5, feature_importances, params, k_values=np.arange(2, 10), alpha=0.05, power=0.80, margin_of_error=0.05
# )

# # Combine the results
# current_simulations = pd.concat([result[0] for result in results], ignore_index=True)
# proposed_simulations = pd.concat([result[1] for result in results], ignore_index=True)

# # Visualize the results
# visualize_results(current_simulations, proposed_simulations)

# # Visualize clusters
# sns.scatterplot(data=merged_gdf_r1_r5, x='Centroid_x', y='Centroid_y', hue='cluster', palette='viridis')
# plt.show()

X_scaled

!pip install numpy_financial scikit-optimize

# Step 1: Set the Parameters Manually
best_k = 92  # Number of clusters
best_metric_weights = {
    'silhouette': -0.4851878480934705,
    'davies_bouldin': -0.3583672213721575,
    'calinski_harabasz': 0.9040797203164077,
    'eigenvalue_gap': 0.9445553020482649,
    'p_value': 0.17522293592073557
}
best_financial_weights = {
    'npv': -0.9354489526439986,
    'irr': 0.33745414842647503,
    'em': -0.09510069855452441
}
 = 0.8757282891773386  # Regularization term
sample_size = 23316  # Total sample size

# Initialize sample_tracker as an empty dictionary
sample_tracker = {}

# Example metric targets
metric_targets = {
    'silhouette': 0.3,
    'davies_bouldin': 1.2,
    'calinski_harabasz': 1000,
    'eigenvalue_gap': 0.1,
    'p_value': 0.05
}

# Step 2: Run the Clustering with These Parameters
silhouette_avg, davies_bouldin_avg, calinski_harabasz_avg, eigenvalue_gap, p_value, financial_metrics, labels = evaluate_spectral_clustering_with_sampling(
    X_scaled_df,  # Your scaled data
    best_k,  # Number of clusters
    best_metric_weights,  # Weights for clustering metrics
    ,  # Regularization term
    None,  # No previous weights if running once
    num_samples=sample_size,  # Sample size
    sample_tracker=sample_tracker,  # Initialize sample tracker
    metric_targets=metric_targets  # Pass the metric targets
)

# Step 3: Analyze the Results
print(f"Silhouette: {silhouette_avg}")
print(f"Davies-Bouldin: {davies_bouldin_avg}")
print(f"Calinski-Harabasz: {calinski_harabasz_avg}")
print(f"Eigenvalue Gap: {eigenvalue_gap}")
print(f"p-value: {p_value}")
print(f"NPV: {financial_metrics['npv']}")
print(f"IRR: {financial_metrics['irr']}")
print(f"EM: {financial_metrics['em']}")

# Analyze cluster labels
print(f"Cluster labels: {labels}")

# Step 4: Optionally Save the Results
import pandas as pd

# Assuming X_scaled_df is your original data or a DataFrame
results_df = pd.DataFrame(X_scaled_df)
results_df['Cluster'] = labels

# Save to CSV or any other format
results_df.to_csv('clustering_results.csv', index=False)

# Step 5: Visualize the Clusters using PCA
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Step 5: Visualize the Clusters using PCA with Random Colors
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled_df)

# Generate random colors
unique_labels = np.unique(labels)
num_clusters = len(unique_labels)
colors = np.random.rand(num_clusters, 3)  # Generate random colors for each cluster

# Print the number of unique labels
print(f"Number of unique labels (clusters): {num_clusters}")

# Create a color map based on the labels
color_map = {label: colors[i] for i, label in enumerate(unique_labels)}

# Map each label to its corresponding color
cluster_colors = [color_map[label] for label in labels]

# Plot the results
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_colors, s=50, alpha=0.7)

# Create a legend for the clusters
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[label], markersize=10, label=f'Cluster {label}') for label in unique_labels]
plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

plt.title('PCA of Clusters with Random Colors')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

import numpy as np
import pandas as pd
from sklearn.cluster import SpectralClustering
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import kneighbors_graph

# Set the parameters manually
best_k = 100  # Desired number of clusters
 = 0.5  # Reduced regularization term to encourage more clusters
sample_size = 23316  # Total sample size
n_neighbors = 20  # Number of neighbors for the affinity matrix

# Adjust the affinity matrix to encourage more clusters
affinity_matrix = kneighbors_graph(X_scaled_df, n_neighbors=n_neighbors, include_self=False).toarray()

# Apply Spectral Clustering with adjusted parameters
clustering = SpectralClustering(n_clusters=best_k, affinity='precomputed', assign_labels="discretize", random_state=42).fit(affinity_matrix)
labels = clustering.labels_

# Calculate clustering metrics
silhouette_avg = silhouette_score(X_scaled_df, labels)
davies_bouldin_avg = davies_bouldin_score(X_scaled_df, labels)
calinski_harabasz_avg = calinski_harabasz_score(X_scaled_df, labels)

# Eigenvalue Gap Calculation (simplified for this example)
eigenvalue_gap = np.abs(np.diff(np.sort(np.linalg.eigvals(affinity_matrix))[-2:]))

# Simulated financial metrics for demonstration
p_value = np.random.uniform(0, 1)
financial_metrics = {
    'npv': np.random.uniform(-100, 100),
    'irr': np.random.uniform(0, 1),
    'em': np.random.uniform(0, 10)
}

# Output results
print(f"Number of unique clusters: {len(np.unique(labels))}")
print(f"Silhouette: {silhouette_avg}")
print(f"Davies-Bouldin: {davies_bouldin_avg}")
print(f"Calinski-Harabasz: {calinski_harabasz_avg}")
print(f"Eigenvalue Gap: {eigenvalue_gap}")
print(f"p-value: {p_value}")
print(f"NPV: {financial_metrics['npv']}")
print(f"IRR: {financial_metrics['irr']}")
print(f"EM: {financial_metrics['em']}")

# Analyze cluster labels
unique_labels = np.unique(labels)
print(f"Unique labels: {unique_labels}")

# Save results to a DataFrame
results_df = pd.DataFrame(X_scaled_df)
results_df['Cluster'] = labels

# Save to CSV
results_df.to_csv('clustering_results.csv', index=False)

# PCA Visualization with Random Coloration
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled_df)

# Random colors for each cluster
colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))

plt.figure(figsize=(12, 8))
for i, label in enumerate(unique_labels):
    plt.scatter(X_pca[labels == label, 0], X_pca[labels == label, 1], color=colors[i], label=f'Cluster {label}', s=50, alpha=0.7)
plt.title('PCA of Clusters with Random Colors')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title="Clusters")
plt.show()

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
import numpy as np
from sklearn.neighbors import kneighbors_graph

# Step 1: Set the Parameters Manually
best_k = 100  # Desired number of clusters
 = 0.5  # Reduced regularization term to encourage more clusters
sample_size = 23316  # Total sample size
n_neighbors = 20  # Number of neighbors for the affinity matrix

# Adjust the affinity matrix to encourage more clusters
affinity_matrix = kneighbors_graph(X_scaled, n_neighbors=n_neighbors, include_self=False).toarray()

# Apply Spectral Clustering with adjusted parameters
clustering = SpectralClustering(n_clusters=best_k, affinity='precomputed', assign_labels="discretize", random_state=42).fit(affinity_matrix)
labels = clustering.labels_

# Calculate clustering metrics
silhouette_avg = silhouette_score(X_scaled, labels)
davies_bouldin_avg = davies_bouldin_score(X_scaled, labels)
calinski_harabasz_avg = calinski_harabasz_score(X_scaled, labels)

# Eigenvalue Gap Calculation (simplified for this example)
eigenvalue_gap = np.abs(np.diff(np.sort(np.linalg.eigvals(affinity_matrix))[-2:]))

# Simulated financial metrics for demonstration
p_value = np.random.uniform(0, 1)
financial_metrics = {
    'npv': np.random.uniform(-100, 100),
    'irr': np.random.uniform(0, 1),
    'em': np.random.uniform(0, 10)
}

# Output results
print(f"Number of unique clusters: {len(np.unique(labels))}")
print(f"Silhouette: {silhouette_avg}")
print(f"Davies-Bouldin: {davies_bouldin_avg}")
print(f"Calinski-Harabasz: {calinski_harabasz_avg}")
print(f"Eigenvalue Gap: {eigenvalue_gap}")
print(f"p-value: {p_value}")
print(f"NPV: {financial_metrics['npv']}")
print(f"IRR: {financial_metrics['irr']}")
print(f"EM: {financial_metrics['em']}")

# Analyze cluster labels
unique_labels = np.unique(labels)
print(f"Unique labels: {unique_labels}")

# Step 2: Create a copy of the original GeoDataFrame
df_sample_copy = df_sample.copy()

# Assign the cluster labels to the copy
df_sample_copy['Cluster'] = labels

# Use the sample_mask to get the corresponding geometries from merged_gdf_r1_r5
geometries = merged_gdf_r1_r5.loc[sample_mask, 'geometry'].reset_index(drop=True)

# Ensure that the geometries are correctly aligned with df_sample_copy
df_sample_copy = gpd.GeoDataFrame(df_sample_copy, geometry=geometries)

# Step 3: Plot the clusters on a map using the original geometries
fig, ax = plt.subplots(1, 1, figsize=(12, 8))
df_sample_copy.plot(column='Cluster', ax=ax, legend=True, cmap='tab20', linewidth=0.8, edgecolor='black', alpha=0.7)
plt.title('Cluster Visualization on Original Geometries (EPSG 2263)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Add a legend for clusters
handles, legend_labels = ax.get_legend_handles_labels()

# Filter numeric labels and convert to integers
numeric_labels = [label for label in legend_labels if label.isdigit()]
ax.legend(handles[:len(numeric_labels)], [f'Cluster {label}' for label in numeric_labels], title="Clusters", loc='best')

plt.show()

# Step 4: Optionally save the DataFrame with clustering results and geometries
df_sample_copy.to_csv('clustering_results_with_geometries.csv', index=False)

sample_mask.shape

merged_gdf_r1_r5.shape

df_sample

# Display the first few rows of the DataFrame to inspect the 'centroid_x' and 'centroid_y' columns
print(X_scaled_df[['Centroid_x', 'Centroid_y']].head())

X_scaled_df.columns

import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Read the file
with open('/content/drive/MyDrive/BayOptSpectClust3.txt', 'r') as file:
    text = file.read()

def extract_metrics(text):
    data = {
        'iteration': [],
        'k': [],
        'silhouette_weight': [],
        'davies_bouldin_weight': [],
        'calinski_harabasz_weight': [],
        'eigenvalue_gap_weight': [],
        'p_value_weight': [],
        'npv_weight': [],
        'irr_weight': [],
        'em_weight': [],
        'silhouette': [],
        'davies_bouldin': [],
        'calinski_harabasz': [],
        'eigenvalue_gap': [],
        'p_value': [],
        'npv': [],
        'irr': [],
        'em': [],
        'total_score': []
    }

    iteration_pattern = re.compile(r'Iteration (\d+):')
    k_pattern = re.compile(r'Number of clusters \(k\): (\d+)')
    metric_pattern = re.compile(r'(\w+(?:\s\w+)?): ([-\d.]+) \(Target: [-\d.]+\)')
    total_score_pattern = re.compile(r'Total score: ([-\d.]+)')
    weight_pattern = re.compile(r'(\w+): ([-\d.]+)')

    current_iteration = None
    current_data = {}

    for line in text.split('\n'):
        if match := iteration_pattern.search(line):
            if current_iteration is not None:
                for key in data.keys():
                    data[key].append(current_data.get(key, np.nan))
            current_iteration = int(match.group(1))
            current_data = {'iteration': current_iteration}
        elif match := k_pattern.search(line):
            current_data['k'] = int(match.group(1))
        elif match := metric_pattern.search(line):
            metric, value = match.groups()
            value = float(value)
            if metric == 'Silhouette':
                current_data['silhouette'] = value
            elif metric == 'Davies-Bouldin':
                current_data['davies_bouldin'] = value
            elif metric == 'Calinski-Harabasz':
                current_data['calinski_harabasz'] = value
            elif metric == 'Eigenvalue gap':
                current_data['eigenvalue_gap'] = value
            elif metric == 'p-value':
                current_data['p_value'] = value
            elif metric == 'NPV':
                current_data['npv'] = value
            elif metric == 'IRR':
                current_data['irr'] = value
            elif metric == 'EM':
                current_data['em'] = value
        elif match := weight_pattern.findall(line):
            for weight_name, weight_value in match:
                try:
                    weight_value = float(weight_value)
                except ValueError:
                    print(f"Warning: Skipping invalid weight value: {weight_value} for {weight_name}")
                    continue

                if weight_name == 'silhouette':
                    current_data['silhouette_weight'] = weight_value
                elif weight_name == 'davies_bouldin':
                    current_data['davies_bouldin_weight'] = weight_value
                elif weight_name == 'calinski_harabasz':
                    current_data['calinski_harabasz_weight'] = weight_value
                elif weight_name == 'eigenvalue_gap':
                    current_data['eigenvalue_gap_weight'] = weight_value
                elif weight_name == 'p_value':
                    current_data['p_value_weight'] = weight_value
                elif weight_name == 'npv':
                    current_data['npv_weight'] = weight_value
                elif weight_name == 'irr':
                    current_data['irr_weight'] = weight_value
                elif weight_name == 'em':
                    current_data['em_weight'] = weight_value
        elif match := total_score_pattern.search(line):
            current_data['total_score'] = float(match.group(1))

    # Add the last iteration
    if current_iteration is not None:
        for key in data.keys():
            data[key].append(current_data.get(key, np.nan))

    return pd.DataFrame(data)

def plot_metrics(df, metric_name):
    plt.figure(figsize=(12, 6))
    plt.plot(df['iteration'], df[metric_name])
    plt.title(f'{metric_name} over Iterations')
    plt.xlabel('Iteration')
    plt.ylabel(metric_name)
    plt.show()

# Extract metrics
df = extract_metrics(text)

# Debugging: Print the first few rows of the dataframe to check its content
print("First few rows of the extracted DataFrame:")
print(df.head())

# Debugging: Check for NaN values
print("Check for NaN values in the DataFrame:")
print(df.isna().sum())

# Drop columns with NaN values from analysis
df_filtered = df.dropna(axis=1, how='any')

# Debugging: Print the first few rows of the filtered dataframe
print("First few rows of the filtered DataFrame:")
print(df_filtered.head())

# Plot key metrics (only those that don't have NaNs)
for metric in df_filtered.columns:
    if metric not in ['iteration', 'k', 'total_score']:  # Skip iteration, k, and total_score for individual metric plots
        plot_metrics(df_filtered, metric)

# Analyze k values
if not df_filtered.empty:
    print(f"Min k: {df_filtered['k'].min()}")
    print(f"Max k: {df_filtered['k'].max()}")
    print(f"Mean k: {df_filtered['k'].mean():.2f}")
    print(f"Median k: {df_filtered['k'].median():.2f}")

    # Find best iteration
    best_iteration = df_filtered.loc[df_filtered['total_score'].idxmax(), 'iteration']
    best_score = df_filtered['total_score'].max()
    best_weights = df_filtered.loc[df_filtered['total_score'].idxmax(), [col for col in df_filtered.columns if '_weight' in col]]
    print(f"Best iteration: {best_iteration}")
    print(f"Best score: {best_score:.4f}")
    print("Best weights to start with for the next run:")
    print(best_weights)

    # Analyze convergence
    last_50_scores = df_filtered['total_score'].tail(50)
    score_range = last_50_scores.max() - last_50_scores.min()
    print(f"Range of scores in last 50 iterations: {score_range:.4f}")

    # Analyze correlation between metrics
    correlation_matrix = df_filtered.corr()
    plt.figure(figsize=(12, 10))
    plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
    plt.colorbar()
    plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)
    plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
    plt.title('Correlation Matrix of Metrics')
    plt.tight_layout()
    plt.show()

    # Analyze the relationship between k and total_score
    plt.figure(figsize=(12, 6))
    plt.scatter(df_filtered['k'], df_filtered['total_score'])
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Total Score')
    plt.title('Total Score vs Number of Clusters')
    plt.show()
else:
    print("Filtered DataFrame is empty. Please check the input data or filtering criteria.")

import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler

# Mount Google Drive
drive.mount('/content/drive')

# Read the file
with open('/content/drive/MyDrive/BayOptSpectClust3.txt', 'r') as file:
    text = file.read()

def extract_metrics(text):
    data = {
        'iteration': [],
        'k': [],
        'silhouette_weight': [],
        'eigenvalue_gap_weight': [],
        'npv_weight': [],
        'irr_weight': [],
        'em_weight': [],
        'silhouette': [],
        'eigenvalue_gap': [],
        'npv': [],
        'irr': [],
        'em': [],
        'total_score': []
    }

    iteration_pattern = re.compile(r'Iteration (\d+):')
    k_pattern = re.compile(r'Number of clusters \(k\): (\d+)')
    metric_pattern = re.compile(r'(\w+(?:\s\w+)?): ([-\d.]+) \(Target: [-\d.]+\)')
    total_score_pattern = re.compile(r'Total score: ([-\d.]+)')
    weight_pattern = re.compile(r'(\w+): ([-\d.]+)')

    current_iteration = None
    current_data = {}

    for line in text.split('\n'):
        if match := iteration_pattern.search(line):
            if current_iteration is not None:
                for key in data.keys():
                    data[key].append(current_data.get(key, np.nan))
            current_iteration = int(match.group(1))
            current_data = {'iteration': current_iteration}
        elif match := k_pattern.search(line):
            current_data['k'] = int(match.group(1))
        elif match := metric_pattern.search(line):
            metric, value = match.groups()
            value = float(value)
            if metric == 'Silhouette':
                current_data['silhouette'] = value
            elif metric == 'Eigenvalue gap':
                current_data['eigenvalue_gap'] = value
            elif metric == 'NPV':
                current_data['npv'] = value
            elif metric == 'IRR':
                current_data['irr'] = value
            elif metric == 'EM':
                current_data['em'] = value
        elif match := weight_pattern.findall(line):
            for weight_name, weight_value in match:
                try:
                    weight_value = float(weight_value)
                except ValueError:
                    print(f"Warning: Skipping invalid weight value: {weight_value} for {weight_name}")
                    continue

                if weight_name == 'silhouette':
                    current_data['silhouette_weight'] = weight_value
                elif weight_name == 'eigenvalue_gap':
                    current_data['eigenvalue_gap_weight'] = weight_value
                elif weight_name == 'npv':
                    current_data['npv_weight'] = weight_value
                elif weight_name == 'irr':
                    current_data['irr_weight'] = weight_value
                elif weight_name == 'em':
                    current_data['em_weight'] = weight_value
        elif match := total_score_pattern.search(line):
            current_data['total_score'] = float(match.group(1))

    # Add the last iteration
    if current_iteration is not None:
        for key in data.keys():
            data[key].append(current_data.get(key, np.nan))

    return pd.DataFrame(data)

def normalize_metrics(df, metrics):
    scaler = MinMaxScaler()
    df[metrics] = scaler.fit_transform(df[metrics])
    return df

def calculate_relative_weights(df, metrics, weights, targets):
    relative_weights = {}
    for metric, weight in zip(metrics, weights):
        differences = df[metric] - targets[metric]
        weighted_sum = (df[weight] * differences.abs()).sum()
        sum_of_differences = differences.abs().sum()
        relative_weights[weight] = weighted_sum / sum_of_differences
    return relative_weights

def calculate_weighted_averages(df, weights):
    weighted_averages = {}
    if 'total_score' in df.columns:
        for weight in weights:
            weighted_sum = (df[weight] * df['total_score']).sum()
            sum_of_scores = df['total_score'].sum()
            weighted_averages[weight] = weighted_sum / sum_of_scores
    else:
        print("Warning: 'total_score' column is missing. Skipping weighted averages calculation.")
    return weighted_averages

def analyze_preferred_ranges(df, weights):
    preferred_ranges = {}
    for weight in weights:
        lower_bound = df[weight].quantile(0.25)
        upper_bound = df[weight].quantile(0.75)
        preferred_ranges[weight] = (lower_bound, upper_bound)
    return preferred_ranges

# Extract metrics
df = extract_metrics(text)

# Drop columns with NaN values from analysis
df_filtered = df.dropna(axis=1, how='any')

# List of metrics and corresponding weights
metrics = ['silhouette', 'eigenvalue_gap', 'npv', 'irr', 'em']
weights = ['silhouette_weight', 'eigenvalue_gap_weight', 'npv_weight', 'irr_weight', 'em_weight']

# Define numerical targets
metric_targets = {
    'silhouette': 0.3,
    'eigenvalue_gap': 0.1,
    'npv': 0.0,
    'irr': 0.12,
    'em': 1.5
}

# Normalize the metrics
df_filtered = normalize_metrics(df_filtered, metrics)

# Calculate relative weights
relative_weights = calculate_relative_weights(df_filtered, metrics, weights, metric_targets)

# Calculate weighted averages using total score
weighted_averages = calculate_weighted_averages(df_filtered, weights)

# Analyze preferred ranges for weights
preferred_ranges = analyze_preferred_ranges(df_filtered, weights)

# Output results
print("Relative Weights based on metric differences from targets:")
for weight, value in relative_weights.items():
    print(f"{weight}: {value:.4f}")

print("\nWeighted Averages based on total score:")
for weight, value in weighted_averages.items():
    print(f"{weight}: {value:.4f}")

print("\nPreferred Ranges for Weights (Interquartile Range):")
for weight, range_values in preferred_ranges.items():
    print(f"{weight}: {range_values[0]:.4f} to {range_values[1]:.4f}")

from google.colab import drive
import re
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Read the file
with open('/content/drive/MyDrive/BayOptSpectClust.txt', 'r') as file:
    text = file.read()

def extract_metrics(text):
    data = {
        'iteration': [],
        'k': [],
        'silhouette': [],
        'davies_bouldin': [],
        'calinski_harabasz': [],
        'eigenvalue_gap': [],
        'p_value': [],
        'npv': [],
        'irr': [],
        'em': [],
        'total_score': []
    }

    iteration_pattern = re.compile(r'Iteration (\d+):')
    k_pattern = re.compile(r'Number of clusters \(k\): (\d+)')
    metric_pattern = re.compile(r'(\w+(?:\s\w+)?): ([-\d.]+) \(Target: [-\d.]+\)')
    total_score_pattern = re.compile(r'Total score: ([-\d.]+)')

    current_iteration = None
    current_data = {}

    for line in text.split('\n'):
        if match := iteration_pattern.search(line):
            if current_iteration is not None:
                for key in data.keys():
                    data[key].append(current_data.get(key, np.nan))
            current_iteration = int(match.group(1))
            current_data = {'iteration': current_iteration}
        elif match := k_pattern.search(line):
            current_data['k'] = int(match.group(1))
        elif match := metric_pattern.search(line):
            metric, value = match.groups()
            value = float(value)
            if metric == 'Silhouette':
                current_data['silhouette'] = value
            elif metric == 'Davies-Bouldin':
                current_data['davies_bouldin'] = value
            elif metric == 'Calinski-Harabasz':
                current_data['calinski_harabasz'] = value
            elif metric == 'Eigenvalue gap':
                current_data['eigenvalue_gap'] = value
            elif metric == 'p-value':
                current_data['p_value'] = value
            elif metric == 'NPV':
                current_data['npv'] = value
            elif metric == 'IRR':
                current_data['irr'] = value
            elif metric == 'EM':
                current_data['em'] = value
        elif match := total_score_pattern.search(line):
            current_data['total_score'] = float(match.group(1))

    # Add the last iteration
    if current_iteration is not None:
        for key in data.keys():
            data[key].append(current_data.get(key, np.nan))

    return pd.DataFrame(data)

def plot_metrics(df, metric_name):
    plt.figure(figsize=(12, 6))
    plt.plot(df['iteration'], df[metric_name])
    plt.title(f'{metric_name} over Iterations')
    plt.xlabel('Iteration')
    plt.ylabel(metric_name)
    plt.show()

# Extract metrics
df = extract_metrics(text)

# Plot key metrics
for metric in ['silhouette', 'davies_bouldin', 'calinski_harabasz', 'eigenvalue_gap', 'p_value', 'npv', 'irr', 'em', 'total_score']:
    plot_metrics(df, metric)

# Analyze k values
print(f"Min k: {df['k'].min()}")
print(f"Max k: {df['k'].max()}")
print(f"Mean k: {df['k'].mean():.2f}")
print(f"Median k: {df['k'].median():.2f}")

# Find best iteration
best_iteration = df.loc[df['total_score'].idxmax(), 'iteration']
best_score = df['total_score'].max()
print(f"Best iteration: {best_iteration}")
print(f"Best score: {best_score:.4f}")

# Analyze convergence
last_50_scores = df['total_score'].tail(50)
score_range = last_50_scores.max() - last_50_scores.min()
print(f"Range of scores in last 50 iterations: {score_range:.4f}")

# Analyze correlation between metrics
correlation_matrix = df[['k', 'silhouette', 'davies_bouldin', 'calinski_harabasz', 'eigenvalue_gap', 'p_value', 'npv', 'irr', 'em', 'total_score']].corr()
plt.figure(figsize=(12, 10))
plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
plt.colorbar()
plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)
plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
plt.title('Correlation Matrix of Metrics')
plt.tight_layout()
plt.show()

# Analyze the relationship between k and total_score
plt.figure(figsize=(12, 6))
plt.scatter(df['k'], df['total_score'])
plt.xlabel('Number of clusters (k)')
plt.ylabel('Total Score')
plt.title('Total Score vs Number of Clusters')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Assuming df is your DataFrame with the extracted metrics

# 1. Distribution of k values
plt.figure(figsize=(12, 6))
sns.histplot(df['k'], kde=True)
plt.title('Distribution of k values')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Frequency')
plt.show()

# 2. Top 10 best scores and their corresponding k values
top_10 = df.nlargest(10, 'total_score')
print("Top 10 best scores and their corresponding k values:")
print(top_10[['iteration', 'k', 'total_score']])

# 3. Scatter plot of k vs total_score with color-coded iterations
plt.figure(figsize=(12, 6))
scatter = plt.scatter(df['k'], df['total_score'], c=df['iteration'], cmap='viridis')
plt.colorbar(scatter, label='Iteration')
plt.title('k vs Total Score (color: iteration)')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Total Score')
plt.show()

# 4. Moving average of total_score
window_size = 50
df['moving_avg_score'] = df['total_score'].rolling(window=window_size).mean()

plt.figure(figsize=(12, 6))
plt.plot(df['iteration'], df['total_score'], alpha=0.5, label='Total Score')
plt.plot(df['iteration'], df['moving_avg_score'], label=f'{window_size}-iteration Moving Average')
plt.title('Total Score and Moving Average over Iterations')
plt.xlabel('Iteration')
plt.ylabel('Score')
plt.legend()
plt.show()

# 5. Heatmap of k vs metrics
metrics = ['silhouette', 'davies_bouldin', 'calinski_harabasz', 'eigenvalue_gap', 'p_value', 'npv', 'irr', 'em']
k_metric_corr = df.groupby('k')[metrics].mean().corr()

plt.figure(figsize=(12, 10))
sns.heatmap(k_metric_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Correlation between k and Metrics')
plt.show()

# 6. Box plot of total_score for different k ranges
df['k_range'] = pd.cut(df['k'], bins=[0, 50, 100, 150, 200, 250, 300], labels=['0-50', '51-100', '101-150', '151-200', '201-250', '251-300'])
plt.figure(figsize=(12, 6))
sns.boxplot(x='k_range', y='total_score', data=df)
plt.title('Distribution of Total Score for different k ranges')
plt.xlabel('Range of k')
plt.ylabel('Total Score')
plt.show()

# 7. Trend of best score found so far
df['best_score_so_far'] = df['total_score'].cummax()
plt.figure(figsize=(12, 6))
plt.plot(df['iteration'], df['best_score_so_far'])
plt.title('Best Score Found So Far')
plt.xlabel('Iteration')
plt.ylabel('Best Score')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from scipy import stats

# Assuming df is your DataFrame with the extracted metrics

# 1. Rescaled total score plot
plt.figure(figsize=(12, 6))
plt.plot(df['iteration'], df['total_score'])
plt.title('Total Score over Iterations (Rescaled)')
plt.xlabel('Iteration')
plt.ylabel('Total Score')
plt.ylim(21.5, 22.4)  # Set y-axis limits to focus on the relevant range
plt.show()

# 2. Distribution of k values for top 10% of scores
top_10_percent = df['total_score'].quantile(0.9)
top_scores = df[df['total_score'] >= top_10_percent]

plt.figure(figsize=(12, 6))
sns.histplot(top_scores['k'], kde=True)
plt.title('Distribution of k values for top 10% of scores')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Frequency')
plt.show()

# 3. Scatter plot of k vs total_score (focused on top scores)
plt.figure(figsize=(12, 6))
plt.scatter(df['k'], df['total_score'], alpha=0.5)
plt.scatter(top_scores['k'], top_scores['total_score'], color='red')
plt.title('k vs Total Score (red: top 10% scores)')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Total Score')
plt.ylim(21.5, 22.4)  # Set y-axis limits to focus on the relevant range
plt.show()

# 4. Moving average of total_score (rescaled)
window_size = 50
df['moving_avg_score'] = df['total_score'].rolling(window=window_size).mean()

plt.figure(figsize=(12, 6))
plt.plot(df['iteration'], df['total_score'], alpha=0.5, label='Total Score')
plt.plot(df['iteration'], df['moving_avg_score'], label=f'{window_size}-iteration Moving Average')
plt.title('Total Score and Moving Average over Iterations (Rescaled)')
plt.xlabel('Iteration')
plt.ylabel('Score')
plt.legend()
plt.ylim(21.5, 22.4)  # Set y-axis limits to focus on the relevant range
plt.show()

# 5. Additional printed analysis
print(f"Mean k for top 10% scores: {top_scores['k'].mean():.2f}")
print(f"Median k for top 10% scores: {top_scores['k'].median():.2f}")
print(f"Standard deviation of k for top 10% scores: {top_scores['k'].std():.2f}")

# Correlation between k and total_score
corr, p_value = stats.pearsonr(df['k'], df['total_score'])
print(f"Correlation between k and total_score: {corr:.4f} (p-value: {p_value:.4f})")

# Analysis of score improvement
score_improvements = df['total_score'].diff()
print(f"Mean score improvement per iteration: {score_improvements.mean():.6f}")
print(f"Median score improvement per iteration: {score_improvements.median():.6f}")

# Stability analysis
last_100_scores = df['total_score'].tail(100)
stability_range = last_100_scores.max() - last_100_scores.min()
print(f"Range of scores in last 100 iterations: {stability_range:.4f}")

# Convergence analysis
convergence_threshold = 0.001  # You can adjust this value
converged_iterations = (abs(score_improvements) < convergence_threshold).sum()
print(f"Number of iterations with improvement less than {convergence_threshold}: {converged_iterations}")

# Best k analysis
k_counts = top_scores['k'].value_counts()
print("\nTop 5 most frequent k values in top 10% scores:")
print(k_counts.head())

# Cluster size ranges analysis
print("\nCluster size ranges analysis:")
for range_start in range(0, 301, 50):
    range_end = range_start + 50
    range_scores = df[(df['k'] >= range_start) & (df['k'] < range_end)]['total_score']
    print(f"k range {range_start}-{range_end-1}:")
    print(f"  Mean score: {range_scores.mean():.4f}")
    print(f"  Max score: {range_scores.max():.4f}")
    print(f"  Min score: {range_scores.min():.4f}")
    print(f"  Score range: {range_scores.max() - range_scores.min():.4f}")

X_scaled_df

# Ensure df_sample has a reset index
df_sample.reset_index(drop=True, inplace=True)

import numpy as np

# Check for NaN values in the DataFrame
nan_check = df_scaled.isna().sum().sum()
print(f'Total NaN values in df_sample: {nan_check}')

df_final

df_sample

df_scaled

# Before scaling
nan_check_before = df_sample.isna().sum().sum()
print(f'Total NaN values before scaling: {nan_check_before}')

# After scaling
scaler = StandardScaler()
X_scaled_df = pd.DataFrame(scaler.fit_transform(df_sample), columns=df_sample.columns)
nan_check_after = X_scaled_df.isna().sum().sum()
print(f'Total NaN values after scaling: {nan_check_after}')

import statsmodels.stats.power as smp

# Set parameters
effect_size = 0.5  # Medium effect size
alpha = 0.05       # Significance level
power = 0.8        # Desired power level
n_features = 37    # Number of features in your dataset

# Estimate the sample size needed
sample_size = smp.NormalIndPower().solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1)
sample_size *= n_features  # Scale by the number of features

print(f"Estimated minimum number of samples needed: {int(sample_size)}")

!pip install numpy_financial scikit-optimize

merged_gdf_r1_r5.columns

df_final.columns

# Create copies of df_final and merged_gdf_r1_r5
df_final_copy = df_final.copy()
merged_gdf_r1_r5_copy = merged_gdf_r1_r5.copy()

# Ensure the indexes are aligned by resetting them
df_final_copy.reset_index(drop=True, inplace=True)
merged_gdf_r1_r5_copy.reset_index(drop=True, inplace=True)

# Add 'ZoneDist1' and 'Block' from merged_gdf_r1_r5_copy to df_final_copy based on the index
df_final_copy['ZoneDist1'] = merged_gdf_r1_r5_copy['ZoneDist1']
df_final_copy['Block'] = merged_gdf_r1_r5_copy['Block']

# Combine ZoneDist1 and Block for stratification
df_final_copy['strata'] = df_final_copy[['ZoneDist1', 'Block']].apply(lambda x: '_'.join(x.astype(str)), axis=1)

# Remove strata that have fewer than 2 samples
strata_counts = df_final_copy['strata'].value_counts()
valid_strata = strata_counts[strata_counts >= 2].index
df_final_copy = df_final_copy[df_final_copy['strata'].isin(valid_strata)]

# Perform stratified sampling to get exactly 2,500 samples based on the 'strata' column
from sklearn.model_selection import train_test_split

stratified_sample, _ = train_test_split(df_final_copy, train_size=2500, stratify=df_final_copy['strata'], random_state=42)

# Create a mask for the selected samples, based on the original index
original_indices_mask = df_final_copy.index.isin(stratified_sample.index)

# Create a mapping from the original df_final_copy indices to the sampled indices
sampled_indices = df_final_copy[original_indices_mask].index

# Use the sampled indices to select the corresponding geometries from merged_gdf_r1_r5
geometries = merged_gdf_r1_r5_copy.loc[sampled_indices, 'geometry'].reset_index(drop=True)

# Keep only the sampled rows in df_final_copy
df_sample = df_final_copy[original_indices_mask].copy()

# Drop temporary columns and reset the index
df_sample.drop(columns=['ZoneDist1', 'Block', 'strata'], inplace=True)
df_sample.reset_index(drop=True, inplace=True)

# Assign the geometry to df_sample to convert it to a GeoDataFrame
df_sample_gdf = gpd.GeoDataFrame(df_sample, geometry=geometries)

# df_sample_gdf now contains the stratified sample with the original columns and geometries

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import SpectralClustering
from sklearn.neighbors import kneighbors_graph, NearestNeighbors
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, pairwise_distances
from scipy.spatial.distance import pdist, squareform

# Step 1: Identify Continuous Features (same as you provided)
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_zoning', 'max_building_height_zoning',
    'max_residential_far_zoning', 'BldgArea', 'BuildingAspectRatio', 'Area'
]

# Spatial features
spatial_columns = ['Centroid_x', 'Centroid_y']

# Categorical features (assuming these are one-hot encoded)
categorical_columns = [col for col in df_sample.columns if col.startswith('LandUse_')]

# Binary features
binary_columns = [
    'NumFloors_estimated_flag', 'BuildingAspectRatio_estimated_flag',
    'is_front_yard_compliant_current', 'is_front_yard_compliant_proposed',
    'is_height_compliant', 'is_FAR_compliant',
    'is_rear_yard_compliant_current', 'is_rear_yard_compliant_proposed',
    'is_side_yard_compliant_current', 'is_side_yard_compliant_proposed'
]

# Combine all columns
all_columns = updated_continuous_columns + spatial_columns + categorical_columns + binary_columns

# Step 2: Apply RobustScaler to continuous and spatial columns
scaler = RobustScaler()

# Fit the scaler on df_sample (on continuous and spatial columns)
df_sample_to_scale = df_sample[updated_continuous_columns + spatial_columns]
scaler.fit(df_sample_to_scale)

# Transform both df_sample and df_final using the fitted scaler
df_sample_scaled_cont_spat = pd.DataFrame(scaler.transform(df_sample_to_scale), columns=updated_continuous_columns + spatial_columns)
df_final_to_scale = df_final[updated_continuous_columns + spatial_columns]
df_final_scaled_cont_spat = pd.DataFrame(scaler.transform(df_final_to_scale), columns=updated_continuous_columns + spatial_columns)

# Step 3: Combine scaled features with the rest of the features for both sample and full dataset
df_sample_scaled = df_sample.copy()
df_sample_scaled[updated_continuous_columns + spatial_columns] = df_sample_scaled_cont_spat

df_final_scaled = df_final.copy()
df_final_scaled[updated_continuous_columns + spatial_columns] = df_final_scaled_cont_spat

# Step 4: Perform Spectral Clustering on the sample
best_k = 75  # Desired number of clusters
n_neighbors = 50  # Number of neighbors for the affinity matrix

# Adjust the affinity matrix to encourage more clusters
affinity_matrix = kneighbors_graph(df_sample_scaled[all_columns], n_neighbors=n_neighbors, include_self=False).toarray()

# Apply Spectral Clustering with adjusted parameters
clustering = SpectralClustering(n_clusters=best_k, affinity='precomputed', assign_labels="discretize", random_state=42).fit(affinity_matrix)
sample_labels = clustering.labels_

# Step 5: Calculate the centroids of the clusters based on the features
cluster_centroids = df_sample_scaled.groupby(sample_labels).mean()

# Step 6: Calculate pairwise distances between the cluster centroids
centroid_distances = pdist(cluster_centroids.values)
centroid_dist_matrix = squareform(centroid_distances)

# Step 7: Use a sequential ordering based on the centroid distances
ordering = np.argsort(np.mean(centroid_dist_matrix, axis=1))
ordered_labels = np.zeros_like(sample_labels)

for new_label, old_label in enumerate(ordering):
    ordered_labels[sample_labels == old_label] = new_label

# Step 8: Apply k-NN to propagate the ordered clusters to the full dataset
knn = NearestNeighbors(n_neighbors=1)
knn.fit(df_sample_scaled[all_columns])
distances, indices = knn.kneighbors(df_final_scaled[all_columns])
full_labels = ordered_labels[indices.flatten()]

# Assign the full_labels to the full dataset
df_final['Cluster'] = full_labels

# Step 9: Calculate clustering metrics and compare to targets
silhouette_avg = silhouette_score(df_sample_scaled[all_columns], sample_labels)
davies_bouldin_avg = davies_bouldin_score(df_sample_scaled[all_columns], sample_labels)
calinski_harabasz_avg = calinski_harabasz_score(df_sample_scaled[all_columns], sample_labels)
eigenvalue_gap = np.diff(np.sort(np.linalg.eigvals(affinity_matrix))[-2:])[0]

# Set target values for each metric
target_silhouette = 0.5  # Example target for Silhouette Score
target_davies_bouldin = 0.5  # Example target for Davies-Bouldin Index
target_calinski_harabasz = 1000  # Example target for Calinski-Harabasz Index
target_eigenvalue_gap = 0.1  # Example target for Eigenvalue Gap

# Compute percentages or ratios to targets
silhouette_percentage = (silhouette_avg / target_silhouette) * 100
davies_bouldin_ratio = target_davies_bouldin / davies_bouldin_avg
calinski_harabasz_percentage = (calinski_harabasz_avg / target_calinski_harabasz) * 100
eigenvalue_gap_percentage = (eigenvalue_gap / target_eigenvalue_gap) * 100

# Print the clustering scores and how close they are to their targets
print(f"Silhouette Score: {silhouette_avg} ({silhouette_percentage:.2f}% of target)")
print(f"Davies-Bouldin Index: {davies_bouldin_avg} (Target ratio: {davies_bouldin_ratio:.2f})")
print(f"Calinski-Harabasz Index: {calinski_harabasz_avg} ({calinski_harabasz_percentage:.2f}% of target)")
print(f"Eigenvalue Gap: {eigenvalue_gap} ({eigenvalue_gap_percentage:.2f}% of target)")

# Step 10: Visualize the clusters on the full dataset
# Ensure the GeoDataFrame is in EPSG:2263
df_final = gpd.GeoDataFrame(df_final, geometry=merged_gdf_r1_r5['geometry'], crs="EPSG:2263")

# Use a sequential colormap
cmap = plt.cm.get_cmap('Spectral', best_k)

# Plot the clusters on a map using the original geometries
fig, ax = plt.subplots(1, 1, figsize=(20, 15))  # Adjust figure size as needed
df_final.plot(column='Cluster', ax=ax, legend=True, cmap=cmap, edgecolor=None, alpha=0.7)

plt.title('Ordered Cluster Visualization on Original Geometries (EPSG 2263)', fontsize=20)
plt.xlabel('Longitude', fontsize=15)
plt.ylabel('Latitude', fontsize=15)

plt.show()

# Step 11: Optionally save the DataFrame with clustering results and geometries
df_final.to_csv('ordered_full_clustering_results_with_geometries.csv', index=False)

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import SpectralClustering
from sklearn.neighbors import kneighbors_graph, NearestNeighbors
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
import seaborn as sns

# Assuming df_sample and df_final are already defined
# If not, you need to load your data here

# Define feature columns
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_zoning', 'max_building_height_zoning',
    'max_residential_far_zoning', 'BldgArea', 'BuildingAspectRatio', 'Area'
]

spatial_columns = ['Centroid_x', 'Centroid_y']
categorical_columns = [col for col in df_sample.columns if col.startswith('LandUse_')]
binary_columns = [
    'NumFloors_estimated_flag', 'BuildingAspectRatio_estimated_flag',
    'is_front_yard_compliant_current', 'is_front_yard_compliant_proposed',
    'is_height_compliant', 'is_FAR_compliant',
    'is_rear_yard_compliant_current', 'is_rear_yard_compliant_proposed',
    'is_side_yard_compliant_current', 'is_side_yard_compliant_proposed'
]

all_columns = updated_continuous_columns + spatial_columns + categorical_columns + binary_columns

# Scale the features
scaler = RobustScaler()
df_sample_to_scale = df_sample[updated_continuous_columns + spatial_columns]
df_final_to_scale = df_final[updated_continuous_columns + spatial_columns]
scaler.fit(df_sample_to_scale)

df_sample_scaled_cont_spat = pd.DataFrame(scaler.transform(df_sample_to_scale), columns=updated_continuous_columns + spatial_columns)
df_final_scaled_cont_spat = pd.DataFrame(scaler.transform(df_final_to_scale), columns=updated_continuous_columns + spatial_columns)

df_sample_scaled = df_sample.copy()
df_sample_scaled[updated_continuous_columns + spatial_columns] = df_sample_scaled_cont_spat

df_final_scaled = df_final.copy()
df_final_scaled[updated_continuous_columns + spatial_columns] = df_final_scaled_cont_spat

# Perform Spectral Clustering
best_k = 18
n_neighbors = 10

affinity_matrix = kneighbors_graph(df_sample_scaled[all_columns], n_neighbors=n_neighbors, include_self=False).toarray()
clustering = SpectralClustering(n_clusters=best_k, affinity='precomputed', assign_labels="discretize", random_state=42).fit(affinity_matrix)
sample_labels = clustering.labels_

# Add cluster labels to df_sample
df_sample['Cluster'] = sample_labels

# Propagate clusters to full dataset
knn = NearestNeighbors(n_neighbors=1)
knn.fit(df_sample_scaled[all_columns])
distances, indices = knn.kneighbors(df_final_scaled[all_columns])
df_final['Cluster'] = sample_labels[indices.flatten()]

# Visualize Cluster Assignments
df_final = gpd.GeoDataFrame(df_final, geometry=df_final['geometry'], crs="EPSG:2263")
cmap = plt.cm.get_cmap('Spectral', best_k)
fig, ax = plt.subplots(1, 1, figsize=(20, 15))
df_final.plot(column='Cluster', ax=ax, legend=True, cmap=cmap, edgecolor=None, alpha=0.7)
plt.title('Ordered Cluster Visualization on Original Geometries (EPSG 2263)', fontsize=20)
plt.xlabel('Longitude', fontsize=15)
plt.ylabel('Latitude', fontsize=15)
plt.show()

# Correlation Heatmap
correlation_matrix = df_sample[updated_continuous_columns].corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Continuous Features')
plt.show()

# Pairplot
sns.pairplot(df_sample, vars=updated_continuous_columns[:6], hue='Cluster', palette='tab20')
plt.suptitle('Pairplot of Continuous Features (First 6)', y=1.02)
plt.show()

# Feature Importances
X = df_sample[all_columns]
y = df_sample['Cluster']
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()

# PCA Analysis
pca = PCA(n_components=2)
pca_result = pca.fit_transform(X)
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA - Explained Variance by Number of Components')
plt.grid()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='tab20')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA - Scatter Plot of the First Two Principal Components')
plt.show()

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import MiniBatchKMeans
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
import seaborn as sns

# Assuming df_sample and df_final are already defined
# If not, you need to load your data here

# Define feature columns
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_zoning', 'max_building_height_zoning',
    'max_residential_far_zoning', 'BldgArea', 'BuildingAspectRatio', 'Area'
]

spatial_columns = ['Centroid_x', 'Centroid_y']
categorical_columns = [col for col in df_sample.columns if col.startswith('LandUse_')]
binary_columns = [
    'NumFloors_estimated_flag', 'BuildingAspectRatio_estimated_flag',
    'is_front_yard_compliant_current', 'is_front_yard_compliant_proposed',
    'is_height_compliant', 'is_FAR_compliant',
    'is_rear_yard_compliant_current', 'is_rear_yard_compliant_proposed',
    'is_side_yard_compliant_current', 'is_side_yard_compliant_proposed'
]

all_columns = updated_continuous_columns + spatial_columns + categorical_columns + binary_columns

# Scale the features
scaler = RobustScaler()
df_sample_to_scale = df_sample[updated_continuous_columns + spatial_columns]
df_final_to_scale = df_final[updated_continuous_columns + spatial_columns]
scaler.fit(df_sample_to_scale)

df_sample_scaled_cont_spat = pd.DataFrame(scaler.transform(df_sample_to_scale), columns=updated_continuous_columns + spatial_columns)
df_final_scaled_cont_spat = pd.DataFrame(scaler.transform(df_final_to_scale), columns=updated_continuous_columns + spatial_columns)

df_sample_scaled = df_sample.copy()
df_sample_scaled[updated_continuous_columns + spatial_columns] = df_sample_scaled_cont_spat

df_final_scaled = df_final.copy()
df_final_scaled[updated_continuous_columns + spatial_columns] = df_final_scaled_cont_spat

# Perform MiniBatchKMeans Clustering
best_k = 165
mb_kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=100)
mb_kmeans.fit(df_sample_scaled[all_columns])
sample_labels = mb_kmeans.labels_

# Add cluster labels to df_sample
df_sample['Cluster'] = sample_labels

# Propagate clusters to full dataset
knn = NearestNeighbors(n_neighbors=1)
knn.fit(df_sample_scaled[all_columns])
distances, indices = knn.kneighbors(df_final_scaled[all_columns])
df_final['Cluster'] = sample_labels[indices.flatten()]

# Visualize Cluster Assignments
df_final = gpd.GeoDataFrame(df_final, geometry=df_final['geometry'], crs="EPSG:2263")
cmap = plt.cm.get_cmap('Spectral', best_k)
fig, ax = plt.subplots(1, 1, figsize=(20, 15))
df_final.plot(column='Cluster', ax=ax, legend=True, cmap=cmap, edgecolor=None, alpha=0.7)
plt.title('Ordered Cluster Visualization on Original Geometries (EPSG 2263)', fontsize=20)
plt.xlabel('Longitude', fontsize=15)
plt.ylabel('Latitude', fontsize=15)
plt.show()

# Correlation Heatmap
correlation_matrix = df_sample[updated_continuous_columns].corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Continuous Features')
plt.show()

# Pairplot
sns.pairplot(df_sample, vars=updated_continuous_columns[:6], hue='Cluster', palette='tab20')
plt.suptitle('Pairplot of Continuous Features (First 6)', y=1.02)
plt.show()

# Feature Importances
X = df_sample[all_columns]
y = df_sample['Cluster']
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()

# PCA Analysis
pca = PCA(n_components=2)
pca_result = pca.fit_transform(X)
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA - Explained Variance by Number of Components')
plt.grid()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='tab20')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA - Scatter Plot of the First Two Principal Components')
plt.show()

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import MiniBatchKMeans
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import seaborn as sns

# Assuming df_sample and df_final are already defined
# If not, you need to load your data here

# Define feature columns
updated_continuous_columns = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_zoning', 'max_building_height_zoning',
    'max_residential_far_zoning', 'BldgArea', 'BuildingAspectRatio', 'Area'
]

spatial_columns = ['Centroid_x', 'Centroid_y']
categorical_columns = [col for col in df_sample.columns if col.startswith('LandUse_')]
binary_columns = [
    'NumFloors_estimated_flag', 'BuildingAspectRatio_estimated_flag',
    'is_front_yard_compliant_current', 'is_front_yard_compliant_proposed',
    'is_height_compliant', 'is_FAR_compliant',
    'is_rear_yard_compliant_current', 'is_rear_yard_compliant_proposed',
    'is_side_yard_compliant_current', 'is_side_yard_compliant_proposed'
]

all_columns = updated_continuous_columns + spatial_columns + categorical_columns + binary_columns

# Scale the features
scaler = RobustScaler()
df_sample_to_scale = df_sample[updated_continuous_columns + spatial_columns]
df_final_to_scale = df_final[updated_continuous_columns + spatial_columns]
scaler.fit(df_sample_to_scale)

df_sample_scaled_cont_spat = pd.DataFrame(scaler.transform(df_sample_to_scale), columns=updated_continuous_columns + spatial_columns)
df_final_scaled_cont_spat = pd.DataFrame(scaler.transform(df_final_to_scale), columns=updated_continuous_columns + spatial_columns)

df_sample_scaled = df_sample.copy()
df_sample_scaled[updated_continuous_columns + spatial_columns] = df_sample_scaled_cont_spat

df_final_scaled = df_final.copy()
df_final_scaled[updated_continuous_columns + spatial_columns] = df_final_scaled_cont_spat

# Perform MiniBatchKMeans Clustering
best_k = 80
mb_kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=100)
mb_kmeans.fit(df_sample_scaled[all_columns])
sample_labels = mb_kmeans.labels_

# Add cluster labels to df_sample
df_sample['Cluster'] = sample_labels

# Propagate clusters to full dataset
knn = NearestNeighbors(n_neighbors=1)
knn.fit(df_sample_scaled[all_columns])
distances, indices = knn.kneighbors(df_final_scaled[all_columns])
df_final['Cluster'] = sample_labels[indices.flatten()]

# Clustering Statistics
inertia = mb_kmeans.inertia_
silhouette_avg = silhouette_score(df_sample_scaled[all_columns], sample_labels)
db_index = davies_bouldin_score(df_sample_scaled[all_columns], sample_labels)
ch_score = calinski_harabasz_score(df_sample_scaled[all_columns], sample_labels)

# Gap Statistic
def gap_statistic(data, n_refs=3, max_clusters=140):
    gaps = np.zeros((max_clusters,))
    results = []

    for k in range(60, max_clusters+1):
        ref_disps = np.zeros(n_refs)

        for i in range(n_refs):
            # Create a random uniform reference distribution for the data
            random_reference = np.random.random_sample(size=data.shape)
            km = MiniBatchKMeans(k)
            km.fit(random_reference)
            ref_disp = km.inertia_
            ref_disps[i] = ref_disp

        km = MiniBatchKMeans(k)
        km.fit(data)

        orig_disp = km.inertia_
        gap = np.log(np.mean(ref_disps)) - np.log(orig_disp)

        gaps[k-1] = gap

        # Collect results in a list of dictionaries
        results.append({'cluster_count': k, 'gap': gap})

    # Convert the list of dictionaries into a DataFrame
    results_df = pd.DataFrame(results)

    return gaps, results_df

# Compute Gap Statistic
gaps, results_df = gap_statistic(df_sample_scaled[all_columns], max_clusters=100)
best_gap_k = np.argmax(gaps) + 1
best_gap_value = gaps[best_gap_k-1]

# Define benchmark or target values (These are examples, adjust based on your context)
target_silhouette = 0.5  # Higher is better
target_db_index = 1.0    # Lower is better
target_ch_score = 3000   # Higher is better
target_gap_statistic = max(gaps)  # Gap statistic should be maximized

# Compare against benchmarks and interpret
print(f'Clustering Statistics for k={best_k}:')
print(f'Inertia: {inertia} (No direct target, lower is generally better but depends on k)')
print(f'Silhouette Score: {silhouette_avg} (Target: >={target_silhouette})')
print(f'Davies-Bouldin Index: {db_index} (Target: <={target_db_index})')
print(f'Calinski-Harabasz Index: {ch_score} (Target: >={target_ch_score})')
print(f'Gap Statistic at best k: {best_gap_value} (Target: Max Gap at k={best_gap_k})')

# Visualize Cluster Assignments
df_final = gpd.GeoDataFrame(df_final, geometry=df_final['geometry'], crs="EPSG:2263")
cmap = plt.cm.get_cmap('Spectral', best_k)
fig, ax = plt.subplots(1, 1, figsize=(20, 15))
df_final.plot(column='Cluster', ax=ax, legend=True, cmap=cmap, edgecolor=None, alpha=0.7)
plt.title('Ordered Cluster Visualization on Original Geometries (EPSG 2263)', fontsize=20)
plt.xlabel('Longitude', fontsize=15)
plt.ylabel('Latitude', fontsize=15)
plt.show()

# Correlation Heatmap
correlation_matrix = df_sample[updated_continuous_columns].corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Continuous Features')
plt.show()

# Pairplot
sns.pairplot(df_sample, vars=updated_continuous_columns[:6], hue='Cluster', palette='tab20')
plt.suptitle('Pairplot of Continuous Features (First 6)', y=1.02)
plt.show()

# Feature Importances
X = df_sample[all_columns]
y = df_sample['Cluster']
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()

# PCA Analysis
pca = PCA(n_components=2)
pca_result = pca.fit_transform(X)
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA - Explained Variance by Number of Components')
plt.grid()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='tab20')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA - Scatter Plot of the First Two Principal Components')
plt.show()

# Plot Gap Statistic
plt.figure(figsize=(10, 6))
plt.plot(range(1, 81), gaps, marker='o', linestyle='--', color='blue')
plt.axvline(x=best_gap_k, color='red', linestyle='--')
plt.title('Gap Statistic')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap Value')
plt.grid(True)
plt.show()

# Display the Gap Statistic DataFrame
print("Gap Statistic Results:")
print(results_df)

# Plot Gap Statistic
plt.figure(figsize=(10, 6))
plt.plot(range(1, 81), gaps, marker='o', linestyle='--', color='blue')
plt.axvline(x=best_gap_k, color='red', linestyle='--')
plt.title('Gap Statistic')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap Value')
plt.grid(True)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = df_sample[updated_continuous_columns].corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Continuous Features')
plt.show()
import seaborn as sns

# Create a pairplot for a subset of features
sns.pairplot(df_sample, vars=updated_continuous_columns[:6], hue='Cluster', palette='tab20')
plt.suptitle('Pairplot of Continuous Features (First 6)', y=1.02)
plt.show()
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Prepare data for Random Forest
X = df_sample[updated_continuous_columns + spatial_columns + categorical_columns + binary_columns]
y = df_sample['Cluster']

# Train a Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# Get feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()
from sklearn.decomposition import PCA

# Apply PCA
pca = PCA(n_components=2)  # You can adjust n_components as needed
pca_result = pca.fit_transform(X)

# Plot the explained variance
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA - Explained Variance by Number of Components')
plt.grid()
plt.show()

# Scatter plot of the first two principal components
plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='tab20')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA - Scatter Plot of the First Two Principal Components')
plt.show()



import pandas as pd
import numpy as np
from sklearn.metrics import pairwise_distances

# Step 1: Create a copy of the scaled DataFrame and exclude the geometry column for centroid calculation
df_final_scaled_copy = df_final_scaled.copy()
features_of_interest = [
    'LotArea', 'LotFront', 'BldgFront', 'NumFloors', 'height',
    'current_FAR', 'min_front_yard_depth_current', 'min_front_yard_depth_zoning',
    'max_building_height_zoning', 'max_residential_far_zoning', 'BldgArea',
    'BuildingAspectRatio', 'Area'
]

# Filter the DataFrame to include only the features of interest
df_final_scaled_copy = df_final_scaled_copy[features_of_interest + ['Cluster']]

# Calculate the cluster centroids based on the features of interest
cluster_centroids = df_final_scaled_copy.groupby('Cluster').mean()

# Step 2: Identify the Representative Lot for Each Cluster in the Scaled Data
representative_lots_indices = []

for cluster_label in cluster_centroids.index:
    # Get all lots in the cluster
    cluster_data = df_final_scaled_copy[df_final_scaled_copy['Cluster'] == cluster_label]

    # Calculate the distance of each lot to the cluster centroid
    centroid = cluster_centroids.loc[cluster_label, :].values.reshape(1, -1)
    distances = pairwise_distances(cluster_data[features_of_interest], centroid)

    # Identify the lot with the minimum distance to the centroid
    representative_index = cluster_data.index[distances.argmin()]
    representative_lots_indices.append(representative_index)

# Step 3: Map the Representative Lots to the Original Unscaled Data
df_final_copy = df_final.copy()  # Create a copy of the unscaled DataFrame
representative_lots_unscaled = df_final_copy.loc[representative_lots_indices, features_of_interest + ['Cluster']]

# Step 4: Generate the summary table for the representative features of each cluster
summary_table_unscaled = representative_lots_unscaled.groupby('Cluster').mean().reset_index()

# Display the summary table of representative features
summary_table_unscaled.head(10)  # Display the first 10 rows of the summary table

df_final.columns

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Step 1: Combine LotDepth and LotFront into a single feature if they exist
if 'LotFront' in df.columns and 'LotDepth' in df.columns:
    df['LotDimensionsRatio'] = df['LotFront'] / df['LotDepth']
    features_to_remove = ['LotFront', 'LotDepth']
else:
    print("Warning: 'LotFront' or 'LotDepth' not found in the dataframe.")
    features_to_remove = []

# Step 2: Remove unnecessary features
features_to_remove.extend([
    'LotCoverageRatio', 'BuildableArea_current', 'BuildableArea_proposed',
    'FloorAreaPotential_current', 'FloorAreaPotential_proposed',
    'SetbackComplianceRatio_current', 'SetbackComplianceRatio_proposed'
])
df = df.drop(columns=features_to_remove, errors='ignore')

# Step 3: Identify numeric features (including the new LotDimensionsRatio if created)
numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()

# Step 4: Handle NaNs explicitly
# Print rows with NaNs for inspection
print("Rows with NaNs before handling:")
print(df[df[numeric_features].isna().any(axis=1)])

# Fill NaNs with appropriate value (e.g., 0, mean, or median)
df[numeric_features] = df[numeric_features].fillna(0)  # You can choose to fill with other values as needed

# Check for NaNs after filling
print("Rows with NaNs after handling:")
print(df[df[numeric_features].isna().any(axis=1)])

# Step 5: Replace infinite values with column mean or median
print("Replacing infinite values with column means.")
df[numeric_features] = df[numeric_features].replace([np.inf, -np.inf], np.nan)
df[numeric_features] = df[numeric_features].apply(lambda x: x.fillna(x.mean()), axis=0)

# Step 6: Scale numeric features
features_to_scale = [feature for feature in numeric_features if feature not in categorical_features]
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[features_to_scale]), columns=features_to_scale)

# Step 7: Combine scaled numerical features with one-hot encoded categorical features and binary flags
df_final = pd.concat([df_scaled, landuse_encoded, df[['NumFloors_estimated_flag', 'ResArea_estimated_flag', 'BuildingAspectRatio_estimated_flag']]], axis=1)

# Step 8: Update the features list for clustering
features_for_clustering = df_final.columns.tolist()

# Print the updated features list for clustering
print("Updated features list for clustering:")
print(features_for_clustering)

# Step 9: Calculate VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(df, features):
    vif_data = pd.DataFrame()
    vif_data["Feature"] = features
    vif_data["VIF"] = [variance_inflation_factor(df[features].values, i) for i in range(len(features))]
    return vif_data

try:
    vif = calculate_vif(df_final, features_to_scale)
    print(vif)
except Exception as e:
    print(f"Error calculating VIF: {e}")

# Step 10: Function to remove features with high VIF
def remove_high_vif_features(df, features, threshold=10.0):
    while True:
        vif = calculate_vif(df, features)
        max_vif = vif['VIF'].max()
        if max_vif > threshold:
            feature_to_remove = vif.loc[vif['VIF'].idxmax(), 'Feature']
            print(f"Removing feature with high VIF: {feature_to_remove}")
            features.remove(feature_to_remove)
        else:
            break
    return features

# Remove features with high VIF
reduced_continuous_columns = remove_high_vif_features(df_final, features_to_scale)

# Get indices of reduced continuous features in df_final
reduced_continuous_indices = [df_final.columns.get_loc(col) for col in reduced_continuous_columns]

# Apply the preprocessing to the reduced continuous columns
try:
    inv_cov_matrix = preprocess_covariance_matrix(df_final.values, reduced_continuous_indices)
    print("Covariance matrix preprocessing completed successfully.")
except Exception as e:
    print(f"Error during covariance matrix preprocessing: {e}")

import pandas as pd
import numpy as np

# Assuming merged_gdf_r1_r5 is your original DataFrame
df = merged_gdf_r1_r5.copy()

# List of features to check
features_to_check = [
    ('min_front_yard_depth_current', 'min_front_yard_depth_proposed'),
    ('max_building_height_current', 'max_building_height_proposed'),
    ('max_residential_far_current', 'max_residential_far_proposed')
]

# Function to check equality and print results
def check_equality(df, col1, col2):
    are_equal = df[col1].equals(df[col2])
    if are_equal:
        print(f"{col1} and {col2} are exactly equal.")
    else:
        # Calculate percentage of unequal values
        unequal_percentage = (df[col1] != df[col2]).mean() * 100
        print(f"{col1} and {col2} are not equal.")
        print(f"Percentage of unequal values: {unequal_percentage:.2f}%")

        # Print some sample unequal values
        unequal_samples = df[df[col1] != df[col2]].sample(min(5, (df[col1] != df[col2]).sum()))
        print("\nSample of unequal values:")
        print(unequal_samples[[col1, col2]])

# Check equality for each pair of features
for current_col, proposed_col in features_to_check:
    print(f"\nChecking equality for {current_col} and {proposed_col}:")
    check_equality(df, current_col, proposed_col)

# Additional check for NaN values
print("\nChecking for NaN values:")
for col in set([col for pair in features_to_check for col in pair]):
    nan_count = df[col].isna().sum()
    nan_percentage = (nan_count / len(df)) * 100
    print(f"{col}: {nan_count} NaN values ({nan_percentage:.2f}%)")

import pandas as pd

# Set display options to avoid ellipses
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.expand_frame_repr', False)

# Check if df has data
print(f"Number of rows in df: {df_final.shape[0]}")
print("Preview of df:")
print(df_final.head().to_string())  # Ensures full display of the first few rows

# Run the check_special_values function on df
special_values_report_df = check_special_values(df_final, df_final.columns)
print("Special values report for df:")
print(special_values_report_df.to_string())  # Ensures full display of the special values report

# Reset the display options to default if needed
pd.reset_option('display.max_rows')
pd.reset_option('display.max_columns')
pd.reset_option('display.expand_frame_repr')

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Ensure we're only using continuous columns
X = df_final[updated_continuous_columns]

print("Shape of X (continuous columns only):", X.shape)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Shape of X_scaled:", X_scaled.shape)

def preprocess_covariance_matrix(X_scaled):
    # Check the data types of X_scaled
    print(f"Data types of continuous features: {X_scaled.dtype}")

    # Check for any NaN or Inf values that might cause issues
    if np.isnan(X_scaled).any():
        raise ValueError("NaN values detected in X_scaled.")
    if np.isinf(X_scaled).any():
        raise ValueError("Infinite values detected in X_scaled.")

    # Calculate the Covariance Matrix for the continuous features
    try:
        cov_matrix = np.cov(X_scaled, rowvar=False)
        print(f"Shape of covariance matrix: {cov_matrix.shape}")
    except Exception as e:
        raise ValueError(f"Error calculating covariance matrix: {e}")

    # Check if the Covariance Matrix is Singular
    try:
        det_cov_matrix = np.linalg.det(cov_matrix)
        if det_cov_matrix == 0:
            raise ValueError("Covariance matrix is singular, cannot invert.")
        else:
            print(f"Determinant of covariance matrix: {det_cov_matrix}")
    except Exception as e:
        raise ValueError(f"Error calculating determinant: {e}")

    # Invert the Covariance Matrix
    try:
        inv_cov_matrix = np.linalg.inv(cov_matrix)
    except Exception as e:
        raise ValueError(f"Error inverting covariance matrix: {e}")

    # Check for Collinearity (Condition Number)
    try:
        cond_number = np.linalg.cond(cov_matrix)
        print(f"Condition number of covariance matrix: {cond_number}")

        # Categorize the condition number
        if cond_number < 100:
            print("The matrix is well-conditioned.")
        elif cond_number < 1000:
            print("The matrix is moderately conditioned.")
        else:
            print("The matrix is ill-conditioned. There may be multicollinearity issues.")

    except Exception as e:
        raise ValueError(f"Error calculating condition number: {e}")

    return inv_cov_matrix, cond_number

# Apply the preprocessing to the continuous columns
try:
    inv_cov_matrix, condition_number = preprocess_covariance_matrix(X_scaled)
    print("Covariance matrix preprocessing completed successfully.")

    # Calculate condition number using eigenvalues for comparison
    eigenvalues = np.linalg.eigvals(np.cov(X_scaled, rowvar=False))
    cond_number_eigen = np.max(np.abs(eigenvalues)) / np.min(np.abs(eigenvalues))
    print(f"Condition Number (using eigenvalues): {cond_number_eigen}")

except Exception as e:
    print(f"Error during covariance matrix preprocessing: {e}")

# Print correlation matrix
corr_matrix = X.corr()
print("\nCorrelation Matrix:")
print(corr_matrix)

# Identify highly correlated feature pairs
print("\nHighly correlated feature pairs:")
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > 0.8:  # You can adjust this threshold
            print(f"{corr_matrix.columns[i]} - {corr_matrix.columns[j]} : {corr_matrix.iloc[i, j]}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming df_analysis is your DataFrame and updated_continuous_columns is your list of continuous columns

# Extract the continuous features
X = df_final[updated_continuous_columns]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Calculate the covariance matrix
cov_matrix = np.cov(X_scaled, rowvar=False)

# Calculate the condition number of the covariance matrix
eigenvalues = np.linalg.eigvals(cov_matrix)
condition_number = np.max(np.abs(eigenvalues)) / np.min(np.abs(eigenvalues))

print(f"Condition Number of Covariance Matrix: {condition_number}")

# Interpret the condition number
if condition_number < 100:
    print("The features are well-conditioned. Multicollinearity is not a significant issue.")
elif condition_number < 1000:
    print("The features are moderately conditioned. There may be some multicollinearity.")
else:
    print("The features are ill-conditioned. Severe multicollinearity is likely present.")

# Calculate and print the logarithm of the condition number
log_condition_number = np.log10(condition_number)
print(f"Log10 of Condition Number: {log_condition_number}")

# Additional analysis: Correlation matrix
corr_matrix = X.corr()

# Print the correlation matrix
print("\nCorrelation Matrix:")
print(corr_matrix)

# Identify highly correlated feature pairs
print("\nHighly correlated feature pairs:")
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > 0.8:  # You can adjust this threshold
            print(f"{corr_matrix.columns[i]} - {corr_matrix.columns[j]} : {corr_matrix.iloc[i, j]}")

# Visualize the correlation matrix
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(df, features):
    vif_data = pd.DataFrame()
    vif_data["Feature"] = features
    vif_data["VIF"] = [variance_inflation_factor(df[features].values, i) for i in range(len(features))]
    return vif_data

# Calculate VIF for continuous features
vif = calculate_vif(df_final, continuous_columns)
print(vif)

def remove_high_vif_features(df, features, threshold=10.0):
    while True:
        vif = calculate_vif(df, features)
        max_vif = vif['VIF'].max()
        if max_vif > threshold:
            feature_to_remove = vif.loc[vif['VIF'].idxmax(), 'Feature']
            print(f"Removing feature with high VIF: {feature_to_remove}")
            features.remove(feature_to_remove)
        else:
            break
    return features

# Remove features with high VIF
reduced_continuous_columns = remove_high_vif_features(df_final, continuous_columns)

# Get indices of reduced continuous features in df_final
reduced_continuous_indices = [df_final.columns.get_loc(col) for col in reduced_continuous_columns]

# Apply the preprocessing to the reduced continuous columns
try:
    inv_cov_matrix = preprocess_covariance_matrix(X_scaled, reduced_continuous_indices)
    print("Covariance matrix preprocessing completed successfully.")
except Exception as e:
    print(f"Error during covariance matrix preprocessing: {e}")

!pip install scikit-optimize

def analyze_redevelopment_potential_single(gdf, zoning_data, params):
    try:
        # Ensure zoning_data is a DataFrame
        if isinstance(zoning_data, dict):
            zoning_data = pd.DataFrame(zoning_data)

        zone = gdf['ZoneDist1']

        if zone not in zoning_data.index:
            print(f"Zone {zone} not found in zoning data.")
            return pd.DataFrame({key: np.nan for key in ['current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

        # Existing Building Strategy (Strategy 2)
        current_value = gdf['BldgArea'] * float(params['market_value_per_sqft'])
        existing_land_value = gdf['LotArea'] * float(params['land_value_per_sqft'])
        existing_revenue = current_value * 0.05  # Assuming 5% cap rate
        holding_period_years = float(params['holding_period_years'])
        appreciation_rate = float(params['appreciation_rate'])
        discount_rate = float(params['discount_rate'])

        # Calculate cashflows for keeping the existing building
        existing_cashflows = np.array([
            -current_value,
            *([existing_revenue] * int(holding_period_years)),
            existing_revenue * (1 + appreciation_rate)**holding_period_years
        ])

        npv_existing = safe_npv(discount_rate, existing_cashflows)
        irr_existing = safe_irr(existing_cashflows)
        equity_multiple_existing = np.sum(existing_cashflows) / current_value

        # Redevelopment Strategy (Strategy 1)
        proposed_far = float(zoning_data.loc[zone, 'Residential FAR (max)'])
        front_yard = float(zoning_data.loc[zone, 'Front Yard Depth (min)']) if zoning_data.loc[zone, 'Front Yard Depth (min)'] is not None else 0
        rear_yard = float(zoning_data.loc[zone, 'Rear Yard Depth (min)']) if zoning_data.loc[zone, 'Rear Yard Depth (min)'] is not None else 0
        side_yards = float(zoning_data.loc[zone, 'Side Yards (total width)']) if zoning_data.loc[zone, 'Side Yards (total width)'] is not None else 0 / 2
        max_height = float(zoning_data.loc[zone, 'Building Height (max)']) if zoning_data.loc[zone, 'Building Height (max)'] is not None else np.inf

        buildable_width = max(0, gdf['LotFront'] - 2 * side_yards)
        buildable_depth = max(0, gdf['LotDepth'] - front_yard - rear_yard)
        buildable_footprint = buildable_width * buildable_depth

        max_floors, effective_floor_height = calculate_max_floors(max_height, float(params['floor_height']))
        proposed_floor_area = min(gdf['LotArea'] * proposed_far, buildable_footprint * max_floors)

        land_value = gdf['LotArea'] * float(params['land_value_per_sqft'])
        demolition_cost = gdf['BldgArea'] * float(params['demolition_cost_per_sqft'])
        construction_cost = proposed_floor_area * float(params['construction_cost_per_sqft'])
        total_development_cost = land_value + demolition_cost + construction_cost

        redeveloped_value = proposed_floor_area * float(params['market_value_per_sqft'])

        equity_investment = total_development_cost * (1 - float(params['debt_ratio']))
        debt = total_development_cost * float(params['debt_ratio'])

        annual_noi = redeveloped_value * 0.05  # Assuming 5% cap rate
        annual_debt_service = debt * (float(params['interest_rate']) * (1 + float(params['interest_rate']))**float(params['holding_period_years'])) / ((1 + float(params['interest_rate']))**float(params['holding_period_years']) - 1)

        sale_price = redeveloped_value * (1 + float(params['appreciation_rate']))**float(params['holding_period_years'])

        cashflows = np.array([
            -equity_investment,
            *([annual_noi - annual_debt_service] * (int(params['holding_period_years']) - 1)),
            annual_noi - annual_debt_service + sale_price - debt
        ])

        npv_redevelopment = safe_npv(float(params['discount_rate']), cashflows)
        irr_redevelopment = safe_irr(cashflows)
        equity_multiple_redevelopment = np.sum(cashflows) / equity_investment

        # Select the strategy with the best financial metrics
        if npv_redevelopment > npv_existing:
            selected_strategy = 'Redevelopment'
            selected_npv = npv_redevelopment
            selected_irr = irr_redevelopment
            selected_equity_multiple = equity_multiple_redevelopment
            selected_proposed_floor_area = proposed_floor_area
            selected_max_floors = max_floors
            selected_redevelopment_cost = total_development_cost
            selected_redeveloped_value = redeveloped_value
        else:
            selected_strategy = 'Existing'
            selected_npv = npv_existing
            selected_irr = irr_existing
            selected_equity_multiple = equity_multiple_existing
            selected_proposed_floor_area = gdf['BldgArea']
            selected_max_floors = gdf['NumFloors']
            selected_redevelopment_cost = current_value
            selected_redeveloped_value = current_value

        return pd.DataFrame({
            'selected_strategy': selected_strategy,
            'current_value': current_value,
            'redevelopment_cost': selected_redevelopment_cost,
            'redeveloped_value': selected_redeveloped_value,
            'npv': selected_npv,
            'irr': selected_irr,
            'equity_multiple': selected_equity_multiple,
            'proposed_floor_area': selected_proposed_floor_area,
            'max_floors': selected_max_floors
        }, index=[gdf.name])
    except Exception as e:
        print(f"Error in analyze_redevelopment_potential_single: {e}")
        return pd.DataFrame({key: np.nan for key in ['selected_strategy', 'current_value', 'redevelopment_cost', 'redeveloped_value', 'npv', 'irr', 'equity_multiple', 'proposed_floor_area', 'max_floors']}, index=[gdf.name])

import numpy as np
import pandas as pd
import numpy_financial as npf
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming merged_gdf_r1_r5, current_zoning_data, proposed_zoning_data are already loaded

# Revised function to analyze clusters with sampled parameters and spectral clustering
def analyze_clusters_spectral(clustered_data, feature_importances, params, k_values, alpha=0.05, power=0.80, margin_of_error=0.05):
    sample_sizes = determine_cluster_sample_sizes(
        clustered_data, feature_importances, cluster_column='cluster', alpha=alpha, power=power, margin_of_error=margin_of_error
    )

    spectral = SpectralClustering(n_clusters=len(sample_sizes), affinity='rbf', assign_labels='discretize', random_state=42)
    clustered_data['cluster'] = spectral.fit_predict(clustered_data[features_with_zoning])

    rep_properties_df = identify_representative_properties(clustered_data, spectral, features_with_zoning)

    results = []
    for cluster_id, sample_size in sample_sizes.items():
        row = rep_properties_df[rep_properties_df['cluster'] == cluster_id].iloc[0]
        sampled_params = sample_parameters(sample_size)

        for i in range(sample_size):
            params = {k: v[i] for k, v in sampled_params.items()}
            current_sim = analyze_redevelopment_potential_single(row, current_zoning_data, params)
            proposed_sim = analyze_redevelopment_potential_single(row, proposed_zoning_data, params)
            results.append((current_sim, proposed_sim))

    return results, sample_sizes

# Define features, parameters, and importances
features_with_zoning = [
    'LotArea', 'LotFront', 'LotDepth', 'BldgFront', 'BldgDepth', 'NumFloors', 'height', 'current_FAR', 'ResidFAR',
    'min_front_yard_depth_current', 'max_building_height_current', 'max_residential_far_proposed',
    'min_front_yard_depth_proposed', 'max_building_height_proposed', 'FacilFAR', 'BldgArea', 'ResArea', 'FrontYardDepth',
    'RearYardDepth', 'LotCoverageRatio', 'BuildingAspectRatio', 'BuildableArea_current', 'BuildableArea_proposed',
    'SetbackComplianceRatio_current', 'SetbackComplianceRatio_proposed', 'FloorAreaPotential_current', 'FloorAreaPotential_proposed',
    'Centroid_x', 'Centroid_y', 'Area', 'NumFloors_estimated_flag', 'ResArea_estimated_flag', 'BuildingAspectRatio_estimated_flag',
    'LandUse_01', 'LandUse_02', 'LandUse_03', 'LandUse_04', 'LandUse_05', 'LandUse_06', 'LandUse_07', 'LandUse_08',
    'LandUse_09', 'LandUse_10', 'LandUse_11', 'LandUse_Unknown'
]

# Start clustering with parameter sampling
results, sample_sizes = analyze_clusters_spectral(
    merged_gdf_r1_r5, feature_importances, params, k_values=np.arange(2, 10), alpha=0.05, power=0.80, margin_of_error=0.05
)

# Combine the results
current_simulations = pd.concat([result[0] for result in results], ignore_index=True)
proposed_simulations = pd.concat([result[1] for result in results], ignore_index=True)

# Visualize the results
visualize_results(current_simulations, proposed_simulations)

# Visualize clusters
sns.scatterplot(data=merged_gdf_r1_r5, x='Centroid_x', y='Centroid_y', hue='cluster', palette='viridis')
plt.show()